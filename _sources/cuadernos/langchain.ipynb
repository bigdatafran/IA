{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "dab750dd",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "# Introducción a LangChain\n",
    "\n",
    "En el ámbito de la inteligencia artificial y el desarrollo de aplicaciones basadas en modelos de lenguaje, **LangChain** se ha convertido en una de las herramientas más innovadoras y versátiles. Se trata de un framework diseñado para facilitar la integración de **modelos de lenguaje de gran tamaño (LLMs, por sus siglas en inglés)** en aplicaciones dinámicas e interactivas. Su objetivo principal es simplificar el desarrollo de flujos de trabajo complejos que combinan diferentes fuentes de datos, almacenamiento de memoria, ejecución de agentes y procesamiento estructurado de información.  \n",
    "\n",
    "Una de las características más destacadas de LangChain es su capacidad para conectar modelos de lenguaje con bases de datos, APIs y documentos externos, permitiendo a los desarrolladores crear aplicaciones avanzadas como **asistentes conversacionales, motores de búsqueda mejorados, generación de código automático y automatización de tareas empresariales**. Para lograrlo, LangChain proporciona módulos modulares y componibles que permiten personalizar y optimizar la interacción con los modelos de IA.  \n",
    "\n",
    "El framework se basa en cinco componentes clave:  \n",
    "\n",
    "1. **Modelos de lenguaje**: Integra modelos como OpenAI GPT, Hugging Face Transformers, Cohere y muchos más, facilitando su uso en diversas tareas de procesamiento del lenguaje natural.  \n",
    "2. **Encadenamiento de procesos (Chains)**: Permite combinar múltiples pasos en una sola secuencia lógica para tareas más avanzadas.  \n",
    "3. **Memoria**: Proporciona almacenamiento de contexto en conversaciones para mejorar la coherencia y continuidad de las interacciones.  \n",
    "4. **Recuperación y conexión con datos externos**: Facilita el acceso a bases de datos, documentos y APIs para enriquecer las respuestas del modelo.  \n",
    "5. **Agentes y herramientas**: Permite el uso de modelos como agentes capaces de tomar decisiones y ejecutar acciones basadas en entradas dinámicas.  \n",
    "\n",
    "El uso de LangChain está revolucionando sectores como la **automatización empresarial, la educación, el soporte al cliente y la investigación**, ofreciendo soluciones más inteligentes y adaptadas a las necesidades del usuario. Su flexibilidad y capacidad de integración hacen de este framework una opción ideal para quienes buscan desarrollar aplicaciones de inteligencia artificial con capacidades conversacionales avanzadas y una gestión eficiente del conocimiento.  \n",
    "\n",
    "En conclusión, LangChain representa un avance significativo en la forma en que los modelos de lenguaje interactúan con entornos del mundo real. Su estructura modular, junto con su compatibilidad con diferentes fuentes de información, permite a los desarrolladores crear soluciones más sofisticadas e inteligentes, abriendo un abanico de posibilidades en el campo de la inteligencia artificial aplicada.  \n",
    "\n",
    "\n",
    "## Apartados de Langchain.\n",
    "\n",
    "Langchain cuenta con cinco grandes bloques o apartados: Models, Prompts, indexes, memori, cahin y agentes:\n",
    "\n",
    "\n",
    "* Models: Indica la red neuronal que se va a uitilizar.\n",
    "\n",
    "* Promts: son los textos enviados al modelo. Pueden ser de diferentes tipos:\n",
    "\n",
    "1.- Promt template: Son textos que sirven como guia del modelo\n",
    "\n",
    "2.- chat promt template. Modelos para los chats\n",
    "\n",
    "3.- promt value. Seria el texto completo ya formateado\n",
    "\n",
    "4.- Example selector. Ayuda y genera mejor las respuestas.\n",
    "\n",
    "* Index. Para dar acceso a distinta fuentes de datos. Permite indexar un gran volumen de documentos. Pueden ser:\n",
    "\n",
    "1.- Document Loader: abrir, cargar y procesar diferentes archivos.\n",
    "\n",
    "2.- Text spliter: partir un documento en bloques más cortos\n",
    "\n",
    "3.- Vector stores: Donde almacena los embedings\n",
    "\n",
    "4.- Retrieves. Ayuda a traer información de algún documento específico\n",
    "\n",
    "* Memory: Dar memoria para por ejemplo los chats\n",
    "\n",
    "* cadenas/chain: Permitir unir modelos o cadenas entre sí\n",
    "\n",
    "* Agentes: Dar acceso a ciertas herramientas para solucionar mejor una determinada tarea\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e546dfbc-9103-40df-a57b-096cdea22f0d",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "## Cómo trabajar con LangChain en remoto\n",
    "\n",
    "Existen métodos de pagos con los que se puede trabajar en IA de forma remota. Nosotros y con el fin de que el lector pueda adquirir conocimientos sobre esta materia sin incurrir en ningún costo, hemos desarrollado la mayor parte de los apartados en forma local de manera que no se genere coste alguno.\n",
    "\n",
    "No obstante para aquellos que quieran saber cómo proceder mediante alguno de los métodos de pago existente, se invita al lector a [visitar este apartado](pago).\n",
    "\n",
    "## Como trabajar con Google Colab.\n",
    "```{index} Colab\n",
    "```\n",
    "\n",
    "Google Colab es una buena herramienta computacional para realizar trabajos con LLM, en este apartado vamos a mostrar dos formas de poder trabajar este tipo de modelos.\n",
    "\n",
    "1.- Descargando directamente el model de Hugging Face, como se [muestra en este apartado](llama2), y se <a href=\"https://www.youtube.com/watch?v=Xc5xNRM_hvk\" target=\"_blank\"> pude ver en este vídeo </a>.\n",
    "\n",
    "2.- Otra opción interesante es instalar en Colab *colab-xtera\" y alli instalar ollama. La forma de proceder en este caso <a href=\"https://www.youtube.com/watch?v=4tVdDLrucOk\" target=\"_blank\"> la puedes ver en este vídeo </a>."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7cce8e4-cc32-4d13-915b-927c5c5c0777",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "## Cómo trabajar con LangChain en local.\n",
    "\n",
    "Para poder trabajar en local, sin necesidad de tener conexión con proveedor de pago, lo que vamos a hacer es trabajar con ollama, cuyo formato de uso lo puedes ver dentro de este trabajo y en concreto [en este apartado](ollama1), al que se invita a ir al lector si no conoce la materia. En adelante se presupone que el lector conoce esta herramienta para poder trabajar con modelos en local.\n",
    "\n",
    "Una vez se tenga ese conocimiento, como inicio lo que vamos a crear una serie de solicitudes de entrada básicas para modelos y vamos a ver cómo gestionar los resultados que nos devuelven los LLM.\n",
    "\n",
    "En esta sección nuestro interés se va a centrar en las funcionalidades básicas y la sintaxis que se necesita para hacer esto con LangChain.\n",
    "\n",
    "El uso de Langchain y el componente Modelo IO nos permitirá construir cadenas más adelante, pero también nos dará más flexibilidad para cambiar de proveedor de LLM en el futuro , ya que la sintaxis está estandarizada en todos los LLM y solo cambian los parámetros o argumentos proporcionados.\n",
    "\n",
    "Debemos tener en cuenta 2 parámetros importantes en las solicitudes a las APIs de los LLMs:\n",
    "\n",
    "![](fig/tipos.PNG)\n",
    "\n",
    "Para verificar cómo conectar a los diferentes LLMs integrados en Langchain, ver el siguiente enlace :\n",
    "\n",
    "https://python.langchain.com/v0.2/docs/integrations/chat/\n",
    "\n",
    "Procedemos a continuación a mostrar un ejemplo sobre la creación de un chat en local utilizando el modelo ue tenemos cargado en ollama y que se denomina 'llama3.1'."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1aaba2ca-dff7-457f-89d7-29be97b7e27f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import langchain\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain.schema import SystemMessage, HumanMessage\n",
    "\n",
    "chat = ChatOpenAI(\n",
    "    model=\"llama3.2\",\n",
    "    base_url = 'http://localhost:11434/v1',\n",
    "    api_key='ollama', # required, but unused,\n",
    ")\n",
    "\n",
    "resultado = chat.invoke([HumanMessage(content=\"¿Puedes decirme donde se encuentra cáceres?\")])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5513a0ae-018b-4c51-a9c5-cb4a26d7db95",
   "metadata": {},
   "outputs": [],
   "source": [
    "resultado"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b867397-12bf-4af9-9180-55c529df4de4",
   "metadata": {},
   "source": [
    "Si queremos ver sólo el resultado buscado, debemos ejecutar la siguiente instrucción"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b1e00e6-4f63-4c3e-888d-5230d0585264",
   "metadata": {},
   "outputs": [],
   "source": [
    "resultado.content"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97546bd4-72c4-4fd7-bef0-ce85a51c7a1b",
   "metadata": {},
   "source": [
    "```{index} SystemMessage,HumanMessage\n",
    "```\n",
    "Especificamos el SystemMessage para definir la personalidad que debe tomar el sistema"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd3d2664-9fbd-48de-9bfb-d20df4968be7",
   "metadata": {},
   "outputs": [],
   "source": [
    "resultado = chat.invoke([SystemMessage(content='Eres un historiador que conoce los detalles de todas las ciudades del mundo'),\n",
    "               HumanMessage(content='¿Puedes decirme dónde se encuentra Cáceres')])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87075aa1-a2ae-4a32-95b6-41cb6eecc1b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "resultado.content"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7eb2af10-d468-4db8-a3d0-2f24e6b36d01",
   "metadata": {},
   "source": [
    "Se pueden obtener varios resultados invocando al chat de OpenAI con \"generate\". Observar en este ejemplo la importancia que tiene el dar una información previa al modelo para indicar qué tipo de respuesta nos va a dar."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc76d89f-d6b3-4595-929e-d3e2ae16f280",
   "metadata": {},
   "outputs": [],
   "source": [
    "resultado = chat.generate(\n",
    "    [\n",
    "        [SystemMessage(content='Eres un historiador que conoce los detalles de todas las ciudades del mundo'),\n",
    "         HumanMessage(content='¿Puedes decirme dónde se encuentra Cáceres')],\n",
    "        [SystemMessage(content='Eres un joven rudo que no le gusta que le pregunten, solo quiere estar de fiesta'),\n",
    "         HumanMessage(content='¿Puedes decirme dónde se encuentra Cáceres')]\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9617ee8-fa1a-420b-a063-43fa3a71e240",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Resultado con primer sistema\n",
    "print(resultado.generations[0][0].text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d3a9a59-b7f1-4b09-b8bd-ef225f4e21ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Resultado con segundo sistema\n",
    "print(resultado.generations[1][0].text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11497ae3-30fb-4329-b638-b7e097d672c7",
   "metadata": {},
   "source": [
    "## Las plantillas (templates) en LangChain.\n",
    "```{index} Plantillas, Templates\n",
    "```\n",
    "* Las plantillas nos permiten configurar y modificar fácilmente nuestras indicaciones de entrada para las\n",
    "llamadas de LLM.\n",
    "• Las plantillas ofrecen un enfoque más sistemático para pasar variables a solicitudes de modelos, en lugar de\n",
    "usar literales de cadena f o llamadas . format (), PromptTemplate las convierte en nombres de parámetros de\n",
    "función que podemos pasar.\n",
    "• Es recomendable usar plantillas para estandarizar los mensajes que enviamos a los LLMs para mayor\n",
    "flexibilidad y facilidad en futuros usos.\n",
    "\n",
    "Veamos cómo utilizar estas plantillas en LangChain. Primero importamos las librerías:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86999ceb-9b2f-425d-9f3e-a374f60859aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.prompts import (\n",
    "    ChatPromptTemplate,\n",
    "    PromptTemplate,\n",
    "    SystemMessagePromptTemplate,\n",
    "    AIMessagePromptTemplate,\n",
    "    HumanMessagePromptTemplate,\n",
    ")\n",
    "from langchain.schema import (\n",
    "    AIMessage,\n",
    "    HumanMessage,\n",
    "    SystemMessage\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cbeca29a-4c6c-44ef-8d09-830c1aeef8cb",
   "metadata": {},
   "source": [
    "Y después procedemeos a crear diferentes plantillas. Comenzamos con una plantilla para el sistema"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0a04ec6-1aa2-4eb3-8e6f-a2b3e0747ca2",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Creamos la plantilla del sistema (system_template)\n",
    "system_template=\"Eres una IA especializada en coches de tipo {tipo_coches} y generar artículos que se leen en {tiempo_lectura}.\"\n",
    "# Las variables las metemeos entre corchetes\n",
    "system_message_prompt = SystemMessagePromptTemplate.from_template(system_template)\n",
    "# Mostramos las variables que admite esta plantilla\n",
    "system_message_prompt.input_variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21090cdd-4272-4ab7-851c-f274a2159c62",
   "metadata": {},
   "outputs": [],
   "source": [
    "#De forma similar al procedimiento anterior, creamos la plantilla de usuario (human_template)\n",
    "human_template=\"Necesito un artículo para vehículos con motor {peticion_tipo_motor}\"\n",
    "human_message_prompt = HumanMessagePromptTemplate.from_template(human_template)\n",
    "# También mostramos las variables que admite\n",
    "human_message_prompt.input_variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3dfce83-e23d-48c9-a710-15ef4aad2b40",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Creamos una plantilla de chat con la concatenación tanto de mensajes del sistema como del humano\n",
    "chat_prompt = ChatPromptTemplate.from_messages([system_message_prompt, human_message_prompt])\n",
    "# Los nombres de las variables, serán todas las definidas anteriormente\n",
    "chat_prompt.input_variables"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2181a82-0f87-40f7-847b-33da0578f0c1",
   "metadata": {},
   "source": [
    "Una vez definida la plantilla, lo que nos queda es introducir los valores concretos que queremos tenga las variables definidas anteriormente"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3488f271-2c11-4950-b742-ab685ba1ea7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Completar el chat gracias al formateo de los mensajes\n",
    "chat_prompt.format_prompt(peticion_tipo_motor=\"híbrido enchufable\", tiempo_lectura=\"10 min\", tipo_coches=\"japoneses\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1cb555bc-791e-425c-ab6a-33696324d06a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Transformamos el objeto prompt a una lista de mensajes y lo guardamos en \"solicitud_completa\" que es lo que pasaremos al LLM finalmente\n",
    "solicitud_completa = chat_prompt.format_prompt(peticion_tipo_motor=\"híbrido enchufable\", tiempo_lectura=\"10 min\", tipo_coches=\"japoneses\").to_messages()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4374e722-9bc5-4afd-a697-32e62af334f2",
   "metadata": {},
   "source": [
    "Con esto ya tenemos completada nuestra plantilla y lista para que pueda ser procesada pro el LLM con el que estemos trabajando"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4acbc40c-ff33-4ef0-8dfb-83644c1b952d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import langchain\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain.schema import SystemMessage, HumanMessage\n",
    "\n",
    "chat = ChatOpenAI(\n",
    "    model=\"llama3.2\",\n",
    "    base_url = 'http://localhost:11434/v1',\n",
    "    api_key='ollama', # required, but unused,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7a6147c-1418-4f6d-ada8-c848aac5ea6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "result = chat.invoke(solicitud_completa)\n",
    "result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64166e1a-df72-4060-a60a-f8cecd3c6070",
   "metadata": {},
   "outputs": [],
   "source": [
    "result.content"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "521bc2c7-713f-4f8d-ac67-2df5b9e34b97",
   "metadata": {},
   "source": [
    "## Parsear y procesar la salida.\n",
    "```{index} parseadores, Format_instructions\n",
    "```\n",
    "\n",
    "A menudo, al conectar la salida de un LLM (modelo de lenguaje grande), necesitas que esté en un formato particular, por ejemplo, puedes querer un objeto datetime de Python o un objeto JSON.\n",
    "\n",
    "LangChain viene con utilidades de análisis que te permiten convertir fácilmente las salidas en tipos de datos\n",
    "precisos gracias a los **parseadores**.\n",
    "\n",
    "Los elementos claves de estos parseadores son los siguientes:\n",
    "\n",
    "* Método parse (): Método concreto para evaluar la cadena de texto string ) y parsearla al tipo deseado.\n",
    "\n",
    "* Format_instructions : Una cadena de texto extra que Langchain añade al final del prompt para asistir y facilitar la interpretación por el LLM del formato deseado.\n",
    "\n",
    "Si no consigues el resultado parseado correctamente (por ejemplo, la respuesta del LLM es más extensa que únicamente una fecha que quieras parsear ), hay dos soluciones:\n",
    "\n",
    "1.- Usar parseador Auto fix\n",
    "\n",
    "2.- Usar un “ system prompt ” para dar mayor detalle al LLM de cómo debe actuar y responder.\n",
    "\n",
    "Veamos algunos ejemplos prácticos para dejar más claro cómo se puede operar en LangChain con estos parseadores. Primero impostamos librerías necesarias e instanciamos un modelo de chat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7df462e-3195-4be2-acbf-9a7ba65f670a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import langchain\n",
    "from langchain.prompts import PromptTemplate, SystemMessagePromptTemplate,ChatPromptTemplate, HumanMessagePromptTemplate\n",
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "\n",
    "chat = ChatOpenAI(\n",
    "    model=\"llama3.2\",\n",
    "    base_url = 'http://localhost:11434/v1',\n",
    "    api_key='ollama', # required, but unused,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8942897-ed66-42b3-8f62-09ce4e4813bd",
   "metadata": {},
   "source": [
    "### Parsear lista elementos separados por comas.\n",
    "\n",
    "Este puede ser el caso que necesitemos obtener una salida de los elementos que después sean procesados como un fichero de tipo csv."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1748af6c-ef46-4f66-a24d-5f4b64ee3bd3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.output_parsers import CommaSeparatedListOutputParser\n",
    "\n",
    "output_parser = CommaSeparatedListOutputParser()\n",
    "format_instructions = output_parser.get_format_instructions() \n",
    "#Nos devuelve las instrucciones que va a pasar al LLM en función del parseador concreto\n",
    "print(format_instructions)\n",
    "# Como vemos es una instrucción que el parseador manda al LLM para obtener la lista separada por comas.\n",
    "# Viene en ingles, pero funciona perfectamente si trabajamos en castellano"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ab11e5d-bf1a-4217-bafc-d8eb947c102f",
   "metadata": {},
   "source": [
    "Vamos a ver un ejemplo imaginario. Suponemos que nos ha devuelto una serie de palabras, separadas por comas. Entonces vamos a ver como pasar eso a una lista que contenga las palabras anteriores que están separadas pro comas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f33bd01a-aba5-4e8a-8a70-b5413e5ebeaf",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Respuesta imaginaria\n",
    "respuesta = \"coche, árbol, carretera\"\n",
    "output_parser.parse(respuesta)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2caaec67-a04d-40d9-9c2c-eda7368ee55f",
   "metadata": {},
   "source": [
    "Veamos un ejemplo más concreto."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0c14993-6c92-40c1-8502-19cc889e4706",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Creamos la plantilla de usuario (human_template) con la concatenación de la variable \"request\" (la solicitud) y la variable \"format_instructions\" con \n",
    "#las instrucciones adicionales que le pasaremos al LLM\n",
    "human_template = '{request}\\n{format_instructions}'\n",
    "human_prompt = HumanMessagePromptTemplate.from_template(human_template)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9c9c04a-fe52-4e8c-810e-5aca1ddfd6a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Creamos el prompt y le damos formato a las variables\n",
    "chat_prompt = ChatPromptTemplate.from_messages([human_prompt])\n",
    "\n",
    "chat_prompt.format_prompt(request=\"dime 5 características de los coches americanos\",\n",
    "                   format_instructions = output_parser.get_format_instructions()) #Las instrucciones son las que proporciona el propio parseador"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ac93514-1fd7-4da1-9a71-33ae85736db1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Transformamos el objeto prompt a una lista de mensajes y lo guardamos en \"solicitud_completa\" que es lo que pasaremos al LLM finalmente\n",
    "solicitud_completa = chat_prompt.format_prompt(request=\"dime 5 características de los coches americanos\",\n",
    "                   format_instructions = output_parser.get_format_instructions()).to_messages()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a86bfca5-c4e3-42b3-a03a-aa18b3c8f95a",
   "metadata": {},
   "outputs": [],
   "source": [
    "result = chat.invoke(solicitud_completa)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49d0109e-d816-4230-be36-fa7853d41fd1",
   "metadata": {},
   "outputs": [],
   "source": [
    "result.content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34ac5803-3c2d-4989-af9d-cfb4b26cf304",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convertir a la salida esperada\n",
    "output_parser.parse(result.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d30de24-f204-4e59-b702-efaccb21b679",
   "metadata": {},
   "source": [
    "### Parseador en formato de fechas\n",
    "\n",
    "Veamos ahora cómo podemos parsear una fecha"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3890c939-1495-4ed9-9f9c-1b4d2b982d2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.output_parsers import DatetimeOutputParser\n",
    "\n",
    "output_parser = DatetimeOutputParser()\n",
    "print(output_parser.get_format_instructions())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75793685-9cfd-4caa-989d-7423ec35d66e",
   "metadata": {},
   "outputs": [],
   "source": [
    "template_text = \"{request}\\n{format_instructions}\"\n",
    "human_prompt=HumanMessagePromptTemplate.from_template(template_text)\n",
    "chat_prompt = ChatPromptTemplate.from_messages([human_prompt])\n",
    "print(chat_prompt.format(request=\"¿Cuándo es el día de la declaración de independencia de los EEUU?\",\n",
    "                   format_instructions=output_parser.get_format_instructions()\n",
    "                   ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8cb14985-a95b-4ac2-8b81-3ce3fa1b790b",
   "metadata": {},
   "outputs": [],
   "source": [
    "solicitud_completa = chat_prompt.format_prompt(request=\"¿Cuándo es el día de la declaración de independencia de los EEUU?\",\n",
    "                   format_instructions=output_parser.get_format_instructions()\n",
    "                   ).to_messages()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fefe677c-9f29-4ddf-a638-59377babbe62",
   "metadata": {},
   "outputs": [],
   "source": [
    "result = chat.invoke(solicitud_completa)\n",
    "result.content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65da8fe0-c699-4c53-ba81-d8ed4f34188a",
   "metadata": {},
   "outputs": [],
   "source": [
    "output_parser.parse(result.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03014ea7-f90f-49ef-8d17-b2b2faad06ec",
   "metadata": {},
   "source": [
    "### Métodos para solucionar problemas de parcheo.\n",
    "\n",
    "Hay ocasiones en las que no obtenemos la salida que nosotros queremos y existen algunas soluciones para dar solución a esos problemas.\n",
    "\n",
    "#### Auto-Fix Parser.\n",
    "```{index} Auto-Fix parser\n",
    "```\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a748d150-879f-4109-9db3-1fb11fa2b67e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.output_parsers import OutputFixingParser\n",
    "\n",
    "output_parser_dates = DatetimeOutputParser()\n",
    "\n",
    "misformatted = result.content\n",
    "\n",
    "misformatted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03125a04-5a61-43a0-bdfd-22bf943c366c",
   "metadata": {},
   "outputs": [],
   "source": [
    "new_parser = OutputFixingParser.from_llm(parser=output_parser_dates, llm=chat)\n",
    "new_parser.parse(misformatted)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8bdfa90-eed8-4deb-8846-3ef57914eec3",
   "metadata": {},
   "source": [
    "#### System Promt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1250d2b9-50b7-4e33-b633-1e10e315e3d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "system_prompt = SystemMessagePromptTemplate.from_template(\"Tienes que responder únicamente con un patrón de fechas\")\n",
    "template_text = \"{request}\\n{format_instructions}\"\n",
    "human_prompt=HumanMessagePromptTemplate.from_template(template_text)\n",
    "chat_prompt = ChatPromptTemplate.from_messages([system_prompt,human_prompt])\n",
    "print(chat_prompt.format(request=\"¿Cuándo es el día de la declaración de independencia de los EEUU?\",\n",
    "                   format_instructions=output_parser_dates.get_format_instructions()\n",
    "                   ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad1f92ec-0628-4db5-8fd2-0bbed8128e9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "solicitud_completa = chat_prompt.format_prompt(request=\"¿Cuándo es el día de la declaración de independencia de los EEUU?\",\n",
    "                   format_instructions=output_parser_dates.get_format_instructions()\n",
    "                   ).to_messages()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e200fc11-83ed-4455-9998-e6d3ab33185e",
   "metadata": {},
   "outputs": [],
   "source": [
    "result = chat.invoke(solicitud_completa)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78e62e16-96fa-446a-ac68-a3281e6fae13",
   "metadata": {},
   "outputs": [],
   "source": [
    "result.content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "907b1087-afa4-4b59-afbc-9e74cc3e0a93",
   "metadata": {},
   "outputs": [],
   "source": [
    "output_parser_dates.parse(result.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45bbb22b-e642-4af8-b795-ca582af71e17",
   "metadata": {},
   "source": [
    "Podemos ver que al final con esta última herramienta hemos podido solucionar los errores de parseo que obteníamos anteriormente"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67afb9e2-1dc8-466a-a3d3-c9340828d795",
   "metadata": {},
   "source": [
    "## Serialización de Prompts."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05331098-625b-4871-81f4-be72eb311f86",
   "metadata": {},
   "source": [
    "En un capítulo anterior, hemos visto cómo poder trabajar con los denominados Pompts de LangChain. En ciertas ocasiones y bajo determinadas circunstancias, pudiera ocurrir que esos prompts lo queramos guardar, paro por ejemplo utilizarlos para futuros trabajos o compartir con otras personas. En esta sección, vamos a ver cómo podemos conseguir todo esto."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1269d55f-f661-48a2-a9ea-86d5a29b7bb2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.prompts import PromptTemplate, SystemMessagePromptTemplate,ChatPromptTemplate, HumanMessagePromptTemplate\n",
    "from langchain_openai import ChatOpenAI"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7d39d26-8b69-4560-861a-7c38352d7da0",
   "metadata": {},
   "source": [
    "Creamos la plantilla y la guardamos con la denominación \"prompt.json\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4fc4129d-f223-4ec9-915a-19524e7201ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "plantilla = \"Pregunta: {pregunta_usuario}\\n\\nRespuesta: Vamos a verlo paso a paso.\"\n",
    "prompt = PromptTemplate(template=plantilla)\n",
    "prompt.save(\"prompt.json\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78acc7bb-90dd-44b0-8693-4609b1f74824",
   "metadata": {},
   "source": [
    "Una vez hecho todo esto, posteriormente podremos cargar y utilizar de nuevo esa plantilla, de la siguiente manera:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1601bbc3-8e75-4cb7-b02f-ec2af7474c3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.prompts import load_prompt\n",
    "prompt_cargado = load_prompt('prompt.json')\n",
    "prompt_cargado"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
