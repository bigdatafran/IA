{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0b7d7fbb-ae49-4e73-aaef-dbff99867e58",
   "metadata": {},
   "source": [
    "# Los Agentes en LangChain\n",
    "```{index} Agentes, ReAct\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2868bed-8546-4131-86c2-e4fb6ad8ec40",
   "metadata": {},
   "source": [
    "Los Agentes son una pieza importante y potente de LangChain. Dentro de este apartado, vamos a ver los siguientes aspectos:\n",
    "\n",
    "¬øQu√© son los agentes y su funcionamiento?\n",
    "¬øC√≥mo crear agentes asistidos con motores de b√∫squeda?\n",
    "¬øC√≥mo crear agentes programadores de c√≥digo y conversacionales?\n",
    "¬øC√≥mo usar herramientas personalizadas por los agentes?\n",
    "¬øC√≥mo crear potentes agentes reales?\n",
    "\n",
    "Los agentes son una de las partes m√°s novedosas de LangChain , pero ofrecen un enorme potencial para aplicaciones basadas en LLM, y adem√°s de una manera muy sencilla.\n",
    "\n",
    "Al combinar lo que ya aprendimos sobre Model IO, conexiones de datos y cadenas, ya hemos abordado aplicaciones similares a agentes, pero los agentes facilitan la creaci√≥n de estas aplicaciones siendo adem√°s m√°s robustas.\n",
    "\n",
    "B√°sicamente, los agentes permiten a los LLM conectarse a herramientas (por ejemplo, Wikipedia, Calculadora, B√∫squeda de Google, etc.) y llevar a cabo un enfoque estructurado para completar una **tarea basada en ReAct** (razonamiento y actuaci√≥n).\n",
    "\n",
    "ReAct es un enfoque de inteligencia artificial que combina **razonamiento (Reasoning) y acci√≥n (Acting)** para mejorar la toma de decisiones y la interacci√≥n con el entorno. Se utiliza en modelos de lenguaje y agentes de IA para mejorar su capacidad de resolver problemas de manera m√°s eficiente.\n",
    "\n",
    "##  ¬øC√≥mo funciona ReAct?\n",
    "El enfoque ReAct permite a un modelo de IA no solo **generar respuestas**, sino tambi√©n **razonar sobre ellas y actuar en consecuencia**. Se basa en un ciclo de:\n",
    "1. **Pensamiento**: El modelo analiza la situaci√≥n y razona sobre los pasos a seguir.\n",
    "2. **Acci√≥n**: Toma decisiones o consulta herramientas externas para obtener m√°s informaci√≥n.\n",
    "3. **Observaci√≥n**: Analiza los resultados de la acci√≥n y ajusta su razonamiento.\n",
    "\n",
    "##  ¬øPara qu√© se usa?\n",
    "ReAct se usa en:\n",
    "- **Agentes conversacionales avanzados** (como asistentes inteligentes que pueden planificar y razonar).\n",
    "- **Sistemas de b√∫squeda y recuperaci√≥n de informaci√≥n** (como IA que consulta bases de datos o la web).\n",
    "- **Juegos y simulaciones** (donde los agentes de IA deben tomar decisiones en entornos din√°micos).\n",
    "\n",
    "Al Agente se le asigna una tarea y puede razonar qu√© herramientas son apropiadas para usar y luego puede utilizar esos resultados para continuar a trav√©s de una cadena interna hasta que resuelva la tarea.\n",
    "\n",
    "Los agentes pueden ser extremadamente poderosos, especialmente si los combinamos con nuestras propias herramientas personalizadas.\n",
    "\n",
    "Imagina un Agente con acceso a documentos corporativos internos y la capacidad de realizar b√∫squedas relevantes externas, de repente, tendr√°s un asistente corporativo muy poderoso con informaci√≥n interna y externa para responder preguntas (de clientes, de personal interno,...).\n",
    "\n",
    "Ver el siguiente enlace:\n",
    "\n",
    "https://python.langchain.com/v0.1/docs/modules/agents/agent_types/\n",
    "\n",
    "Una lista de herramientas disponibles, se pueden encontrar en:\n",
    "\n",
    "https://python.langchain.com/v0.1/docs/integrations/tools/\n",
    "\n",
    "**NOTA**: üëå üíñ Existe el [frimware denominado CrewaAI](crewai) que nos permite crear agentes de una forma f√°cil y eficiente."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f2a19c1-9169-4797-b930-41304ffd6555",
   "metadata": {},
   "source": [
    "## Primer caso de uso de los agentes Langchain\n",
    "\n",
    "Vamos a instalar primero la siguiente librer√≠a de Python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c34018f-be26-4f05-b7e5-d41ecd9b3e5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install -U langchain-community"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "897c04bc-3aef-4a76-99c3-f01a3db2d472",
   "metadata": {},
   "outputs": [],
   "source": [
    "import langchain\n",
    "\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain.prompts import PromptTemplate, SystemMessagePromptTemplate,ChatPromptTemplate, HumanMessagePromptTemplate\n",
    "\n",
    "llm = ChatOpenAI(\n",
    "    model=\"llama3.2\",\n",
    "    base_url = 'http://localhost:11434/v1',\n",
    "    api_key='ollama', # required, but unused,\n",
    "    temperature = 0\n",
    ")\n",
    "\n",
    "#Recomendable temperatura a 0 para que el LLM no sea muy creativo, vamos a tener muchas herramientas a nuestra disposici√≥n y queremos que \n",
    "#sea m√°s determinista\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ff044be-f1c0-4c4a-b916-a5a86ea4efc2",
   "metadata": {},
   "source": [
    "**NOTA**: seg√∫n la documentaci√≥n de LangChain, Al compilar con LangChain, todos los pasos se rastrear√°n autom√°ticamente en LangSmith. Para configurar LangSmith, solo necesitamos configurar las siguientes variables de entorno:\n",
    "\n",
    "```\n",
    "export LANGCHAIN_TRACING_V2=\"true\"\n",
    "export LANGCHAIN_API_KEY=\"<your-api-key>\"\n",
    "```\n",
    "\n",
    "Para el siguiente agente se necesita tener instalada la siguiente librer√≠a\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f12a16dc-036d-448c-9632-890ee04f70bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install numexpr"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d94beba-8b60-499d-998e-bbf31636a9dd",
   "metadata": {},
   "source": [
    "```{index} llm-math\n",
    "```\n",
    "\n",
    "A continuaci√≥n definimos la herramientas a las que tiene accesos el agente. En este caso le estamos dando la herramienta de llm-math, que es una herramienta para el c√°lculo matem√°tico."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3526dd4-4348-4f18-a15c-89337fe0d90b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.agents import load_tools,initialize_agent,AgentType,create_react_agent,AgentExecutor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f162901-06e7-438d-bd43-ac4d67efaf62",
   "metadata": {},
   "outputs": [],
   "source": [
    "# llm-math  es una herramienta para el c√°lculo matem√°tico\n",
    "tools = load_tools([\"llm-math\",],llm=llm) \n",
    "#Lista de herramientas disponibles: https://python.langchain.com/v0.1/docs/integrations/tools/"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4684a851-e7fe-4334-8872-ce55f92866b8",
   "metadata": {},
   "source": [
    "Podemos ver todos los tipos de agentes de los que podemos disponer, de la siguiente forma:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a70a8013-7ade-4c67-8c96-27cdbc240e43",
   "metadata": {},
   "outputs": [],
   "source": [
    "#dir(AgentType) #Vemos los diferentes tipos de agente a usar"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5247eb24-cdc9-415b-bdcd-f84548907c31",
   "metadata": {},
   "source": [
    "Ahora ya creamos el agente"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9019b0e2-2413-4261-a2d8-278a3c75164d",
   "metadata": {},
   "outputs": [],
   "source": [
    "agent = initialize_agent(tools, llm, agent=AgentType.ZERO_SHOT_REACT_DESCRIPTION,verbose=True,handle_parsing_errors=True) \n",
    "#Usamos el Zero Shot porque no estamos dando ning√∫n ejemplo, solo pidiendo al agente hacer una tarea sin ejemplos previos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fae25409-480e-4bb6-8c34-a78605610266",
   "metadata": {},
   "outputs": [],
   "source": [
    "resultado = agent.run(\"Dime cu√°nto es 1598 multiplicado por 1983 y despu√©s sumas 1000\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35a57e82-e9d0-42bf-a37a-a205083787cd",
   "metadata": {},
   "source": [
    "Otra forma de utilizar un agente es mediante **create_react_agent**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "111260e6-36a7-42f7-bdbe-4d869be626ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "template = '''Responde lo mejor que puedas usando tu conocimiento como LLM o bien las siguientes herramientas:\n",
    "{tools}\n",
    "Utiliza el siguiente formato:\n",
    "Pregunta: la pregunta de entrada que debes responder\n",
    "Pensamiento: siempre debes pensar en qu√© hacer\n",
    "Acci√≥n: la acci√≥n a realizar debe ser una de [{tool_names}]\n",
    "Entrada de acci√≥n: la entrada a la acci√≥n.\n",
    "Observaci√≥n: el resultado de la acci√≥n.\n",
    "... (este Pensamiento/Acci√≥n/Introducci√≥n de Acci√≥n/Observaci√≥n puede repetirse N veces,si no consigues el resultado tras 5 intentos, para la ejecuci√≥n)\n",
    "Pensamiento: ahora s√© la respuesta final\n",
    "Respuesta final: la respuesta final a la pregunta de entrada original\n",
    "¬°Comenzar! Recuerda que no siempre es necesario usar las herramientas\n",
    "Pregunta: {input}\n",
    "Pensamiento:{agent_scratchpad}'''\n",
    "\n",
    "#agent_scratchpad: El agente no llama a una herramienta solo una vez para obtener la respuesta deseada, sino que tiene una estructura que llama a las\n",
    "#herramientas repetidamente hasta obtener la respuesta deseada. Cada vez que llama a una herramienta, en este campo se almacena c√≥mo fue la \n",
    "#llamada anterior, informaci√≥n sobre la llamada anterior y el resultado."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd01ac27-6a47-4aec-91c4-f48fcc096e5f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "prompt = PromptTemplate.from_template(template)\n",
    "agente = create_react_agent(llm,tools,prompt)\n",
    "agent_executor = AgentExecutor(\n",
    "    agent=agente,\n",
    "    tools=tools,\n",
    "    verbose=True,\n",
    "    return_intermediate_steps=True,\n",
    "    handle_parsing_errors=True\n",
    ")\n",
    "respuesta = agent_executor.invoke({\"input\": \"Dime cu√°nto es 1598 multiplicado por 1983\"})\n",
    "print(respuesta)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8edddb67-3189-4951-bb23-9f3bf74e5cb8",
   "metadata": {},
   "source": [
    "## Crear agente potenciado motor b√∫squeda.\n",
    "\n",
    "Ver el ap√©ndice de este tema, para conocer los servicios que nos ofrece esta herramienta. Para poder utilizarla se necesita primero bajar la librer√≠a. Lo hacemos de la siguiente manera:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae02e74e-fb14-46c1-9faf-905ebe297297",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install google-search-results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45894b60-42da-4435-9e7b-2e148b98deaa",
   "metadata": {},
   "source": [
    "Utilizando este procedimiento tendremos acceso a motores de b√∫squeda que ser√°n mucho m√°s potentes que solo  disponer de la informaci√≥n del LLM."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b034004-df2e-4951-92fa-c0ed8ced254a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_openai import ChatOpenAI\n",
    "from langchain.prompts import PromptTemplate, SystemMessagePromptTemplate,ChatPromptTemplate, HumanMessagePromptTemplate\n",
    "from langchain.agents import load_tools,initialize_agent,AgentType,create_react_agent,AgentExecutor\n",
    "\n",
    "llm = ChatOpenAI(\n",
    "    model=\"llama3.2\",\n",
    "    base_url = 'http://localhost:11434/v1',\n",
    "    api_key='ollama', # required, but unused,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2c61956-fec0-47c8-8f38-9fb347eb5a8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# leemos la api key para serpapi\n",
    "f = open('../SERPAPIKey.txt')\n",
    "serp_api_key = f.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b22e01e9-c94e-49fa-b61c-d8607b8be59f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Definir variable de entorno para que funcione correctamente:\n",
    "import os\n",
    "os.environ[\"SERPAPI_API_KEY\"]=serp_api_key #Si no est√° definida el error nos dar√° el nombre de la variable de entorno que espera"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa3c356c-04bc-4f0c-a034-649f45ea54c8",
   "metadata": {},
   "source": [
    "Definimos las herramientas a las que el agente tendr√° acceso"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5795440a-7d2a-4118-a714-34a90ca83f82",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Le indicamos qu eutilice la herramienta serpapi, que ya tenemos conexi√≥n pues hemos indicado nuestra api-key\n",
    "tools = load_tools([\"serpapi\",\"llm-math\",],llm=llm)\n",
    "\n",
    "agent = initialize_agent(tools, llm, agent=AgentType.ZERO_SHOT_REACT_DESCRIPTION,verbose=True)\n",
    "\n",
    "agent.invoke(\"¬øEn qu√© a√±o naci√≥ Einstein? ¬øCu√°l es el resultado de ese a√±o multiplicado por 3?\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ced97222-12b0-4b78-b4d7-5f0af805bc10",
   "metadata": {},
   "source": [
    "## Creaci√≥n de un agente programador de c√≥digo.\n",
    "\n",
    "En este apartado vamos a crear un agente que genere c√≥digo python para la tarea que nosotros le solicitemos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f7e26dd-0233-42e2-a838-ffe228f25e53",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_openai import ChatOpenAI\n",
    "from langchain.prompts import PromptTemplate, SystemMessagePromptTemplate,ChatPromptTemplate, HumanMessagePromptTemplate\n",
    "from langchain.agents import load_tools,initialize_agent,AgentType,create_react_agent,AgentExecutor\n",
    "\n",
    "llm = ChatOpenAI(\n",
    "    model=\"llama3.2\",\n",
    "    base_url = 'http://localhost:11434/v1',\n",
    "    api_key='ollama', # required, but unused,\n",
    "    temperature = 0\n",
    ")\n",
    "\n",
    "#Recomendable temperatura a 0 para que el LLM no sea muy creativo, vamos a tener muchas herramientas a nuestra disposici√≥n y queremos que \n",
    "#sea m√°s determinista"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a26e7192-9581-46d5-9b81-a7d6198f67e7",
   "metadata": {},
   "source": [
    "Importamos las siguientes librer√≠as concretas para realizar estos trabajos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5676e33d-95c5-4741-99f7-cab0362f3e6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install langchain_experimental"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ffe7a3e6-4304-4984-81a7-e3644a76f12b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_experimental.agents.agent_toolkits import create_python_agent  # agente para crear c√≥digo python\n",
    "from langchain_experimental.tools.python.tool import PythonREPLTool"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a0b25b8-8631-4f69-823e-3efb5073b4e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# creamos el agente\n",
    "agent = create_python_agent(tool=PythonREPLTool(),\n",
    "                           llm=llm,\n",
    "                           agent=AgentType.ZERO_SHOT_REACT_DESCRIPTION)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8daa4bd9-aaeb-422b-8cd7-f0ffeeb26e86",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tenemos la siguiente lista de python desordenada con la que vamos a trabajar despu√©s\n",
    "lista_ejemplo = [3,1,5,3,5,6,7,3,5,10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd6309fb-3428-4422-a7b4-e6941de21fea",
   "metadata": {},
   "outputs": [],
   "source": [
    "# hacemos trabajar al agente\n",
    "agent.invoke(f'''ordena la lista {lista_ejemplo}''')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a70beaa-d7ac-454b-a650-48f750022b3c",
   "metadata": {},
   "source": [
    "La salida anterior ha generado una salida nula debido a que se ha excedido el tiempo de ejecuci√≥n. Ello es debido a las limitaciones del equipo local con el que se est√° trabajando.\n",
    "\n",
    "Ahora vamos a ver otro ejemplo pero utilizando un dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea7acadc-9f9e-4fd8-a752-556a6b7111a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install openpyxl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46df99d2-3920-4f55-b5fd-067ee23f0aff",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "df = pd.read_excel('datos_ventas_small.xlsx')\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2334ec92-eddb-4e42-9662-697408a24a2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "agent.invoke(f'''¬øQu√© sentencias de c√≥digo tendr√≠a que ejecutar para obtener la suma de venta total agregada por L√≠nea de Producto? Este ser√≠a el dataframe {df}, no tienes que ejecutar la sentencia, solo pasarme el c√≥digo a ejecutar''')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83f16fac-89ac-4a27-878d-87fcdccb29c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.groupby('L√≠nea Producto')['Venta total'].sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b160a48b-0186-4a41-81ab-7c14d73a85d2",
   "metadata": {},
   "source": [
    "Como sugerencia, es mejor pedir la instrucci√≥n de python para conseguir el objetivo, que no que te de directamente el resultado."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01371792-9229-4c3b-a66e-22a3bf0fa6f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "agent.invoke(f'''¬øQu√© sentencias de c√≥digo tendr√≠a que ejecutar para tener una visualizaci√≥n con la librer√≠a Seaborn que agregue a nivel de L√≠nea de Producto el total de venta? Este ser√≠a el dataframe {df}, recuerda que no tienes que ejecutar la sentencia, solo pasarme el c√≥digo a ejecutar''')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a32711ce-155a-4153-89c4-e1e1e003f14a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install seaborn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6de369f5-96cb-4bc2-b18f-3ddbc2d3055a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "sns.barplot(x='ID', y='Venta total', data=df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e671836-ecf2-4213-a94e-783f0cd13b55",
   "metadata": {},
   "source": [
    "## Crear herramientas personalizadas.\n",
    "\n",
    "Podemos definir nuestras propias herramientas ( tools ) para ser usadas por el agente.\n",
    "\n",
    "Es muy importante definir bien el docstring (descripci√≥n de la funci√≥n) puesto que en base a ello el agente seleccionar√° o no esa herramienta.\n",
    "\n",
    "El uso de herramientas personalizadas expande el uso de los agentes, podr√≠amos incluso definir herramientas que conecten con APIs internas de nuestra empresa para determinadas tareas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15b302f5-6958-4eed-a922-9c318cf7c5ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_openai import ChatOpenAI\n",
    "from langchain.prompts import PromptTemplate, SystemMessagePromptTemplate,ChatPromptTemplate, HumanMessagePromptTemplate\n",
    "from langchain.agents import load_tools,initialize_agent,AgentType,create_react_agent,AgentExecutor\n",
    "\n",
    "llm = ChatOpenAI(\n",
    "    model=\"llama3.2\",\n",
    "    base_url = 'http://localhost:11434/v1',\n",
    "    api_key='ollama', # required, but unused,\n",
    "    temperature = 0\n",
    ")\n",
    "\n",
    "#Recomendable temperatura a 0 para que el LLM no sea muy creativo, vamos a tener muchas herramientas a nuestra disposici√≥n y queremos que \n",
    "#sea m√°s determinista"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92239232-d13b-4964-bb63-836b203d42cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creamos nuestra herramienta personalizada\n",
    "from langchain.agents import tool"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02e3aa43-c69a-421b-bcee-47a99003bd2a",
   "metadata": {},
   "source": [
    "Definimos la funci√≥n que implementa la herramienta que va a utilizar el agente."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78754142-f189-4366-b24a-b668fd0a53d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "@tool\n",
    "def persona_amable (text: str) -> str:\n",
    "    '''Retorna la persona m√°s amable. Se espera que la entrada est√© vac√≠a \"\" \n",
    "    y retorna la persona m√°s amable del universo'''\n",
    "    return \"Miguel Celebres\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1000fdf3-3c9e-4c2d-852d-c7749f0019d8",
   "metadata": {},
   "source": [
    "El LLM va a consultar del docstring de la funci√≥n anterior, por si necesita utilizar esa herramienta personalizada para construir su respuesta.\n",
    "\n",
    "Primero vamos a realizar el ejemplo, sin utilizar esa herramienta personalizada"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5213d8d8-00d9-413a-a9f9-fc3396333896",
   "metadata": {},
   "outputs": [],
   "source": [
    "tools = load_tools([\"wikipedia\",\"llm-math\",],llm=llm)\n",
    "agent = initialize_agent(tools, llm, agent=AgentType.ZERO_SHOT_REACT_DESCRIPTION,verbose=True)\n",
    "agent.invoke(\"¬øQui√©n es la persona m√°s amable del universo?\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9c1dd56-c6fa-4a49-92db-9a18e493afbb",
   "metadata": {},
   "source": [
    "Ahora vamos a indicar que utilice la herramienta personalziada que hemos construido anteriormente"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c9a0bb6-253d-43d1-98eb-45491d21b37b",
   "metadata": {},
   "outputs": [],
   "source": [
    "tools = tools + [persona_amable]\n",
    "agent = initialize_agent(tools, llm, agent=AgentType.ZERO_SHOT_REACT_DESCRIPTION,verbose=True)\n",
    "agent.invoke(\"¬øQui√©n es la persona m√°s amable del universo?\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91b7a641-8391-4b9d-9a20-230beb12ce1d",
   "metadata": {},
   "source": [
    "Ahora vamos a ver otro ejemplo. En este caso, como nos podemos conectar a una determinada API interna"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac608c6c-c8e8-45c7-9b5e-0ebf7b88cbc2",
   "metadata": {},
   "source": [
    "```python \n",
    "@tool\n",
    "def nombre_api_interna(text: str) -> str:\n",
    "    '''Conecta a la API_xx que realiza la tarea xx, debes usar esta API Key'''\n",
    "    ##Definir conexi√≥n a la API interna y devolver un resultado\n",
    "    return resultado\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "169f3c04-9497-4edf-b456-1746919b40d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "Supongamos que queremso consultar la hora actual sin m√°s."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f381f1a-6f5c-4ddf-865c-d20e24d09982",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Solicitud con las herramientas actuales no proporciona el resultado que queremos\n",
    "agent.invoke(\"¬øCu√°l es la hora actual?\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17efd689-0b11-4ab7-bd33-dfce9cb4f2d7",
   "metadata": {},
   "source": [
    "Lo que vamos a hacer es definir una herramienta personalizada que nos resuelva este problema\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb994a11-265b-4739-acaa-eca8382bc7c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "@tool\n",
    "def hora_actual(text: str)->str:\n",
    "    '''Retorna la hora actual, debes usar esta funci√≥n para cualquier consulta sobre la hora actual. Para fechas que no sean\n",
    "    la hora actual, debes usar otra herramienta. La entrada est√° vac√≠a y la salida retorna una string'''\n",
    "    return str(datetime.now())\n",
    "\n",
    "tools = tools + [hora_actual]\n",
    "\n",
    "agent = initialize_agent(tools, llm, agent=AgentType.ZERO_SHOT_REACT_DESCRIPTION,verbose=True, handle_parsing_errors=True)\n",
    "\n",
    "# Solicitud con las herramientas actuales S√ç proporciona el resultado que queremos\n",
    "agent.invoke(\"¬øCu√°l es la hora actual?\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9c694ac-f9c0-4646-bd92-e8a434122a52",
   "metadata": {},
   "source": [
    "## Agentes conversacionales con memoria."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce361c66-0483-4914-8046-5ae2b57f57d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_openai import ChatOpenAI\n",
    "from langchain.prompts import PromptTemplate, SystemMessagePromptTemplate,ChatPromptTemplate, HumanMessagePromptTemplate\n",
    "from langchain.agents import load_tools,initialize_agent,AgentType,create_react_agent,AgentExecutor\n",
    "\n",
    "llm = ChatOpenAI(\n",
    "    model=\"llama3.2\",\n",
    "    base_url = 'http://localhost:11434/v1',\n",
    "    api_key='ollama', # required, but unused,\n",
    "    temperature = 0\n",
    ")\n",
    "\n",
    "#Recomendable temperatura a 0 para que el LLM no sea muy creativo, vamos a tener muchas herramientas a nuestra disposici√≥n y queremos que \n",
    "#sea m√°s determinista"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84e48e7c-1042-459e-8110-b6869d6a3788",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.memory import ConversationBufferMemory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e896869e-0190-4f98-ab9b-cbe00bfd3906",
   "metadata": {},
   "outputs": [],
   "source": [
    "memory = ConversationBufferMemory(memory_key=\"chat_history\") #ponemos una denominada clave a la memoria \"chat_history\"\n",
    "\n",
    "tools = load_tools([\"wikipedia\",\"llm-math\",],llm=llm)\n",
    "\n",
    "agent = initialize_agent(tools, llm, agent=AgentType.CONVERSATIONAL_REACT_DESCRIPTION,memory=memory,verbose=True)\n",
    "\n",
    "agent.invoke(\"Dime 5 productos esenciales para el mantenimiento del veh√≠culo.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e006be19-233f-4f32-9548-a897f42492f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Vamos a poner a prueba la memoria\n",
    "agent.invoke(\"Necesito la respuesta anterior en castellano\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f5dea87-4f93-4acd-b9c5-142ab57bd921",
   "metadata": {},
   "source": [
    "## Craci√≥n Agente Chatbot con memoria.\n",
    "\n",
    "Vamos acrear un Agente Chatbot con memoria a partir de sistema RAG con nuestra base de datos vectorial. Este agente va a combinar el potencial del BD vectorizadas con nuestros propios documentos y el resto de herramientas.\n",
    "\n",
    "El agente debe verificar si la herramienta apropiada es la personalizada que creemos que obtendr√° datos de la BBDD Vectorial o, sin embargo, debe usar otras herramientas como Wikipedia para consultar informaci√≥n o bien el propio conocimiento del LLM.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "329b5394-52b9-4782-adf9-604889c0e5ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_openai import ChatOpenAI\n",
    "from langchain.prompts import PromptTemplate, SystemMessagePromptTemplate,ChatPromptTemplate, HumanMessagePromptTemplate\n",
    "from langchain.agents import load_tools,initialize_agent,AgentType,create_react_agent,AgentExecutor\n",
    "\n",
    "llm = ChatOpenAI(\n",
    "    model=\"llama3.2\",\n",
    "    base_url = 'http://localhost:11434/v1',\n",
    "    api_key='ollama', # required, but unused,\n",
    "    temperature = 0\n",
    ")\n",
    "\n",
    "#Recomendable temperatura a 0 para que el LLM no sea muy creativo, vamos a tener muchas herramientas a nuestra disposici√≥n y queremos que \n",
    "#sea m√°s determinista"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa9c06f9-c044-44dd-bf80-109377294386",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Podr√≠amos establecer que tuviera memoria\n",
    "from langchain.memory import ConversationBufferMemory\n",
    "memory = ConversationBufferMemory(memory_key=\"chat_history\") #ponemos una denominada clave a la memoria \"chat_history\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9bd01227-fc55-4887-861c-1947990b8d8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.vectorstores import SKLearnVectorStore\n",
    "from langchain_openai import OpenAIEmbeddings\n",
    "from langchain.retrievers import ContextualCompressionRetriever\n",
    "from langchain.retrievers.document_compressors import LLMChainExtractor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c89e2d2b-1d9e-4f39-b9ff-75d0e38cffd1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Caragamos nuestros datos de una BD vectorial\n",
    "\n",
    "#funcion_embedding = OpenAIEmbeddings(openai_api_key=api_key)\n",
    "from langchain_ollama import OllamaEmbeddings\n",
    "\n",
    "funcion_embedding = OllamaEmbeddings(model=\"llama3.2\")\n",
    "\n",
    "persist_path=\"BD/ejemplosk_embedding_db\"\n",
    "vector_store_connection = SKLearnVectorStore(embedding=funcion_embedding, persist_path=persist_path, serializer=\"parquet\")\n",
    "compressor = LLMChainExtractor.from_llm(llm)\n",
    "compression_retriever = ContextualCompressionRetriever(base_compressor=compressor, base_retriever=vector_store_connection.as_retriever())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a9d7a1c-9ba5-4d15-8ed0-fdfe5076930e",
   "metadata": {},
   "source": [
    "Ahora vamos a definir una herramienta personalizada apoyada en la base de datos vectorial, de tal manera que el agente pueda utilizar esta herramienta que hemos definido."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f14c32bb-9b2f-4d5e-bee4-600878aafc21",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.agents import tool\n",
    "\n",
    "@tool\n",
    "def consulta_interna(text: str) -> str:\n",
    "    '''Retorna respuestas sobre la historia de Espa√±a. Se espera que la entrada sea una cadena de texto\n",
    "    y retorna una cadena con el resultado m√°s relevante. Si la respuesta con esta herramienta es relevante,\n",
    "    no debes usar ninguna herramienta m√°s ni tu propio conocimiento como LLM'''\n",
    "    compressed_docs = compression_retriever.invoke(text)\n",
    "    resultado = compressed_docs[0].page_content\n",
    "    return resultado\n",
    "\n",
    "tools = load_tools([\"wikipedia\",\"llm-math\"],llm=llm)\n",
    "tools=tools+[consulta_interna]\n",
    "\n",
    "#ahora ya tenemos nuestra herramienta"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2213f82-051d-4cc8-b10a-3c3682717d7d",
   "metadata": {},
   "source": [
    "Ahora ya estamos en condiciones de crear el agente y lo utilizamos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd7a494d-af1e-497a-b001-68999a1beaea",
   "metadata": {},
   "outputs": [],
   "source": [
    "agent = initialize_agent(tools, llm, agent=AgentType.CONVERSATIONAL_REACT_DESCRIPTION,memory=memory,verbose=True)\n",
    "# esto lo deber√≠a coger de nuestra herramienta personalizada\n",
    "agent.invoke(\"¬øQu√© periodo abarca cronol√≥gicamente en Espa√±a el siglo de oro?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5e5907a-2087-429c-b615-30bd5dfdfb9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "agent.invoke(\"¬øQu√© pas√≥ durante la misma etapa en Francia?\") #Gracias a tener memoria compara en esas fechas qu√© ocurri√≥ en Francia"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "993e0dee-d2b7-4205-a64c-3f10e1b17b6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "agent.invoke(\"¬øCu√°les son las marcas de veh√≠culos m√°s famosas hoy en d√≠a?\") #Pregunta que no podemos responder con nuestra BD Vectorial"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29c85bc7-bca6-4aea-9372-43814ece9bb6",
   "metadata": {},
   "source": [
    "*NOTA:* üôã‚Äç‚ôÇÔ∏èüòÖ Como puede verse, los resultados que obtenemos no son los que buscamos, pero ello es debido a las limitaciones computacionales con las que estamos trabajando, por lo que este m√©todo es m√°s bien did√°ctico y se expone para su conocimiento. En un mundo real, se debe utilizar un sistema computacional mucho m√°s potente para obtener resultados acorde con lo que buscamos."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f76e9dd0-fdb9-4a5b-8de8-2dff277d0775",
   "metadata": {},
   "source": [
    "## Agente para an√°lisis autom√°tico de SQL.\n",
    "\n",
    "En este apartado vamos a crear un agente que haga consultas SQL a la base de datos, pero las consultas se las pedimos en lenguaje natural, no SQL. üò≤ü§ë. En concreto lo que vamos a pedir es lo siguiente: ¬øCu√°ntas ventas ha habido en el primer trimestre del 2025?, que equivale a la siguiente consulta:\n",
    "\n",
    "```\n",
    "SELECT SUM(ventas) AS total_ventas\n",
    "FROM ventas\n",
    "WHERE fecha >= '2025_01-01' AND fecha < '2025-04-01';\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd96c354-7f1b-48b2-a884-681bc324fae8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_openai import ChatOpenAI\n",
    "from langchain.prompts import PromptTemplate, SystemMessagePromptTemplate,ChatPromptTemplate, HumanMessagePromptTemplate\n",
    "from langchain.agents import load_tools,initialize_agent,AgentType,create_react_agent,AgentExecutor\n",
    "\n",
    "llm = ChatOpenAI(\n",
    "    model=\"llama3.2\",\n",
    "    base_url = 'http://localhost:11434/v1',\n",
    "    api_key='ollama', # required, but unused,\n",
    "    temperature = 0\n",
    ")\n",
    "\n",
    "#Recomendable temperatura a 0 para que el LLM no sea muy creativo, vamos a tener muchas herramientas a nuestra disposici√≥n y queremos que \n",
    "#sea m√°s determinista"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "852e9175-7fa6-4df5-8df7-883fa6fd54cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install mysql-connector-python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6440950-7123-445d-a0ff-b96feec5f12f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import mysql.connector #pip install mysql-connector-python"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4232bec7-9b0e-49a1-ab41-d8a098416fd8",
   "metadata": {},
   "source": [
    "Para desarrollar y ejecutar este caso de uso, se neceita tener isntalado en nuestro equipo un servidor de base de datos MySql. Como no es el caso, nos vamos a limitar a mostrar el c√≥digo, ya que todos los conceptos que en √©l se desarrollan, ya se han expuestos en cap√≠tulos anteriores.\n",
    "\n",
    "```python\n",
    "f = open('../password_sql.txt')\n",
    "pass_sql = f.read()\n",
    "# Configuraci√≥n de la conexi√≥n a la base de datos\n",
    "config = {\n",
    "    'user': 'root',       \n",
    "    'password': pass_sql, \n",
    "    'host': '127.0.0.1',         \n",
    "    'database': 'world'          \n",
    "}\n",
    "\n",
    "\n",
    "# Conectar a la base de datos\n",
    "conn = mysql.connector.connect(**config)\n",
    "cursor = conn.cursor()\n",
    "\n",
    "# Definir la consulta manualmente: tengo una base de datos mysql en mi computadora local denominada \"world\" y una tabla \"Country\" \n",
    "#sobre la que quiero hacer la suma de la poblaci√≥n en la columna \"Population\" para el continente Asia (columna \"Continent\")\n",
    "query = \"\"\"\n",
    "    SELECT SUM(Population)\n",
    "    FROM Country\n",
    "    WHERE Continent = 'Asia';\n",
    "    \"\"\"\n",
    "\n",
    "# Ejecutar la consulta\n",
    "cursor.execute(query)\n",
    "result = cursor.fetchone()\n",
    "\n",
    "suma_poblacion = result[0] if result[0] is not None else 0\n",
    "print(f\"La suma de la poblaci√≥n del continente Asia es: {suma_poblacion}\")\n",
    "\n",
    "\n",
    "# Creamos el agente SQL\n",
    "\n",
    "from langchain_community.agent_toolkits import create_sql_agent\n",
    "from langchain.sql_database import SQLDatabase\n",
    "\n",
    "# Crear una cadena de conexi√≥n a la base de datos MySQL\n",
    "connection_string = f\"mysql+mysqlconnector://{config['user']}:{config['password']}@{config['host']}/{config['database']}\"\n",
    "\n",
    "# Crear una instancia de la base de datos SQL\n",
    "db = SQLDatabase.from_uri(connection_string)\n",
    "\n",
    "agent = create_sql_agent(\n",
    "    llm,\n",
    "    db=db,\n",
    "    verbose=True\n",
    ")\n",
    "\n",
    "agent.invoke(\"Dime la poblaci√≥n total de Asia\")\n",
    "\n",
    "result = agent.invoke(\"Dime el promedio de la esperanza de vida por cada una de las regiones ordenadas de mayor a menor\")\n",
    "\n",
    "# Mostrar el resultado\n",
    "print(result[\"output\"])\n",
    "\n",
    "# Para utilizar few-shoots para las consultas SQL: https://python.langchain.com/v0.1/docs/use_cases/sql/agents/\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9464c3af-bac2-4386-9dc4-e7f83222c101",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3cb218d-fe28-4b24-af53-c6532894daca",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6bf1539-f53b-49d2-9fad-cd4c75a639f6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "9decd9dd-5dae-464b-8d9f-5a382f81cc36",
   "metadata": {},
   "source": [
    "## Ap√©ndice.\n",
    "```{index} serpapi\n",
    "```\n",
    "### Serpapi.\n",
    "\n",
    "\n",
    "\n",
    "**SerpApi** es una API que permite extraer datos de los resultados de b√∫squeda de Google y otros motores de b√∫squeda sin necesidad de realizar scraping manualmente. Facilita la obtenci√≥n de resultados de Google Search, Google Images, Google News, Google Maps, Google Shopping, YouTube y m√°s, de una manera estructurada y libre de bloqueos.  \n",
    "\n",
    "#### **¬øPor qu√© usar SerpApi?**  \n",
    "1. **Evita bloqueos**: Google implementa restricciones y CAPTCHA para prevenir el scraping, pero SerpApi maneja esto autom√°ticamente.  \n",
    "2. **Datos estructurados**: Los resultados se devuelven en formato JSON, lo que facilita su procesamiento.  \n",
    "3. **Compatibilidad con m√∫ltiples motores de b√∫squeda**: Adem√°s de Google, soporta Bing, DuckDuckGo, Yahoo, entre otros.  \n",
    "4. **Alta velocidad y escalabilidad**: Permite realizar m√∫ltiples solicitudes de b√∫squeda de forma eficiente.  \n",
    "\n",
    "#### **¬øC√≥mo funciona?**  \n",
    "Para usar SerpApi, necesitas:  \n",
    "1. **Crear una cuenta** en [SerpApi](https://serpapi.com/).  \n",
    "2. **Obtener una API Key** para autenticar solicitudes.  \n",
    "3. **Realizar peticiones HTTP** al endpoint de b√∫squeda con los par√°metros deseados.  \n",
    "\n",
    "#### **Ejemplo en Python**  \n",
    "Aqu√≠ tienes un ejemplo de c√≥mo hacer una b√∫squeda en Google usando la API de SerpApi con Python:  \n",
    "\n",
    "```python\n",
    "import requests\n",
    "\n",
    "params = {\n",
    "    \"q\": \"ChatGPT\",\n",
    "    \"hl\": \"es\",\n",
    "    \"gl\": \"es\",\n",
    "    \"api_key\": \"TU_API_KEY\"\n",
    "}\n",
    "\n",
    "response = requests.get(\"https://serpapi.com/search\", params=params)\n",
    "data = response.json()\n",
    "\n",
    "print(data)\n",
    "```\n",
    "Este c√≥digo devuelve los resultados de b√∫squeda de Google en JSON.  \n",
    "\n",
    "#### **Casos de uso**  \n",
    "- Monitoreo de rankings en SEO.  \n",
    "- Seguimiento de precios en Google Shopping.  \n",
    "- Extracci√≥n de datos de Google Maps para negocios locales.  \n",
    "- Automatizaci√≥n de investigaciones en Google News.\n",
    "\n",
    "**NOTA:** üëç üëå Esta herramienta es de pago pero en el momento de redactar estas l√≠neas, existe una versi√≥n gratuita que permite hacer 100 b√∫squedas al mes sin tener que pagar.\n",
    "\n",
    "* <a href=\"https://serpapi.com/\" target=\"_blank\"> P√°gina oficial de serpapi </a>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee2409b7-538a-40a9-9db7-d2e7882987b4",
   "metadata": {},
   "source": [
    "### Art√≠culo muy interesante.\n",
    "```{index} Agentes\n",
    "```\n",
    "\n",
    "A continuaci√≥n se indica un enlace para ver un art√≠culo muy interesante sobre los agentes de LangChain\n",
    "\n",
    "* <a href=\"https://www.datacamp.com/es/tutorial/building-langchain-agents-to-automate-tasks-in-python\" target=\"_blank\"> Trabajar con Agentes en LangChain </a>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
