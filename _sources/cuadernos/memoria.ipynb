{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ebdad4fc-44dc-47a4-b9c3-1c2cef1f9628",
   "metadata": {},
   "source": [
    "# Introducción a la memoria con LangChain.\n",
    "\n",
    "En este apartado vamos a ver los siguientes puntos:\n",
    "\n",
    "*¿Cómo almacenar las conversaciones humano AI?\n",
    "\n",
    "* ¿Cómo guardar en un objeto de memoria las conversación con el LLM?\n",
    "* \n",
    "* ¿Cómo limitar o resumir lo que guardamos en el objeto de memoria?\n",
    "\n",
    "## Tipos de memoria en LangChain\n",
    "\n",
    "Cuando LangChain se refiere al término \" memoria \", generalmente se refiere a realizar un seguimiento del historial de interacción de mensajes.\n",
    "\n",
    "Los diferentes tipos de memoria pueden ser:\n",
    "\n",
    "* **ChatMessageHistory** : Guardar el histórico de mensajes de un chat con los métodos add_user_message y add_ai_message , debemos guardarlo manualmente tras cada\n",
    "\n",
    "*  **ConversationBufferMemory** : A partir de una cadena de tipo ConversationalChain , guardamos en un objeto de\n",
    "memoria todos los mensajes de la conversación.\n",
    "\n",
    "\n",
    "* **ConversationBufferWindowMemory** : Igual que el anterior pero podemos especificar una ventana de k interacciones (las últimas k) a guardar en lugar de todos los mensajes.\n",
    "\n",
    "* **ConversationSummaryMemory** : En lugar de guardar los mensajes literalmente, se realiza un resumen de la memoria en base a todo el historia, con ello reducimos drásticamente el tamaño de la memoria para conversaciones muy largas.\n",
    "\n",
    "Veamos un caso de uso con el primero de los tipos presentado."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "66f8028e-993f-4f0d-8640-c3ae6800d343",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_openai import ChatOpenAI\n",
    "from langchain.schema import SystemMessage, HumanMessage\n",
    "\n",
    "chat = ChatOpenAI(\n",
    "    model=\"llama3.2\",\n",
    "    base_url = 'http://localhost:11434/v1',\n",
    "    api_key='ollama', # required, but unused,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b3a82f5d-70c7-4a17-b168-2d1f8f8b5de7",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Definimos el objeto de histórico de mensajes\n",
    "from langchain.memory import ChatMessageHistory\n",
    "\n",
    "history = ChatMessageHistory()\n",
    "\n",
    "consulta = \"Hola, ¿cómo estás? Necesito ayudar para reconfigurar el router\"\n",
    "\n",
    "#Vamos guardando en el objeto \"history\" los mensajes de usuario y los mensajes AI que queramos\n",
    "history.add_user_message(consulta)\n",
    "\n",
    "resultado = chat.invoke([HumanMessage(content=consulta)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "99e5a066-9a36-4f1b-88bf-5a96b1b303c3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "InMemoryChatMessageHistory(messages=[HumanMessage(content='Hola, ¿cómo estás? Necesito ayudar para reconfigurar el router', additional_kwargs={}, response_metadata={}), AIMessage(content='¡Hola! Estoy aquí para ayudarte. Me alegra que hayas recurrido a mí en este momento de necesidad.\\n\\nEstoy listo para asistirte con la reconfiqueración de tu router. Antes de comenzar, tengo algunas preguntas para asegurarme de que todo salga bien:\\n\\n* ¿Cuál es el modelo de tu router?\\n* ¿Qué problema estás experimentando actualmente (por ejemplo, conexión lenta, desconexiones frecuentes, error de configuración)?\\n* Has realizado alguna vez una reconfiqueración antes?\\n* Tienes acceso al manual del router o la documentación oficial?\\n\\nCon esta información, podré ofrecerte consejos personalizados y orientarte en el proceso de reconfiqueración. ¡Vamos por ello!', additional_kwargs={}, response_metadata={})])"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "history.add_ai_message(resultado.content)\n",
    "\n",
    "history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d3b0b83b-9b4c-4044-b7c7-528baa59cdf2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[HumanMessage(content='Hola, ¿cómo estás? Necesito ayudar para reconfigurar el router', additional_kwargs={}, response_metadata={}),\n",
       " AIMessage(content='¡Hola! Estoy aquí para ayudarte. Me alegra que hayas recurrido a mí en este momento de necesidad.\\n\\nEstoy listo para asistirte con la reconfiqueración de tu router. Antes de comenzar, tengo algunas preguntas para asegurarme de que todo salga bien:\\n\\n* ¿Cuál es el modelo de tu router?\\n* ¿Qué problema estás experimentando actualmente (por ejemplo, conexión lenta, desconexiones frecuentes, error de configuración)?\\n* Has realizado alguna vez una reconfiqueración antes?\\n* Tienes acceso al manual del router o la documentación oficial?\\n\\nCon esta información, podré ofrecerte consejos personalizados y orientarte en el proceso de reconfiqueración. ¡Vamos por ello!', additional_kwargs={}, response_metadata={})]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "history.messages"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12bd81cb-4908-46c1-91ba-8ebc1d4c7437",
   "metadata": {},
   "source": [
    "## Buffer de memoria completa.\n",
    "\n",
    "Con este procedimiento, lo que hacemos es ir guardando todos los mensajes. Veamoslo de forma práctica con un ejemplo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "023e4779-568e-4aad-8fa1-e52eba36631e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chains import ConversationChain\n",
    "from langchain.memory import ConversationBufferMemory\n",
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "llm = ChatOpenAI(\n",
    "    model=\"llama3.2\",\n",
    "    base_url = 'http://localhost:11434/v1',\n",
    "    api_key='ollama', # required, but unused,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "bd034209-3cae-4d2a-b42c-f11bf3bd0647",
   "metadata": {},
   "outputs": [],
   "source": [
    "# creamos el objeto de conversación\n",
    "memory = ConversationBufferMemory()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "b0cf7f1f-9c43-4496-b9d0-d84c80fdfd3e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Francisco\\AppData\\Local\\Temp\\ipykernel_23152\\3004879512.py:2: LangChainDeprecationWarning: The class `ConversationChain` was deprecated in LangChain 0.2.7 and will be removed in 1.0. Use :meth:`~RunnableWithMessageHistory: https://python.langchain.com/v0.2/api_reference/core/runnables/langchain_core.runnables.history.RunnableWithMessageHistory.html` instead.\n",
      "  conversation = ConversationChain(llm=llm,memory = memory,verbose=True)\n"
     ]
    }
   ],
   "source": [
    "#Creamos una instancia de la cadena conversacional con el LLM y el objeto de memoria\n",
    "conversation = ConversationChain(llm=llm,memory = memory,verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c408cd7-ebd6-4906-a8d3-dd8ff80c7998",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Ejemplo con RunnableWithMessageHistory: https://api.python.langchain.com/en/latest/runnables/langchain_core.runnables.history.RunnableWithMessageHistory.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "4c606e20-9bd8-4427-87b2-bb8fd36a8caa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new ConversationChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mThe following is a friendly conversation between a human and an AI. The AI is talkative and provides lots of specific details from its context. If the AI does not know the answer to a question, it truthfully says it does not know.\n",
      "\n",
      "Current conversation:\n",
      "\n",
      "Human: Hola, necesito saber cómo usar mis datos históricos para crear un bot de preguntas y respuestas\n",
      "AI:\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'¡Hola! Me alegra poder ayudarte. Para crear un bot de preguntas y respuestas utilizando tus datos históricos, tenemos varias opciones que puedes considerar.\\n\\nSi estás trabajando con una plataforma de conversación como yo, como Rasa y Stanford CoreNLP, podrías utilizar las APIs de respuesta a preguntas para generar preguntas y respuestas basadas en tus datos históricos. Por ejemplo, podrías utilizar la API de Rasa para crear un modelado de respuesta que te permita elegir la pregunta más relevante según el contexto en el que se realice la conversación.\\n\\nOtra opción es utilizar técnicas de procesamiento del lenguaje natural (NLP) como spaCy y NLTK para analizar tus datos históricos y extracción relevantes. Esto te permitirá crear un modelo de lenguaje con capacidades predecibles según el estilo, contexto y contenido de las respuestas proporcionadas.\\n\\nSi buscas implementar la respuesta más rápida posible del bot, podrías utilizar técnicas de procesamiento de código fuente en Python como requests y BeautifulSoup para acceder a tus datos históricos.\\nPodemos explorar estas opciones en mayor detalle. ¿Cuál es tu pregunta específica?'"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Lanzamos el primer prompt (human message)\n",
    "conversation.predict(input=\"Hola, necesito saber cómo usar mis datos históricos para crear un bot de preguntas y respuestas\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "255e182c-6feb-4074-abec-88ba6c14237e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new ConversationChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mThe following is a friendly conversation between a human and an AI. The AI is talkative and provides lots of specific details from its context. If the AI does not know the answer to a question, it truthfully says it does not know.\n",
      "\n",
      "Current conversation:\n",
      "Human: Hola, necesito saber cómo usar mis datos históricos para crear un bot de preguntas y respuestas\n",
      "AI: ¡Hola! Me alegra poder ayudarte. Para crear un bot de preguntas y respuestas utilizando tus datos históricos, tenemos varias opciones que puedes considerar.\n",
      "\n",
      "Si estás trabajando con una plataforma de conversación como yo, como Rasa y Stanford CoreNLP, podrías utilizar las APIs de respuesta a preguntas para generar preguntas y respuestas basadas en tus datos históricos. Por ejemplo, podrías utilizar la API de Rasa para crear un modelado de respuesta que te permita elegir la pregunta más relevante según el contexto en el que se realice la conversación.\n",
      "\n",
      "Otra opción es utilizar técnicas de procesamiento del lenguaje natural (NLP) como spaCy y NLTK para analizar tus datos históricos y extracción relevantes. Esto te permitirá crear un modelo de lenguaje con capacidades predecibles según el estilo, contexto y contenido de las respuestas proporcionadas.\n",
      "\n",
      "Si buscas implementar la respuesta más rápida posible del bot, podrías utilizar técnicas de procesamiento de código fuente en Python como requests y BeautifulSoup para acceder a tus datos históricos.\n",
      "Podemos explorar estas opciones en mayor detalle. ¿Cuál es tu pregunta específica?\n",
      "Human: Necesito más detalle de cómo implementarlo\n",
      "AI:\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'¡Claro! Vamos a profundizar un poco más en cada una de las opciones que mencioné anteriormente para que puedas decidir la mejor manera de implementar el bot de preguntas y respuestas.\\n\\n**Opción 1: Utilizando APIs de respuesta a preguntas**\\n\\nUna vez que hayas seleccionado una API, podrás empezar a personalizar tu modelo de respuesta. Por ejemplo, si utilizas la API de Rasa, podrías configurar diferentes entrenamientos y ajustes para optimizar el desempeño del modelo.\\n\\nAquí te proporciono un ejemplo básico en Python de cómo acceder a las APIs de respuesta a preguntas:\\n\\n```python\\nimport logging\\nfrom rasa import Client\\nfrom rasa.exceptions import RasaError\\n\\n# Configura la conexión con la API\\nclient = Client(\\'https://api.rasa.com/v1\\', \\'tu_api_key\\')\\n\\ndef crear_pregunta_resposta():\\n    try:\\n        # Carga los datos históricos en el modelo de respuesta\\n        client.train(\\n            data=[\\n                {\\n                    \"intent\": \"greeting\",\\n                    \"Examples\": [\\n                        {\"text\": \"Hola, ¿cómo estás?\", \"intent\": \"greeting\"}\\n                    ]\\n                },\\n                {\\n                    \"intent\": \"goodbye\",\\n                    \"Examples\": [\\n                        {\"text\": \"Adiós, ¿de dónde eres?\", \"intent\": \"goodbye\"}\\n                    ]\\n                }\\n            ]\\n        )\\n\\n    except RasaError as e:\\n        logging.error(f\\'Error al entrenar el modelo de respuesta:\\', e)\\n```\\n\\n**Opción 2: Utilizando técnicas de procesamiento del lenguaje natural (NLP)**\\n\\nSi prefieres un enfoque más manual y te gusta la programación, podrías utilizar bibliotecas como spaCy y NLTK para analizar tus datos históricos.\\n\\nAquí te doy un ejemplo rápido en Python de cómo crear un modelo de lenguaje con spaCy:\\n\\n```python\\nimport spacy\\n\\n# Carga el modelo de NLP\\nnlp = spacy.load(\"es_core_news_sm\")\\n\\ndef generar ModelaDeLenguaje(dataHistorica):\\n    doc = nlp(text=\\'Los datos históricos\\')\\n    entities = doc.ents\\n    return(entities)\\n```\\n\\n**Opción 3: Utilizando técnicas de procesamiento de código fuente en Python**\\n\\nSi buscas la velocidad más rápida posible, podrías utilizar bibliotecas como requests y BeautifulSoup para acceder a tus datos históricos.\\n\\nAquí te doy un ejemplo rápido en Python de cómo crear un bot de preguntas y respuestas:\\n\\n```python\\nimport requests\\n\\ndef obtener DatosHistoricos(url):\\n    respuesta = requests.get(url)\\n    return respuesta.content\\n```\\n\\n¿Te gustaría saber más sobre alguna de las opciones o ¿necesitas que discuta algun detalle?'"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Lanzamos el segundo prompt (human message)\n",
    "conversation.predict(input=\"Necesito más detalle de cómo implementarlo\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "52404ee4-0a83-4b48-bf8d-67f7127d5497",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Human: Hola, necesito saber cómo usar mis datos históricos para crear un bot de preguntas y respuestas\n",
      "AI: ¡Hola! Me alegra poder ayudarte. Para crear un bot de preguntas y respuestas utilizando tus datos históricos, tenemos varias opciones que puedes considerar.\n",
      "\n",
      "Si estás trabajando con una plataforma de conversación como yo, como Rasa y Stanford CoreNLP, podrías utilizar las APIs de respuesta a preguntas para generar preguntas y respuestas basadas en tus datos históricos. Por ejemplo, podrías utilizar la API de Rasa para crear un modelado de respuesta que te permita elegir la pregunta más relevante según el contexto en el que se realice la conversación.\n",
      "\n",
      "Otra opción es utilizar técnicas de procesamiento del lenguaje natural (NLP) como spaCy y NLTK para analizar tus datos históricos y extracción relevantes. Esto te permitirá crear un modelo de lenguaje con capacidades predecibles según el estilo, contexto y contenido de las respuestas proporcionadas.\n",
      "\n",
      "Si buscas implementar la respuesta más rápida posible del bot, podrías utilizar técnicas de procesamiento de código fuente en Python como requests y BeautifulSoup para acceder a tus datos históricos.\n",
      "Podemos explorar estas opciones en mayor detalle. ¿Cuál es tu pregunta específica?\n",
      "Human: Necesito más detalle de cómo implementarlo\n",
      "AI: ¡Claro! Vamos a profundizar un poco más en cada una de las opciones que mencioné anteriormente para que puedas decidir la mejor manera de implementar el bot de preguntas y respuestas.\n",
      "\n",
      "**Opción 1: Utilizando APIs de respuesta a preguntas**\n",
      "\n",
      "Una vez que hayas seleccionado una API, podrás empezar a personalizar tu modelo de respuesta. Por ejemplo, si utilizas la API de Rasa, podrías configurar diferentes entrenamientos y ajustes para optimizar el desempeño del modelo.\n",
      "\n",
      "Aquí te proporciono un ejemplo básico en Python de cómo acceder a las APIs de respuesta a preguntas:\n",
      "\n",
      "```python\n",
      "import logging\n",
      "from rasa import Client\n",
      "from rasa.exceptions import RasaError\n",
      "\n",
      "# Configura la conexión con la API\n",
      "client = Client('https://api.rasa.com/v1', 'tu_api_key')\n",
      "\n",
      "def crear_pregunta_resposta():\n",
      "    try:\n",
      "        # Carga los datos históricos en el modelo de respuesta\n",
      "        client.train(\n",
      "            data=[\n",
      "                {\n",
      "                    \"intent\": \"greeting\",\n",
      "                    \"Examples\": [\n",
      "                        {\"text\": \"Hola, ¿cómo estás?\", \"intent\": \"greeting\"}\n",
      "                    ]\n",
      "                },\n",
      "                {\n",
      "                    \"intent\": \"goodbye\",\n",
      "                    \"Examples\": [\n",
      "                        {\"text\": \"Adiós, ¿de dónde eres?\", \"intent\": \"goodbye\"}\n",
      "                    ]\n",
      "                }\n",
      "            ]\n",
      "        )\n",
      "\n",
      "    except RasaError as e:\n",
      "        logging.error(f'Error al entrenar el modelo de respuesta:', e)\n",
      "```\n",
      "\n",
      "**Opción 2: Utilizando técnicas de procesamiento del lenguaje natural (NLP)**\n",
      "\n",
      "Si prefieres un enfoque más manual y te gusta la programación, podrías utilizar bibliotecas como spaCy y NLTK para analizar tus datos históricos.\n",
      "\n",
      "Aquí te doy un ejemplo rápido en Python de cómo crear un modelo de lenguaje con spaCy:\n",
      "\n",
      "```python\n",
      "import spacy\n",
      "\n",
      "# Carga el modelo de NLP\n",
      "nlp = spacy.load(\"es_core_news_sm\")\n",
      "\n",
      "def generar ModelaDeLenguaje(dataHistorica):\n",
      "    doc = nlp(text='Los datos históricos')\n",
      "    entities = doc.ents\n",
      "    return(entities)\n",
      "```\n",
      "\n",
      "**Opción 3: Utilizando técnicas de procesamiento de código fuente en Python**\n",
      "\n",
      "Si buscas la velocidad más rápida posible, podrías utilizar bibliotecas como requests y BeautifulSoup para acceder a tus datos históricos.\n",
      "\n",
      "Aquí te doy un ejemplo rápido en Python de cómo crear un bot de preguntas y respuestas:\n",
      "\n",
      "```python\n",
      "import requests\n",
      "\n",
      "def obtener DatosHistoricos(url):\n",
      "    respuesta = requests.get(url)\n",
      "    return respuesta.content\n",
      "```\n",
      "\n",
      "¿Te gustaría saber más sobre alguna de las opciones o ¿necesitas que discuta algun detalle?\n"
     ]
    }
   ],
   "source": [
    "#Obtenemos el histórico\n",
    "print(memory.buffer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "97c2ba90-ca97-439a-a7db-20c2afeb6b93",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'history': 'Human: Hola, necesito saber cómo usar mis datos históricos para crear un bot de preguntas y respuestas\\nAI: ¡Hola! Me alegra poder ayudarte. Para crear un bot de preguntas y respuestas utilizando tus datos históricos, tenemos varias opciones que puedes considerar.\\n\\nSi estás trabajando con una plataforma de conversación como yo, como Rasa y Stanford CoreNLP, podrías utilizar las APIs de respuesta a preguntas para generar preguntas y respuestas basadas en tus datos históricos. Por ejemplo, podrías utilizar la API de Rasa para crear un modelado de respuesta que te permita elegir la pregunta más relevante según el contexto en el que se realice la conversación.\\n\\nOtra opción es utilizar técnicas de procesamiento del lenguaje natural (NLP) como spaCy y NLTK para analizar tus datos históricos y extracción relevantes. Esto te permitirá crear un modelo de lenguaje con capacidades predecibles según el estilo, contexto y contenido de las respuestas proporcionadas.\\n\\nSi buscas implementar la respuesta más rápida posible del bot, podrías utilizar técnicas de procesamiento de código fuente en Python como requests y BeautifulSoup para acceder a tus datos históricos.\\nPodemos explorar estas opciones en mayor detalle. ¿Cuál es tu pregunta específica?\\nHuman: Necesito más detalle de cómo implementarlo\\nAI: ¡Claro! Vamos a profundizar un poco más en cada una de las opciones que mencioné anteriormente para que puedas decidir la mejor manera de implementar el bot de preguntas y respuestas.\\n\\n**Opción 1: Utilizando APIs de respuesta a preguntas**\\n\\nUna vez que hayas seleccionado una API, podrás empezar a personalizar tu modelo de respuesta. Por ejemplo, si utilizas la API de Rasa, podrías configurar diferentes entrenamientos y ajustes para optimizar el desempeño del modelo.\\n\\nAquí te proporciono un ejemplo básico en Python de cómo acceder a las APIs de respuesta a preguntas:\\n\\n```python\\nimport logging\\nfrom rasa import Client\\nfrom rasa.exceptions import RasaError\\n\\n# Configura la conexión con la API\\nclient = Client(\\'https://api.rasa.com/v1\\', \\'tu_api_key\\')\\n\\ndef crear_pregunta_resposta():\\n    try:\\n        # Carga los datos históricos en el modelo de respuesta\\n        client.train(\\n            data=[\\n                {\\n                    \"intent\": \"greeting\",\\n                    \"Examples\": [\\n                        {\"text\": \"Hola, ¿cómo estás?\", \"intent\": \"greeting\"}\\n                    ]\\n                },\\n                {\\n                    \"intent\": \"goodbye\",\\n                    \"Examples\": [\\n                        {\"text\": \"Adiós, ¿de dónde eres?\", \"intent\": \"goodbye\"}\\n                    ]\\n                }\\n            ]\\n        )\\n\\n    except RasaError as e:\\n        logging.error(f\\'Error al entrenar el modelo de respuesta:\\', e)\\n```\\n\\n**Opción 2: Utilizando técnicas de procesamiento del lenguaje natural (NLP)**\\n\\nSi prefieres un enfoque más manual y te gusta la programación, podrías utilizar bibliotecas como spaCy y NLTK para analizar tus datos históricos.\\n\\nAquí te doy un ejemplo rápido en Python de cómo crear un modelo de lenguaje con spaCy:\\n\\n```python\\nimport spacy\\n\\n# Carga el modelo de NLP\\nnlp = spacy.load(\"es_core_news_sm\")\\n\\ndef generar ModelaDeLenguaje(dataHistorica):\\n    doc = nlp(text=\\'Los datos históricos\\')\\n    entities = doc.ents\\n    return(entities)\\n```\\n\\n**Opción 3: Utilizando técnicas de procesamiento de código fuente en Python**\\n\\nSi buscas la velocidad más rápida posible, podrías utilizar bibliotecas como requests y BeautifulSoup para acceder a tus datos históricos.\\n\\nAquí te doy un ejemplo rápido en Python de cómo crear un bot de preguntas y respuestas:\\n\\n```python\\nimport requests\\n\\ndef obtener DatosHistoricos(url):\\n    respuesta = requests.get(url)\\n    return respuesta.content\\n```\\n\\n¿Te gustaría saber más sobre alguna de las opciones o ¿necesitas que discuta algun detalle?'}"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Cargamos la variable de memoria\n",
    "memory.load_memory_variables({})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86c9db6a-8da6-4411-a739-a935fb66a28c",
   "metadata": {},
   "source": [
    "## Guardar y Cargar la memoria (posterior uso)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "41b521c3-f152-49a2-b2da-93f1021a527c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ConversationBufferMemory(chat_memory=InMemoryChatMessageHistory(messages=[HumanMessage(content='Hola, necesito saber cómo usar mis datos históricos para crear un bot de preguntas y respuestas', additional_kwargs={}, response_metadata={}), AIMessage(content='¡Hola! Me alegra poder ayudarte. Para crear un bot de preguntas y respuestas utilizando tus datos históricos, tenemos varias opciones que puedes considerar.\\n\\nSi estás trabajando con una plataforma de conversación como yo, como Rasa y Stanford CoreNLP, podrías utilizar las APIs de respuesta a preguntas para generar preguntas y respuestas basadas en tus datos históricos. Por ejemplo, podrías utilizar la API de Rasa para crear un modelado de respuesta que te permita elegir la pregunta más relevante según el contexto en el que se realice la conversación.\\n\\nOtra opción es utilizar técnicas de procesamiento del lenguaje natural (NLP) como spaCy y NLTK para analizar tus datos históricos y extracción relevantes. Esto te permitirá crear un modelo de lenguaje con capacidades predecibles según el estilo, contexto y contenido de las respuestas proporcionadas.\\n\\nSi buscas implementar la respuesta más rápida posible del bot, podrías utilizar técnicas de procesamiento de código fuente en Python como requests y BeautifulSoup para acceder a tus datos históricos.\\nPodemos explorar estas opciones en mayor detalle. ¿Cuál es tu pregunta específica?', additional_kwargs={}, response_metadata={}), HumanMessage(content='Necesito más detalle de cómo implementarlo', additional_kwargs={}, response_metadata={}), AIMessage(content='¡Claro! Vamos a profundizar un poco más en cada una de las opciones que mencioné anteriormente para que puedas decidir la mejor manera de implementar el bot de preguntas y respuestas.\\n\\n**Opción 1: Utilizando APIs de respuesta a preguntas**\\n\\nUna vez que hayas seleccionado una API, podrás empezar a personalizar tu modelo de respuesta. Por ejemplo, si utilizas la API de Rasa, podrías configurar diferentes entrenamientos y ajustes para optimizar el desempeño del modelo.\\n\\nAquí te proporciono un ejemplo básico en Python de cómo acceder a las APIs de respuesta a preguntas:\\n\\n```python\\nimport logging\\nfrom rasa import Client\\nfrom rasa.exceptions import RasaError\\n\\n# Configura la conexión con la API\\nclient = Client(\\'https://api.rasa.com/v1\\', \\'tu_api_key\\')\\n\\ndef crear_pregunta_resposta():\\n    try:\\n        # Carga los datos históricos en el modelo de respuesta\\n        client.train(\\n            data=[\\n                {\\n                    \"intent\": \"greeting\",\\n                    \"Examples\": [\\n                        {\"text\": \"Hola, ¿cómo estás?\", \"intent\": \"greeting\"}\\n                    ]\\n                },\\n                {\\n                    \"intent\": \"goodbye\",\\n                    \"Examples\": [\\n                        {\"text\": \"Adiós, ¿de dónde eres?\", \"intent\": \"goodbye\"}\\n                    ]\\n                }\\n            ]\\n        )\\n\\n    except RasaError as e:\\n        logging.error(f\\'Error al entrenar el modelo de respuesta:\\', e)\\n```\\n\\n**Opción 2: Utilizando técnicas de procesamiento del lenguaje natural (NLP)**\\n\\nSi prefieres un enfoque más manual y te gusta la programación, podrías utilizar bibliotecas como spaCy y NLTK para analizar tus datos históricos.\\n\\nAquí te doy un ejemplo rápido en Python de cómo crear un modelo de lenguaje con spaCy:\\n\\n```python\\nimport spacy\\n\\n# Carga el modelo de NLP\\nnlp = spacy.load(\"es_core_news_sm\")\\n\\ndef generar ModelaDeLenguaje(dataHistorica):\\n    doc = nlp(text=\\'Los datos históricos\\')\\n    entities = doc.ents\\n    return(entities)\\n```\\n\\n**Opción 3: Utilizando técnicas de procesamiento de código fuente en Python**\\n\\nSi buscas la velocidad más rápida posible, podrías utilizar bibliotecas como requests y BeautifulSoup para acceder a tus datos históricos.\\n\\nAquí te doy un ejemplo rápido en Python de cómo crear un bot de preguntas y respuestas:\\n\\n```python\\nimport requests\\n\\ndef obtener DatosHistoricos(url):\\n    respuesta = requests.get(url)\\n    return respuesta.content\\n```\\n\\n¿Te gustaría saber más sobre alguna de las opciones o ¿necesitas que discuta algun detalle?', additional_kwargs={}, response_metadata={})]))"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "conversation.memory"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a20f906-8b75-447d-a3ca-9446c5816c19",
   "metadata": {},
   "source": [
    "Si queremos guardar de forma permanente este objeto lo mejor es trabajar con la librería pickle.\n",
    "```{index} picle\n",
    "```\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "c0a5cce9-8e36-461b-9879-83cb9a140a13",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "pickled_str = pickle.dumps(conversation.memory) #Crea un objeto binario con todo el objeto de la memoria"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "3a2fb800-eead-44d3-ba59-9500fe0cd23e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# para recuperarlo\n",
    "with open('memory.pkl','wb') as f: #wb para indicar que escriba un objeto binario, en este caso en la misma ruta que el script\n",
    "    f.write(pickled_str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "a23645af-a1cc-4baf-bd13-f3dc205e953d",
   "metadata": {},
   "outputs": [],
   "source": [
    "memoria_cargada = open('memory.pkl','rb').read() #rb para indicar que leemos el objeto binario"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "0e3d6390-ee16-40de-ae8c-ded1750aa250",
   "metadata": {},
   "outputs": [],
   "source": [
    "llm = ChatOpenAI(\n",
    "    model=\"llama3.2\",\n",
    "    base_url = 'http://localhost:11434/v1',\n",
    "    api_key='ollama', # required, but unused,\n",
    ") #Creamos una nueva instancia de LLM para asegurar que está totalmente limpia\n",
    "\n",
    "\n",
    "conversacion_recargada = ConversationChain(\n",
    "    llm=llm, \n",
    "    memory = pickle.loads(memoria_cargada),\n",
    "    verbose=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "acc6d6de-4842-4c14-becf-c2feecec71e3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Human: Hola, necesito saber cómo usar mis datos históricos para crear un bot de preguntas y respuestas\\nAI: ¡Hola! Me alegra poder ayudarte. Para crear un bot de preguntas y respuestas utilizando tus datos históricos, tenemos varias opciones que puedes considerar.\\n\\nSi estás trabajando con una plataforma de conversación como yo, como Rasa y Stanford CoreNLP, podrías utilizar las APIs de respuesta a preguntas para generar preguntas y respuestas basadas en tus datos históricos. Por ejemplo, podrías utilizar la API de Rasa para crear un modelado de respuesta que te permita elegir la pregunta más relevante según el contexto en el que se realice la conversación.\\n\\nOtra opción es utilizar técnicas de procesamiento del lenguaje natural (NLP) como spaCy y NLTK para analizar tus datos históricos y extracción relevantes. Esto te permitirá crear un modelo de lenguaje con capacidades predecibles según el estilo, contexto y contenido de las respuestas proporcionadas.\\n\\nSi buscas implementar la respuesta más rápida posible del bot, podrías utilizar técnicas de procesamiento de código fuente en Python como requests y BeautifulSoup para acceder a tus datos históricos.\\nPodemos explorar estas opciones en mayor detalle. ¿Cuál es tu pregunta específica?\\nHuman: Necesito más detalle de cómo implementarlo\\nAI: ¡Claro! Vamos a profundizar un poco más en cada una de las opciones que mencioné anteriormente para que puedas decidir la mejor manera de implementar el bot de preguntas y respuestas.\\n\\n**Opción 1: Utilizando APIs de respuesta a preguntas**\\n\\nUna vez que hayas seleccionado una API, podrás empezar a personalizar tu modelo de respuesta. Por ejemplo, si utilizas la API de Rasa, podrías configurar diferentes entrenamientos y ajustes para optimizar el desempeño del modelo.\\n\\nAquí te proporciono un ejemplo básico en Python de cómo acceder a las APIs de respuesta a preguntas:\\n\\n```python\\nimport logging\\nfrom rasa import Client\\nfrom rasa.exceptions import RasaError\\n\\n# Configura la conexión con la API\\nclient = Client(\\'https://api.rasa.com/v1\\', \\'tu_api_key\\')\\n\\ndef crear_pregunta_resposta():\\n    try:\\n        # Carga los datos históricos en el modelo de respuesta\\n        client.train(\\n            data=[\\n                {\\n                    \"intent\": \"greeting\",\\n                    \"Examples\": [\\n                        {\"text\": \"Hola, ¿cómo estás?\", \"intent\": \"greeting\"}\\n                    ]\\n                },\\n                {\\n                    \"intent\": \"goodbye\",\\n                    \"Examples\": [\\n                        {\"text\": \"Adiós, ¿de dónde eres?\", \"intent\": \"goodbye\"}\\n                    ]\\n                }\\n            ]\\n        )\\n\\n    except RasaError as e:\\n        logging.error(f\\'Error al entrenar el modelo de respuesta:\\', e)\\n```\\n\\n**Opción 2: Utilizando técnicas de procesamiento del lenguaje natural (NLP)**\\n\\nSi prefieres un enfoque más manual y te gusta la programación, podrías utilizar bibliotecas como spaCy y NLTK para analizar tus datos históricos.\\n\\nAquí te doy un ejemplo rápido en Python de cómo crear un modelo de lenguaje con spaCy:\\n\\n```python\\nimport spacy\\n\\n# Carga el modelo de NLP\\nnlp = spacy.load(\"es_core_news_sm\")\\n\\ndef generar ModelaDeLenguaje(dataHistorica):\\n    doc = nlp(text=\\'Los datos históricos\\')\\n    entities = doc.ents\\n    return(entities)\\n```\\n\\n**Opción 3: Utilizando técnicas de procesamiento de código fuente en Python**\\n\\nSi buscas la velocidad más rápida posible, podrías utilizar bibliotecas como requests y BeautifulSoup para acceder a tus datos históricos.\\n\\nAquí te doy un ejemplo rápido en Python de cómo crear un bot de preguntas y respuestas:\\n\\n```python\\nimport requests\\n\\ndef obtener DatosHistoricos(url):\\n    respuesta = requests.get(url)\\n    return respuesta.content\\n```\\n\\n¿Te gustaría saber más sobre alguna de las opciones o ¿necesitas que discuta algun detalle?'"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "conversacion_recargada.memory.buffer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5737dd0e-4c02-4cec-b0d5-9289a5f7dfb1",
   "metadata": {},
   "source": [
    "## Memoria de una venta temporal.\n",
    "\n",
    "En este caso se guardan las últimas k iteraciones que se hayan hecho en el chat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "d9a0ffe6-fe6d-4c00-8b74-bf19042a5f6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chains import ConversationChain\n",
    "from langchain.memory import ConversationBufferMemory, ConversationBufferWindowMemory\n",
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "llm = ChatOpenAI(\n",
    "    model=\"llama3.2\",\n",
    "    base_url = 'http://localhost:11434/v1',\n",
    "    api_key='ollama', # required, but unused,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "bf8a1d26-6eed-48f5-acb2-e082298ae481",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Francisco\\AppData\\Local\\Temp\\ipykernel_23152\\3793591208.py:3: LangChainDeprecationWarning: Please see the migration guide at: https://python.langchain.com/docs/versions/migrating_memory/\n",
      "  memory = ConversationBufferWindowMemory(k=1) #k indica el número de iteraciones (pareja de mensajes human-AI) que guardar\n"
     ]
    }
   ],
   "source": [
    "# creamos un objeto ConversationBufferWindowMemory\n",
    "\n",
    "memory = ConversationBufferWindowMemory(k=1) #k indica el número de iteraciones (pareja de mensajes human-AI) que guardar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "6e2b38d5-2220-45d1-a68f-8b58ec0183c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Creamos una instancia de la cadena conversacional con el LLM y el objeto de memoria\n",
    "conversation = ConversationChain(llm=llm,memory = memory,verbose=True)\n",
    "#Ejemplo con RunnableWithMessageHistory: https://api.python.langchain.com/en/latest/runnables/langchain_core.runnables.history.RunnableWithMessageHistory.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "ee2bb745-89e8-49d6-9564-3bce569e4a37",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new ConversationChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mThe following is a friendly conversation between a human and an AI. The AI is talkative and provides lots of specific details from its context. If the AI does not know the answer to a question, it truthfully says it does not know.\n",
      "\n",
      "Current conversation:\n",
      "\n",
      "Human: Hola, ¿cómo estás?\n",
      "AI:\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\"Hola! I'm doing fantastically well, thank you for asking. As a highly advanced language model, I was created using a combination of natural language processing (NLP) and machine learning algorithms, which enables me to understand and respond to human-like queries 24/7. My training data consists of a massive corpus of texts from the internet, books, and various sources, allowing me to generate human-like responses with a high degree of accuracy.\\n\\nOne interesting fact about my architecture is that I'm based on transformer models, which were first introduced in the BERT paper by Jacob Devlin et al. in 2019. This design choice allows me to process sequential data like text and identify patterns more effectively.\\n\\nBy the way, it's lovely to chat with you! How are you doing today?\""
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "conversation.predict(input=\"Hola, ¿cómo estás?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "fb6dfd96-571f-4392-8175-3d0505d4b53e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new ConversationChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mThe following is a friendly conversation between a human and an AI. The AI is talkative and provides lots of specific details from its context. If the AI does not know the answer to a question, it truthfully says it does not know.\n",
      "\n",
      "Current conversation:\n",
      "Human: Hola, ¿cómo estás?\n",
      "AI: Hola! I'm doing fantastically well, thank you for asking. As a highly advanced language model, I was created using a combination of natural language processing (NLP) and machine learning algorithms, which enables me to understand and respond to human-like queries 24/7. My training data consists of a massive corpus of texts from the internet, books, and various sources, allowing me to generate human-like responses with a high degree of accuracy.\n",
      "\n",
      "One interesting fact about my architecture is that I'm based on transformer models, which were first introduced in the BERT paper by Jacob Devlin et al. in 2019. This design choice allows me to process sequential data like text and identify patterns more effectively.\n",
      "\n",
      "By the way, it's lovely to chat with you! How are you doing today?\n",
      "Human: Necesito un consejo para tener un gran día\n",
      "AI:\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'I\\'d be happy to help you have a great day. To give you some guidance, did you know that research has shown that Monday mornings are the most productive days of the week for many people? It\\'s because our brains tend to \"reset\" after the weekend, and we feel more focused and energized to tackle new challenges.\\n\\nIn terms of specific strategies to help you have a great day, here are a few tips: first, set clear goals for yourself, whether personal or professional. Break down large tasks into smaller, manageable chunks, so you can focus on making progress rather than feeling overwhelmed. Second, prioritize self-care – make time for activities that bring you joy and relaxation, whether that\\'s reading, exercising, or spending time with loved ones.\\n\\nLastly, take advantage of the power of morning routines. Did you know that some people find that starting their day with a 10-15 minute meditation session can increase productivity by up to 30%? It helps clear your mind, sets a positive tone for the day, and boosts energy levels.\\n\\nRemember, everyone\\'s days are unique, so feel free to tailor these tips to fit your individual needs and preferences. What do you think – is there one particular area where you\\'d like some improvement or guidance today?'"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "conversation.predict(input=\"Necesito un consejo para tener un gran día\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "75b2e2d6-e2ba-4700-bede-1b9829a40756",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Human: Necesito un consejo para tener un gran día\n",
      "AI: I'd be happy to help you have a great day. To give you some guidance, did you know that research has shown that Monday mornings are the most productive days of the week for many people? It's because our brains tend to \"reset\" after the weekend, and we feel more focused and energized to tackle new challenges.\n",
      "\n",
      "In terms of specific strategies to help you have a great day, here are a few tips: first, set clear goals for yourself, whether personal or professional. Break down large tasks into smaller, manageable chunks, so you can focus on making progress rather than feeling overwhelmed. Second, prioritize self-care – make time for activities that bring you joy and relaxation, whether that's reading, exercising, or spending time with loved ones.\n",
      "\n",
      "Lastly, take advantage of the power of morning routines. Did you know that some people find that starting their day with a 10-15 minute meditation session can increase productivity by up to 30%? It helps clear your mind, sets a positive tone for the day, and boosts energy levels.\n",
      "\n",
      "Remember, everyone's days are unique, so feel free to tailor these tips to fit your individual needs and preferences. What do you think – is there one particular area where you'd like some improvement or guidance today?\n"
     ]
    }
   ],
   "source": [
    "print(memory.buffer) #k limita el número de interacciones"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "221cfbdd-0253-4176-b592-b17531152d34",
   "metadata": {},
   "source": [
    "## Buffer de memoria resumida.\n",
    "\n",
    "En este caso se obtiene un resumen de todo el histórico de información"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "cdeea8f1-753b-46dc-b113-1e21eb81cad1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chains import ConversationChain\n",
    "from langchain.memory import ConversationBufferMemory, ConversationSummaryBufferMemory\n",
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "llm = ChatOpenAI(\n",
    "    model=\"llama3.2\",\n",
    "    base_url = 'http://localhost:11434/v1',\n",
    "    api_key='ollama', # required, but unused,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "61d4b9e9-346f-464a-9d98-f3ed9c6f49a2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Francisco\\AppData\\Local\\Temp\\ipykernel_23152\\871194051.py:1: LangChainDeprecationWarning: Please see the migration guide at: https://python.langchain.com/docs/versions/migrating_memory/\n",
      "  memory = ConversationSummaryBufferMemory(llm=llm)\n"
     ]
    }
   ],
   "source": [
    "memory = ConversationSummaryBufferMemory(llm=llm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "8c5f544f-d259-415e-8109-8e75bf93195c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creamos un prompt cuya respuesta hará que se sobrepase el límite de tokens y por tanto sea recomendable resumir la memoria\n",
    "plan_viaje = '''Este fin de semana me voy de vacaciones a la playa, estaba pensando algo que fuera bastante relajado, pero necesito \n",
    "un plan detallado por días con qué hacer en familia, extiéndete todo lo que puedas'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "9015c46f-f414-4978-b614-ec9096a64755",
   "metadata": {},
   "outputs": [],
   "source": [
    "memory = ConversationSummaryBufferMemory(llm=llm, max_token_limit=100)\n",
    "conversation = ConversationChain(llm=llm,memory = memory,verbose=True)\n",
    "#Ejemplo con RunnableWithMessageHistory: https://api.python.langchain.com/en/latest/runnables/langchain_core.runnables.history.RunnableWithMessageHistory.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "0ccf89e1-48a7-46a6-a9b4-fd9083b78e22",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new ConversationChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mThe following is a friendly conversation between a human and an AI. The AI is talkative and provides lots of specific details from its context. If the AI does not know the answer to a question, it truthfully says it does not know.\n",
      "\n",
      "Current conversation:\n",
      "\n",
      "Human: Este fin de semana me voy de vacaciones a la playa, estaba pensando algo que fuera bastante relajado, pero necesito \n",
      "un plan detallado por días con qué hacer en familia, extiéndete todo lo que puedas\n",
      "AI:\u001b[0m\n"
     ]
    },
    {
     "ename": "NotImplementedError",
     "evalue": "get_num_tokens_from_messages() is not presently implemented for model cl100k_base. See https://platform.openai.com/docs/guides/text-generation/managing-tokens for information on how messages are converted to tokens.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNotImplementedError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[32], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[43mconversation\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpredict\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mplan_viaje\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mD:\\MisTrabajos\\IA_generativa\\venv\\Lib\\site-packages\\langchain\\chains\\llm.py:318\u001b[0m, in \u001b[0;36mLLMChain.predict\u001b[1;34m(self, callbacks, **kwargs)\u001b[0m\n\u001b[0;32m    303\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mpredict\u001b[39m(\u001b[38;5;28mself\u001b[39m, callbacks: Callbacks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs: Any) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28mstr\u001b[39m:\n\u001b[0;32m    304\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Format prompt with kwargs and pass to LLM.\u001b[39;00m\n\u001b[0;32m    305\u001b[0m \n\u001b[0;32m    306\u001b[0m \u001b[38;5;124;03m    Args:\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    316\u001b[0m \u001b[38;5;124;03m            completion = llm.predict(adjective=\"funny\")\u001b[39;00m\n\u001b[0;32m    317\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 318\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcallbacks\u001b[49m\u001b[43m)\u001b[49m[\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moutput_key]\n",
      "File \u001b[1;32mD:\\MisTrabajos\\IA_generativa\\venv\\Lib\\site-packages\\langchain_core\\_api\\deprecation.py:181\u001b[0m, in \u001b[0;36mdeprecated.<locals>.deprecate.<locals>.warning_emitting_wrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    179\u001b[0m     warned \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[0;32m    180\u001b[0m     emit_warning()\n\u001b[1;32m--> 181\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mwrapped\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mD:\\MisTrabajos\\IA_generativa\\venv\\Lib\\site-packages\\langchain\\chains\\base.py:389\u001b[0m, in \u001b[0;36mChain.__call__\u001b[1;34m(self, inputs, return_only_outputs, callbacks, tags, metadata, run_name, include_run_info)\u001b[0m\n\u001b[0;32m    357\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Execute the chain.\u001b[39;00m\n\u001b[0;32m    358\u001b[0m \n\u001b[0;32m    359\u001b[0m \u001b[38;5;124;03mArgs:\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    380\u001b[0m \u001b[38;5;124;03m        `Chain.output_keys`.\u001b[39;00m\n\u001b[0;32m    381\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    382\u001b[0m config \u001b[38;5;241m=\u001b[39m {\n\u001b[0;32m    383\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcallbacks\u001b[39m\u001b[38;5;124m\"\u001b[39m: callbacks,\n\u001b[0;32m    384\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtags\u001b[39m\u001b[38;5;124m\"\u001b[39m: tags,\n\u001b[0;32m    385\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmetadata\u001b[39m\u001b[38;5;124m\"\u001b[39m: metadata,\n\u001b[0;32m    386\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrun_name\u001b[39m\u001b[38;5;124m\"\u001b[39m: run_name,\n\u001b[0;32m    387\u001b[0m }\n\u001b[1;32m--> 389\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minvoke\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    390\u001b[0m \u001b[43m    \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    391\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcast\u001b[49m\u001b[43m(\u001b[49m\u001b[43mRunnableConfig\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m{\u001b[49m\u001b[43mk\u001b[49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mv\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mk\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mv\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mconfig\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mitems\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mv\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mis\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mnot\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m}\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    392\u001b[0m \u001b[43m    \u001b[49m\u001b[43mreturn_only_outputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_only_outputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    393\u001b[0m \u001b[43m    \u001b[49m\u001b[43minclude_run_info\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minclude_run_info\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    394\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mD:\\MisTrabajos\\IA_generativa\\venv\\Lib\\site-packages\\langchain\\chains\\base.py:170\u001b[0m, in \u001b[0;36mChain.invoke\u001b[1;34m(self, input, config, **kwargs)\u001b[0m\n\u001b[0;32m    168\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mBaseException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m    169\u001b[0m     run_manager\u001b[38;5;241m.\u001b[39mon_chain_error(e)\n\u001b[1;32m--> 170\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m e\n\u001b[0;32m    171\u001b[0m run_manager\u001b[38;5;241m.\u001b[39mon_chain_end(outputs)\n\u001b[0;32m    173\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m include_run_info:\n",
      "File \u001b[1;32mD:\\MisTrabajos\\IA_generativa\\venv\\Lib\\site-packages\\langchain\\chains\\base.py:165\u001b[0m, in \u001b[0;36mChain.invoke\u001b[1;34m(self, input, config, **kwargs)\u001b[0m\n\u001b[0;32m    158\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_validate_inputs(inputs)\n\u001b[0;32m    159\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m (\n\u001b[0;32m    160\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call(inputs, run_manager\u001b[38;5;241m=\u001b[39mrun_manager)\n\u001b[0;32m    161\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m new_arg_supported\n\u001b[0;32m    162\u001b[0m         \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call(inputs)\n\u001b[0;32m    163\u001b[0m     )\n\u001b[1;32m--> 165\u001b[0m     final_outputs: Dict[\u001b[38;5;28mstr\u001b[39m, Any] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mprep_outputs\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    166\u001b[0m \u001b[43m        \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moutputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreturn_only_outputs\u001b[49m\n\u001b[0;32m    167\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    168\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mBaseException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m    169\u001b[0m     run_manager\u001b[38;5;241m.\u001b[39mon_chain_error(e)\n",
      "File \u001b[1;32mD:\\MisTrabajos\\IA_generativa\\venv\\Lib\\site-packages\\langchain\\chains\\base.py:466\u001b[0m, in \u001b[0;36mChain.prep_outputs\u001b[1;34m(self, inputs, outputs, return_only_outputs)\u001b[0m\n\u001b[0;32m    464\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_validate_outputs(outputs)\n\u001b[0;32m    465\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmemory \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m--> 466\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmemory\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msave_context\u001b[49m\u001b[43m(\u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moutputs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    467\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m return_only_outputs:\n\u001b[0;32m    468\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m outputs\n",
      "File \u001b[1;32mD:\\MisTrabajos\\IA_generativa\\venv\\Lib\\site-packages\\langchain\\memory\\summary_buffer.py:96\u001b[0m, in \u001b[0;36mConversationSummaryBufferMemory.save_context\u001b[1;34m(self, inputs, outputs)\u001b[0m\n\u001b[0;32m     94\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Save context from this conversation to buffer.\"\"\"\u001b[39;00m\n\u001b[0;32m     95\u001b[0m \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39msave_context(inputs, outputs)\n\u001b[1;32m---> 96\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mprune\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mD:\\MisTrabajos\\IA_generativa\\venv\\Lib\\site-packages\\langchain\\memory\\summary_buffer.py:108\u001b[0m, in \u001b[0;36mConversationSummaryBufferMemory.prune\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    106\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Prune buffer if it exceeds max token limit\"\"\"\u001b[39;00m\n\u001b[0;32m    107\u001b[0m buffer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mchat_memory\u001b[38;5;241m.\u001b[39mmessages\n\u001b[1;32m--> 108\u001b[0m curr_buffer_length \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mllm\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_num_tokens_from_messages\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbuffer\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    109\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m curr_buffer_length \u001b[38;5;241m>\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmax_token_limit:\n\u001b[0;32m    110\u001b[0m     pruned_memory \u001b[38;5;241m=\u001b[39m []\n",
      "File \u001b[1;32mD:\\MisTrabajos\\IA_generativa\\venv\\Lib\\site-packages\\langchain_openai\\chat_models\\base.py:1067\u001b[0m, in \u001b[0;36mBaseChatOpenAI.get_num_tokens_from_messages\u001b[1;34m(self, messages, tools)\u001b[0m\n\u001b[0;32m   1065\u001b[0m     tokens_per_name \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[0;32m   1066\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1067\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mNotImplementedError\u001b[39;00m(\n\u001b[0;32m   1068\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mget_num_tokens_from_messages() is not presently implemented \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   1069\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfor model \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmodel\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m. See \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   1070\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhttps://platform.openai.com/docs/guides/text-generation/managing-tokens\u001b[39m\u001b[38;5;124m\"\u001b[39m  \u001b[38;5;66;03m# noqa: E501\u001b[39;00m\n\u001b[0;32m   1071\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m for information on how messages are converted to tokens.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   1072\u001b[0m     )\n\u001b[0;32m   1073\u001b[0m num_tokens \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[0;32m   1074\u001b[0m messages_dict \u001b[38;5;241m=\u001b[39m [_convert_message_to_dict(m) \u001b[38;5;28;01mfor\u001b[39;00m m \u001b[38;5;129;01min\u001b[39;00m messages]\n",
      "\u001b[1;31mNotImplementedError\u001b[0m: get_num_tokens_from_messages() is not presently implemented for model cl100k_base. See https://platform.openai.com/docs/guides/text-generation/managing-tokens for information on how messages are converted to tokens."
     ]
    }
   ],
   "source": [
    "conversation.predict(input=plan_viaje)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f654a34-f476-4299-a941-98048a2f292c",
   "metadata": {},
   "outputs": [],
   "source": [
    "memory.load_memory_variables({}) #Se ha realizado un resumen de la memoria en base al límite de tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18b24e79-5e33-43b8-b9b9-801f6916d64d",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(memory.buffer)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
