{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "edb00af4-ea2a-4ab5-a8d0-6e8153b22fb5",
   "metadata": {},
   "source": [
    "# La Api de OpenAI.\n",
    "```{index} assistants\n",
    "```\n",
    "Los assistants de OpenAI representan una revolución en la integración de inteligencia artificial personalizada y conversacional en aplicaciones y servicios. Son agentes que utilizan modelos de lenguaje avanzados, como GPT-4 Turbo, para automatizar tareas, interactuar con usuarios en lenguaje natural y ejecutar funciones adaptadas según las necesidades específicas del contexto.\n",
    "\n",
    "## ¿Qué son los OpenAI Assistants?\n",
    "Un OpenAI Assistant es un agente digital configurable que utiliza modelos de lenguaje entrenados con grandes cantidades de datos para responder preguntas, ejecutar funciones, analizar información y brindar asistencia en una amplia variedad de tareas. Estos asistentes se configuran a través de instrucciones (que definen personalidad y comportamiento), conjuntos de datos personalizados, y funciones programadas por el desarrollador.\n",
    "\n",
    "Los asistentes son inteligencias artificiales generativas diseñadas para propósitos específicos, utilizando los modelos de OpenAI (como GPT-4 y GPT-3.5, etc.). Estos asistentes tienen acceso a archivos, pueden mantener el contexto de la conversación (historial) y pueden usar herramientas especializadas.\n",
    "\n",
    "## Características principales\n",
    "**Personalización:** Es posible definir el comportamiento y personalidad del asistente mediante instrucciones precisas, así como entrenarlo con datos propios (PDFs, artículos, bases de datos, etc.).\n",
    "\n",
    "**Función de llamada:** Los assistants pueden integrarse con funciones externas y APIs para acceder a información en tiempo real, automatizar cálculos o interactuar con otros servicios digitales.\n",
    "\n",
    "**Gestión de datos:** Capaces de procesar tanto datos estructurados (JSON, hojas de cálculo) como no estructurados (texto libre, documentos), extrayendo y transformando información relevante.\n",
    "\n",
    "**Escalabilidad:** La infraestructura de OpenAI permite atender miles de usuarios en simultáneo sin perder calidad en las respuestas.\n",
    "\n",
    "**Aprendizaje continuo:** Gracias a su naturaleza de entrenamiento, los assistants pueden mejorar su precisión y relevancia conforme reciben nuevos datos o retroalimentación.\n",
    "\n",
    "## Ventajas de utilizar OpenAI Assistants\n",
    "**Automatización y productividad:** Permiten gestionar tareas repetitivas, asistir en proyectos, analizar grandes volúmenes de datos y mejorar la atención al cliente.\n",
    "\n",
    "**Accesibilidad y apertura:** El enfoque suele ser abierto para desarrolladores, lo que facilita la colaboración, la creación de asistentes especializados y la rápida evolución tecnológica.\n",
    "\n",
    "**Integración sencilla:** Herramientas y APIs modernas permiten que los asistentes se integren fácilmente en aplicaciones web, equipos de trabajo y sistemas empresariales.\n",
    "\n",
    "Es de advertir que tal y como figura en su página oficial, en el momento de redactar este documento, esta api está *deprecated* y dejará de estar disponible el 26 de agosto de 2026, siendo sustituida por la api de *responses*. \n",
    "\n",
    "A pesar de todo esto se ha creído conveniente proceder a presentar esta las dos API's, con la finalidad de que el lector sepa de donde estamos y cuales serán los nuevos cambios hacia la nueva API.\n",
    "\n",
    "\n",
    "En la siguiente imagen mostramos una visualización que detalla cómo esta compuesta esta API. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db984c37-4ae4-4a66-8ebd-e16a32726540",
   "metadata": {},
   "source": [
    "![](fig/esquema.PNG/)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fcb9f7ec-ac35-494a-b1de-7900c1d2ffb4",
   "metadata": {},
   "source": [
    "En esa figura, observamos los siguientes componentes que pueden configurar el assitants de OpenAi:\n",
    "\n",
    "* Los modelos LLM, que puede ser cualquiera de los modelos que tiene OpenAi, en esta caso se hace referencia a los modelos GPT 4 ó GPT 3.5 turbo.\n",
    "\n",
    "* Un asistente va a tener un identificador (Id) que es único para cada asistente, un nombre, una descripción e instrucciones.\n",
    "\n",
    "* Herramientas o Tools. Las mas importantes son las de recuperación (retrieval), interprete de Código (para trabajar código python) y la llamada a funciones.\n",
    "\n",
    "* Los Thread o hilos de conversaciones. Cada hilo viene a ser una sesion conversación con un usuario. En cada hilo se guarda los promts del usuario y las respuestas correspondientes del asistente\n",
    "\n",
    "* Los Run son los engranajes entre los asistentes y los thread.\n",
    "\n",
    "\n",
    "A continuación vamos a mostrar un ejemplo de cómo podemos crear un asistente con la herramienta de retrieval.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24be9a54-726a-4840-be5e-e9b97f987175",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install --upgrade openai"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "fdd3a8ce-7c1b-4556-a8e4-50f795a0c560",
   "metadata": {},
   "outputs": [],
   "source": [
    "# de esta forma obtenemos los resultados de forma más estética\n",
    "from pprint import pprint\n",
    "# importamos JSON para poder trabajar con este tipo de datos\n",
    "import json\n",
    "# importamos el paqute de openai\n",
    "from openai import OpenAI\n",
    "import time"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "499feea2-2db1-4f5c-973a-e57c8c310cbc",
   "metadata": {},
   "source": [
    "## Ficheros de configuración de claves API\n",
    "\n",
    "A continuación, lo que hacemos es recuperar la clave API de OpenAI, pero para no mostrarla aquí utilizaremos una técnica muy extendida entre los desarrolladores de Python, que consiste en generar un fichero denominado .env y que haciendo una pequeña digresión del tema central de este documento consiste en lo siguiente:\n",
    "Un archivo .env en Python es un archivo de texto plano que almacena variables de entorno, como contraseñas o claves API, para mantener la información sensible fuera del código fuente y del repositorio de control de versiones. Se utiliza con la biblioteca python-dotenv, la cual carga estos pares clave-valor en el entorno de ejecución de la aplicación. Para usarlo, se instala la biblioteca, se crea un archivo .env con las variables, y luego se utiliza os.getenv() para acceder a ellas en el código de Python. \n",
    "\n",
    "```{index} .env, ficheros de claves\n",
    "```\n",
    "### ¿Para qué se usan los archivos .env?\n",
    "\n",
    "* **Seguridad**: Permiten mantener información confidencial, como credenciales de bases de datos o claves de API, separada del código fuente, evitando que se exponga accidentalmente.\n",
    "  \n",
    "* **Configuración**: Facilitan la gestión de diferentes configuraciones para distintos entornos (desarrollo, pruebas, producción) sin tener que modificar el código.\n",
    "\n",
    "  \n",
    "* **Buenas prácticas**: Es una práctica recomendada para separar los datos de configuración del código, lo que hace el proyecto más limpio y seguro.\n",
    "\n",
    "### ¿Cómo se usan en Python?.\n",
    "\n",
    "Primero instalamos la biblioteca:\n",
    "\n",
    "``` Python\n",
    "    pip install python-dotenv\n",
    "```\n",
    "\n",
    "Se crea un archivo denominado .env enla raíz del proyecto y se añaden las variables de entorno en formato CLAVE = VALOR, por ejemplo:\n",
    "\n",
    "```{note}\n",
    "    API_KEY=tu_clave_secreta\n",
    "    DATABASE_URL=postgresql://usuario:contraseña@host:puerto/db\n",
    "```\n",
    " Y ahora ya podemos cargar y usar las variables de entorno en el código de la siguiente manera:\n",
    "\n",
    " ```Python\n",
    "    import os\n",
    "    from dotenv import load_dotenv\n",
    "\n",
    "    # Carga las variables del archivo .env en el entorno\n",
    "    load_dotenv()\n",
    "\n",
    "    # Accede a las variables con os.getenv()\n",
    "    api_key = os.getenv(\"API_KEY\")\n",
    "    db_url = os.getenv(\"DATABASE_URL\")\n",
    "\n",
    "    print(f\"La API Key es: {api_key}\")\n",
    "    print(f\"La URL de la base de datos es: {db_url}\")\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b4b8fabb-5ba7-407b-b79f-33c02caac6bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()\n",
    "openai_key = os.getenv(\"openai_key\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "38aae82a-69d8-4b0a-8268-339373580738",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Si queremos ver la api key\n",
    "#print(openai_key)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ee1c772a-333b-4703-9cc6-4edfdd239ff9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creamos el cliente de OpenAI\n",
    "client = OpenAI(\n",
    "api_key = openai_key\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d1ec46b-6765-4837-94d6-e087bde1a0a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# paso 2: Cargar archivos que nuestro asistente necesita para el tipo retrieval\n",
    "# pueden ser de tipo .pdf, excel, worf, etc\n",
    "#El tamaño máximo del archivo debe ser 512 MB\n",
    "# máximo almacenamiento 100 GB\n",
    "# Cargamos un pequeño fichero .pdf que se encuenta en : https://www.semfyc.es/storage/wp-content/uploads/2021/12/02_Unidad_2020-8.pdf\n",
    "# El contenido del fichero es el dolor de oídos en personas adultas \n",
    "file = client.files.create(\n",
    "    file = open(\"oidos.pdf\",\"rb\"),\n",
    "    purpose = 'assistants'\n",
    ")\n",
    "\n",
    "# imprimimos el identificador del fichero\n",
    "print(f\"El identificador del fichero es:{file.id}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a48c35f-b3c5-4bc5-a940-ce8e7d1c9ab8",
   "metadata": {},
   "source": [
    "Este identificador también lo podemos ver en el *playground de OpenAI* (https://platform.openai.com/chat/edit?models=gpt-5) en el menú denominado: *Storage*.\n",
    "\n",
    "![](fig/Playground.PNG)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26810e14-9e17-4037-87c3-8451cf8ec5f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f47fbb4-0a23-469d-b89f-e775b9891f3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# creamos un asistente sin vector_stores\n",
    "\n",
    "assistant = client.beta.assistants.create(\n",
    "    name=\"Mi Assistant\",\n",
    "    instructions=\"Eres un otorrino de mucho prestigio y con grandes conocimientos sobre el funcionamiento del oído\",\n",
    "    model=\"gpt-4-turbo\",\n",
    "    tools=[{\"type\": \"file_search\"}]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "967a8408-0b2a-408e-b84f-49c127b6037a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3. Crear thread con archivo adjunto\n",
    "thread = client.beta.threads.create(\n",
    "    messages=[\n",
    "        {\n",
    "            \"role\": \"user\", \n",
    "            \"content\": \"¿Cuáles son los puntos principales de este documento?\",\n",
    "            \"attachments\": [\n",
    "                {\n",
    "                    \"file_id\": file.id,\n",
    "                    \"tools\": [{\"type\": \"file_search\"}]\n",
    "                }\n",
    "            ]\n",
    "        }\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3aee7609-c9de-43f0-b2cc-fc59dda62db5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4. Ejecutar \n",
    "run = client.beta.threads.runs.create(\n",
    "    thread_id=thread.id,\n",
    "    assistant_id=assistant.id\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0296a59-4d43-46c3-a111-573951d52119",
   "metadata": {},
   "outputs": [],
   "source": [
    "run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03f14c8e-4f8b-4e2b-ba86-074245ac8a2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Run creado: {run.id}\")\n",
    "print(f\"Estado inicial: {run.status}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c7dff99-a562-4fc7-aec9-f0f44321c3a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. ESPERAR A QUE TERMINE LA EJECUCIÓN\n",
    "while run.status in ['queued', 'in_progress', 'cancelling']:\n",
    "    time.sleep(1)  # Esperar 1 segundo\n",
    "    run = client.beta.threads.runs.retrieve(\n",
    "        thread_id=thread.id,\n",
    "        run_id=run.id\n",
    "    )\n",
    "    print(f\"Estado actual: {run.status}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3eabbdfe-dfae-411a-a1b2-0e74fad5fc2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# 1. ESPERAR A QUE TERMINE LA EJECUCIÓN\n",
    "while run.status in ['queued', 'in_progress', 'cancelling']:\n",
    "    time.sleep(1)  # Esperar 1 segundo\n",
    "    run = client.beta.threads.runs.retrieve(\n",
    "        thread_id=thread.id,\n",
    "        run_id=run.id\n",
    "    )\n",
    "    print(f\"Estado actual: {run.status}\")\n",
    "\n",
    "# 2. VERIFICAR SI COMPLETÓ EXITOSAMENTE\n",
    "if run.status == 'completed':\n",
    "    print(\"✅ Ejecución completada\")\n",
    "    \n",
    "    # 3. OBTENER TODOS LOS MENSAJES DEL THREAD\n",
    "    messages = client.beta.threads.messages.list(\n",
    "        thread_id=thread.id,\n",
    "        order=\"asc\"  # Del más antiguo al más reciente\n",
    "    )\n",
    "    \n",
    "    # 4. EXTRAER LA RESPUESTA DEL ASSISTANT\n",
    "    print(\"\\n\" + \"=\"*50)\n",
    "    print(\"RESPUESTA DEL ASSISTANT:\")\n",
    "    print(\"=\"*50)\n",
    "    \n",
    "    # El último mensaje será la respuesta del assistant\n",
    "    last_message = messages.data[-1]  # Último mensaje\n",
    "    \n",
    "    if last_message.role == 'assistant':\n",
    "        # Extraer el texto de la respuesta\n",
    "        respuesta_texto = last_message.content[0].text.value\n",
    "        print(respuesta_texto)\n",
    "        \n",
    "        # 5. OBTENER CITAS Y ANOTACIONES (si las hay)\n",
    "        annotations = last_message.content[0].text.annotations\n",
    "        \n",
    "        if annotations:\n",
    "            print(\"\\n\" + \"=\"*30)\n",
    "            print(\"CITAS Y REFERENCIAS:\")\n",
    "            print(\"=\"*30)\n",
    "            \n",
    "            for i, annotation in enumerate(annotations):\n",
    "                if hasattr(annotation, 'file_citation'):\n",
    "                    citation = annotation.file_citation\n",
    "                    print(f\"Cita {i+1}: {citation.quote}\")\n",
    "                    print(f\"Archivo ID: {citation.file_id}\")\n",
    "                elif hasattr(annotation, 'file_path'):\n",
    "                    file_path = annotation.file_path\n",
    "                    print(f\"Archivo referenciado {i+1}: {file_path.file_id}\")\n",
    "    \n",
    "elif run.status == 'failed':\n",
    "    print(f\"❌ Error en la ejecución: {run.last_error}\")\n",
    "    \n",
    "elif run.status == 'requires_action':\n",
    "    print(\"⚠️ Requiere acción adicional\")\n",
    "    print(f\"Acción requerida: {run.required_action}\")\n",
    "\n",
    "else:\n",
    "    print(f\"Estado inesperado: {run.status}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35486027-572f-465f-81d6-2736cf3645eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ahora borramos el fichero que hemos almacenado anteriormente, al fin de no incurrir en costos por el almacenamiento\n",
    "client.files.delete(file.id)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53f169b5-6b9d-41b6-aef3-ed055d1d475b",
   "metadata": {},
   "source": [
    "## Llamadas a funciones.\n",
    "\n",
    "Ya hemos visto antes que otra de las herramientas que se pueden utilizar con los assistents, son las llamadas a funciones, cuya documentación la podemos encontrar en el siguiente enlace:\n",
    "\n",
    "https://platform.openai.com/docs/guides/function-calling\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "122f3f18-f636-435b-8b78-58c836cb7662",
   "metadata": {},
   "outputs": [],
   "source": [
    "# cramos nestra primera función\n",
    "# NOMBRE-DESCRPCIÓN-PARÁMETROS\n",
    "sendEmail = {\n",
    "    \"name\" : \"sendEmail\",\n",
    "    \"description\":\"The function allows us to send email by specifying an email address and the title and descrption of the email.\",\n",
    "    \"parameters\":{\n",
    "        \"type\": \"object\",\n",
    "        \"properties\":{\n",
    "            \"email\": {\n",
    "                \"type\":\"string\",\n",
    "                \"description\":\"Email address of the receiver\"\n",
    "            },\n",
    "            \"subject\":{\n",
    "                \"type\":\"string\",\n",
    "                \"description\": \"Subject of the email\"\n",
    "            },\n",
    "            \"textBody\":{\n",
    "                \"type\":\"string\",\n",
    "                \"description\":\"Body of the email\"\n",
    "            }\n",
    "        },\n",
    "         \"required\":[\"email\",\"subject\",\"textBody\"]\n",
    "    }\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e000cdd-3f8c-4b50-b947-3573272d633b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# creamos la segunda función\n",
    "getCurrentWeather ={\n",
    "    \"name\":\"getCurrentWeather\",\n",
    "    \"description\": \"Get the weather in location\",\n",
    "    \"parameters\" :{\n",
    "        \"type\": \"object\",\n",
    "        \"properties\":{\n",
    "            \"location\": {\"type\":\"string\",\"description\":\"The city and e.g. San Francisco , CA\"},\n",
    "            \"unit\":{\"type\":\"string\",\"enum\":[\"c\",\"f\"]}\n",
    "        },\n",
    "        \"required\": [\"location\"]\n",
    "    }\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "045b0ff4-5eba-4321-9de6-d62e6b5e025d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# creamos el asistente\n",
    "assistant = client.beta.assistants.create(\n",
    "    name=\"Chatbot que responde a las preguntas del cliente\",\n",
    "    instructions=\"Eres un chatbot de la Peluqueria LLM Master  y te has especializado en enviar emails. Usa tu concimiento para responder a las preguntas del usuario\",\n",
    "    model = \"gpt-3.5-turbo-1106\",\n",
    "    tools = [\n",
    "        ({'type':'function','function': sendEmail}),\n",
    "        ({'type':'function','function': getCurrentWeather})\n",
    "    ]\n",
    ")\n",
    "\n",
    "print(assistant.id)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5228db95-575f-4d49-a89c-bc9bc7e85bd0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# creamos el hilo\n",
    "thread = client.beta.threads.create()\n",
    "print(thread.id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c235972a-62e3-4615-8411-edd7ad3985d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# creamos el primer mensaje del usuario y lo pasamos al hilo\n",
    "message = client.beta.threads.messages.create(\n",
    "    thread_id = thread.id,\n",
    "    role = \"user\",\n",
    "    content =\"que tiempo hace en Madrid?\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "429a2c93-127a-495a-aade-940c6f8cc560",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ejecutamos el asistente para obtener la respuesta\n",
    "run= client.beta.threads.runs.create(\n",
    "    thread_id = thread.id,\n",
    "        assistant_id = assistant.id\n",
    ")\n",
    "\n",
    "print(run.id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41e0a926-25c1-4d9d-bad9-361ce2e1d0f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Recupera el estado de ejecución (aqui vemos que require_action)\n",
    "# Esto le está diciendo al modelo, que necesita información de nuestra función\n",
    "# es decir, que necesitamos llamar a una función que permita enviar el email\n",
    "run = client.beta.threads.runs.retrieve(\n",
    "    thread_id = thread.id,\n",
    "    run_id=run.id\n",
    ")\n",
    "\n",
    "print(run.status)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e14c6e9-1693-44eb-8bc7-5ca6e8da4953",
   "metadata": {},
   "outputs": [],
   "source": [
    "# como obtenemos requires_action entonces tenemos que llamar a una función\n",
    "# ejemplo una llamada a una api para enciar un mail, titulo y descripción\n",
    "# Ejemplo: Una llamada a un api para obtener la temperatura de Madrid\n",
    "\n",
    "## tools_to_call= a qué función tenemos que llamar\n",
    "tools_to_call = run.required_action.submit_tool_outputs.tool_calls\n",
    "print(len(tools_to_call)) # cuantas llamadas tengoq ue hacer, imaginate que es más de una\n",
    "print(tools_to_call)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38a0c508-77ec-4ec9-82e2-606ed749f847",
   "metadata": {},
   "source": [
    "## Interprete de código.\n",
    "\n",
    "Recordar que esta es otra de las herramientas de los asistentes, según hemos visto al comienzo de este tema. Esta herramienta permite escribir y ejecutar código en python en un entorno seguro y aislado. Este entorno es capaz de procesar archivos de diversos formatos, generar resultados en tiempo real y crear imágenes de gráficos.\n",
    "Veamos a continuación cómo funciona este interprete de código."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b4bac4a-168b-4272-8646-fb4cbb220340",
   "metadata": {},
   "outputs": [],
   "source": [
    "# suponemos que ya hemos creado el ciente "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a0cade0-cac8-49cf-b39e-01c4e5fd53fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "assistant = client.beta.assistants.create(\n",
    "    name = \"profesor matemáticas\",\n",
    "    instructions = \"Eres un chatbot especializado en resolver problemas matemáticos. Escribe y ejecuta código para responder las preguntas de matemáticas\",\n",
    "    model = \"gpt-3.5-turbo-1106\",\n",
    "    tools =[({'type':'code_interpreter'})]\n",
    ")\n",
    "\n",
    "print(assistant.id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1d90230-0e63-4252-9e11-19947d6da586",
   "metadata": {},
   "outputs": [],
   "source": [
    "#creamos el hilo\n",
    "thread = client.beta.threads.create()\n",
    "print(thread.id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86ea7086-d2c4-4cae-afa7-a69ee50e4e23",
   "metadata": {},
   "outputs": [],
   "source": [
    "# creamos el primer mensaje del usuario y lo pasamos al hilo\n",
    "message = client.beta.threads.messages.create(\n",
    "    thread_id=thread.id,\n",
    "    role = \"user\",\n",
    "    content=\"tengo el siguiente problema matemático '5x+5=10'. ¿cual es el valor de x?\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd0809af-1aa8-4432-a29c-d4f7fe58da0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ejecutamos el asistente para obtener la respuesta\n",
    "run = client.beta.threads.runs.create(\n",
    "    thread_id=thread.id,\n",
    "    assistant_id=assistant.id\n",
    ")\n",
    "\n",
    "print(run.id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09b7af61-5277-457f-98d8-4ef38d5a5ff6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# leemos los mensaje\n",
    "messages = client.beta.threads.messages.list(\n",
    "    thread_id=thread.id\n",
    ")\n",
    "\n",
    "with open(\"messages.json\",\"w\") as f:\n",
    "    messages_json = messages.model_dump()\n",
    "    json.dump(messages_json,f,indent=4)\n",
    "\n",
    "print(\"Ver conversación:\")\n",
    "for message in messages.data:\n",
    "    pprint(message.role+\": \"+message.content[0].text.value)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75c0220b-7021-4903-8be7-1d349d736447",
   "metadata": {},
   "source": [
    "# Responses.\n",
    "\n",
    "Como ya se ha dicho anteriormente, la API de asistentes quedará obsoleta a partir del 26  de agosto de 2026, y será sustituida por la API de Responses. Para ayuda de la migración, OpenAi ha creado la página siguiente:\n",
    "\n",
    "https://platform.openai.com/docs/assistants/migration\n",
    "\n",
    "La equivalencia entre una API y la otra se puede ver en el siguiente cuadro:\n",
    "\n",
    "![](fig/cambios.PNG)\n",
    "\n",
    "En lo que sigue vamos a hacer una introducción a esta nueva API denominada **Responses**. Para seguir con este código, vamos a suponer que ya tenemos creado el objeto client como se ha hecho al comienzo de este documento. Comenzamos con un ejemplo bien sencillo para hacer una introducción a este elemento."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73b73ae6-c8e2-447b-ae25-cbadfc858c16",
   "metadata": {},
   "outputs": [],
   "source": [
    "from openai import OpenAI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1b84f03-d4a5-4fc0-935d-b90b526de444",
   "metadata": {},
   "outputs": [],
   "source": [
    "response = client.responses.create(\n",
    "    model =\"gpt-4o-mini\",\n",
    "    input=\"Dime algo sobre la historia de Valladolid\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b47de6ff-209c-4186-bd85-f5c3ce6bbe9a",
   "metadata": {},
   "source": [
    "Lo que se devuelve es un objeto de tipo JSON, cuyo contenido se ve a continuación."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b2ed55c-0a65-4448-a0d8-9986c9d10192",
   "metadata": {},
   "outputs": [],
   "source": [
    "pprint(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5473404-41f4-4720-bde9-1cce0f48eaac",
   "metadata": {},
   "source": [
    "Para obtener el contenido e la respuesta devuelta, lo podemos hace dela siguiente manera:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12b998fe-2ed6-440c-8395-0ec4c14e74c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "response.output[0].content[0].text"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e31640e-f735-40b6-9f07-220ea8284bc0",
   "metadata": {},
   "source": [
    "## Persistencia entre conversaciones.\n",
    "```{index} Responses, Conversation\n",
    "```\n",
    "La API de Responses de OpenAI puede utilizarse con objetos **conversation** para mantener el estado de conversación de manera persistente. Aquí te proporciono un ejemplo completo de cómo utilizarla:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f24e0b5c-257d-4716-9d0b-15914c22e9a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from openai import OpenAI\n",
    "\n",
    "# 1. Crear un objeto conversation\n",
    "conversation = client.conversations.create()\n",
    "\n",
    "# 2. Primera respuesta usando el conversation\n",
    "response = client.responses.create(\n",
    "    model=\"gpt-4o-mini\",\n",
    "    input=[{\"role\": \"user\", \"content\": \"¿Cuáles son los 5 Ds del dodgeball?\"}],\n",
    "    conversation=conversation.id\n",
    ")\n",
    "\n",
    "print(f\"Primera respuesta: {response.output_text}\")\n",
    "\n",
    "# 3. Segunda respuesta usando la misma conversation\n",
    "# El contexto se mantiene automáticamente\n",
    "second_response = client.responses.create(\n",
    "    model=\"gpt-4o-mini\",\n",
    "    input=[{\"role\": \"user\", \"content\": \"¿Puedes explicar el último D?\"}],\n",
    "    conversation=conversation.id\n",
    ")\n",
    "\n",
    "print(f\"Segunda respuesta: {second_response.output_text}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd0695b9-9aeb-4ab6-86dd-af72bf751a5f",
   "metadata": {},
   "source": [
    ":::{note}\n",
    "* La API de Responses la podemos encontrar en el siguiente enlace:\n",
    "\n",
    "https://platform.openai.com/docs/api-reference/responses/create\n",
    "\n",
    "* La API de conversation la podemos encontrar en la siguiente dirección:\n",
    "\n",
    "https://platform.openai.com/docs/api-reference/conversations/create\n",
    "\n",
    "* El mantenimiento del estado de una conversación lo podemos encontrar en el siguiente enlace\n",
    "\n",
    "https://platform.openai.com/docs/guides/conversation-state?api-mode=responses \n",
    "\n",
    ":::"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86165426-1c1b-4bf4-ba45-925730dfeb63",
   "metadata": {},
   "source": [
    "Un ejemplo similar, pero con gestión manual del historial es el siguiente:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e358d8da-9d96-4524-b5a6-d6aafa998c5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from openai import OpenAI\n",
    "\n",
    "#client = OpenAI()\n",
    "\n",
    "# Mantener historial manualmente\n",
    "history = [\n",
    "    {\"role\": \"user\", \"content\": \"cuéntame un chiste\"}\n",
    "]\n",
    "\n",
    "response = client.responses.create(\n",
    "    model=\"gpt-4o-mini\",\n",
    "    input=history,\n",
    "    store=False\n",
    ")\n",
    "\n",
    "print(f\"Chiste: {response.output_text}\")\n",
    "\n",
    "# Agregar la respuesta al historial\n",
    "history += [{\"role\": el.role, \"content\": el.content} for el in response.output]\n",
    "history.append({\"role\": \"user\", \"content\": \"explica por qué es gracioso\"})\n",
    "\n",
    "second_response = client.responses.create(\n",
    "    model=\"gpt-4o-mini\",\n",
    "    input=history,\n",
    "    store=False\n",
    ")\n",
    "\n",
    "print(f\"Explicación: {second_response.output_text}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ddf56f7-5d89-4a4b-af0a-2523426fdbfa",
   "metadata": {},
   "source": [
    "Se puede conseguir un efecto similar utilizando el parámetro *previous_response_id*. Veamos un ejemplo:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5dec9b8-18da-444a-acae-780dbecdf545",
   "metadata": {},
   "outputs": [],
   "source": [
    "from openai import OpenAI\n",
    "\n",
    "#client = OpenAI()\n",
    "\n",
    "# Primera respuesta\n",
    "response = client.responses.create(\n",
    "    model=\"gpt-4o-mini\",\n",
    "    input=\"cuéntame un chiste\",\n",
    "    store=True\n",
    ")\n",
    "\n",
    "print(f\"Chiste: {response.output_text}\")\n",
    "\n",
    "# Segunda respuesta enlazada\n",
    "second_response = client.responses.create(\n",
    "    model=\"gpt-4o-mini\",\n",
    "    previous_response_id=response.id,\n",
    "    input=[{\"role\": \"user\", \"content\": \"explica por qué es gracioso\"}],\n",
    "    store=True\n",
    ")\n",
    "\n",
    "print(f\"Explicación: {second_response.output_text}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5f835b1-8adf-431f-bede-2b6c61739f5a",
   "metadata": {},
   "source": [
    "**Ventajas del objeto conversation:**\n",
    "\n",
    "El objeto conversation ofrece varias ventajas :\n",
    "\n",
    "Persistencia: Mantiene el estado a través de sesiones, dispositivos o trabajos\n",
    "\n",
    "Identificador durable: Cada conversación tiene un ID único persistente\n",
    "\n",
    "Gestión automática: No necesitas manejar manualmente el historial de mensajes\n",
    "\n",
    "Sin límite de 30 días: A diferencia de las respuestas individuales, las conversaciones no tienen TTL\n",
    "\n",
    "**Consideraciones importantes**:\n",
    "\n",
    "Las respuestas individuales se guardan por 30 días por defecto\n",
    "\n",
    "Todos los tokens de entrada previos en la cadena se facturan como tokens de entrada\n",
    "\n",
    "Las conversaciones almacenan items que pueden ser mensajes, llamadas a herramientas y otros datos\n",
    "\n",
    "OpenAI no utiliza datos enviados vía API para entrenar modelos sin consentimiento explícito\n",
    "\n",
    "El **método más recomendado** es usar el objeto conversation ya que simplifica significativamente la gestión del estado y proporciona persistencia a largo plazo.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57864e73-de6b-4240-90c4-94a9c48a0929",
   "metadata": {},
   "source": [
    "## La API de Files.\n",
    "\n",
    "Existe también esta API para poder almacenar ficheros con los que se va a trabajar en OpenAi (tenr en cuenta que mantener estos ficheros tiene un coste). La documentación de esta API se puede encontrar en el siguiente enlace:\n",
    "\n",
    "https://platform.openai.com/docs/api-reference/files/create\n",
    "\n",
    "Existen diferentes posibilidades qur ofrece esta API, como subir ficheros, recuperarlos, borrarlos,etc. Vamos a generar ahora un ejemplo de como subir un fichero:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76cab540-6f38-421f-9e0c-0e0b67b4a4d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from openai import OpenAI\n",
    "client = OpenAI()\n",
    "\n",
    "file = client.files.create(\n",
    "  file=open(\"oidos.pdf\", \"rb\"),\n",
    "  purpose=\"assistants\",\n",
    "  expires_after={\n",
    "    \"anchor\": \"created_at\",\n",
    "    \"seconds\": 4000\n",
    "  }\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9c1d4df-3389-4c06-b3e3-1f8b3ee65b1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Veamos los ficheros existente:\n",
    "client.files.list()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "287b78a9-bd74-429d-a6bf-bd35e3c9fbf4",
   "metadata": {},
   "source": [
    "Como ya hemos indicado en un apartado anterior, podemos ver el fichero en el dashboard de OpenAi y alli borrarrlo, pero otra forma de hacer esto es la siguiente:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e1e4be0-a280-44f5-b293-c450de052c46",
   "metadata": {},
   "outputs": [],
   "source": [
    "client.files.delete(file.id)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1eebcf6f-d4b5-412d-9cf2-31a45dd0047a",
   "metadata": {},
   "source": [
    "## Vector store.\n",
    "\n",
    "Un “vector store” en OpenAI es un almacén de información diseñado para búsqueda semántica y recuperación aumentada por recuperación (RAG), donde los documentos se dividen en fragmentos, se convierten en incrustaciones vectoriales y se indexan para consultas por similitud, de modo que un asistente pueda responder con precisión citando y recuperando los trozos más relevantes de los archivos adjuntos.\n",
    "\n",
    "En la práctica, un objeto vector store actúa como contenedor de archivos procesados para búsqueda: al añadir un archivo, se parsea, se trocea en chunks, se generan embeddings y se guarda en un índice que soporta coincidencias por palabras clave y por semántica, lo que permite recuperar pasajes relevantes dado un prompt o pregunta.\n",
    "\n",
    "Estos almacenes pueden vincularse a asistentes o a conversaciones, habilitando la herramienta de búsqueda de archivos para que el modelo consulte directamente los propios datos; al adjuntarlo, las respuestas del agente pueden fundamentarse en los documentos, mejorando precisión y trazabilidad.\n",
    "\n",
    "Flujo típico:\n",
    "- Ingesta: crear el vector store y subir uno o varios archivos; la plataforma gestiona el análisis, chunking y embeddings de forma asíncrona, con estado visible en contadores de ingesta.\n",
    "- Consulta: la pregunta del usuario se convierte en vector y se ejecuta una búsqueda por similitud (p. ej., distancia coseno) sobre los chunks, devolviendo los más relevantes para que el modelo los use como contexto.\n",
    "- Gestión: se pueden añadir o eliminar archivos individualmente o por lotes (hasta 500 por batch), y hay límites como tamaño máximo por archivo (512 MB) y tope de tokens por archivo.\n",
    "\n",
    "Casos de uso comunes:\n",
    "- Búsqueda semántica y FAQ sobre documentación técnica o bases de conocimiento internas.\n",
    "- Asistentes de soporte que responden con fragmentos de manuales o políticas adjuntas al vector store.\n",
    "- Recomendación, clustering y clasificación basados en proximidad semántica de embeddings.\n",
    "\n",
    "Detalles operativos relevantes:\n",
    "- Adjuntar un vector store otorga capacidad de “file search” al asistente o al hilo, normalmente con un único vector store por asistente/hilo para control de ámbito.\n",
    "- La ingesta es asíncrona; los SDK proporcionan utilidades “create and poll” para esperar a que finalice antes de consultar.\n",
    "- Existen endpoints y objetos específicos para archivos y lotes de archivos dentro del vector store, con propiedades para auditar el estado y conteos de items procesados.\n",
    "\n",
    "En resumen, los objetos vector store son la pieza nativa de OpenAI para convertir archivos en contexto consultable mediante embeddings y similitud, permitiendo construir agentes y flujos RAG sin desplegar una base vectorial externa.\n",
    "\n",
    "El API de estos elementos se puede encontrar en este enlace:\n",
    "\n",
    "https://platform.openai.com/docs/api-reference/vector-stores/create\n",
    "\n",
    " continuación se muestran algunos ejemplos de cómo poder utilizar estos elementos:\n",
    "\n",
    " ### Creación de Vector Store\n",
    "\n",
    " ```python\n",
    "from openai import OpenAI\n",
    "\n",
    "client = OpenAI()\n",
    "\n",
    "# Crear un nuevo vector store\n",
    "vector_store = client.beta.vector_stores.create(\n",
    "    name=\"Mi Base de Conocimientos\",\n",
    "    expires_after={\n",
    "        \"anchor\": \"last_active_at\",\n",
    "        \"days\": 7\n",
    "    }\n",
    ")\n",
    "\n",
    "print(f\"Vector Store creado con ID: {vector_store.id}\")\n",
    "\n",
    "```\n",
    "\n",
    "### Subida de archivos.\n",
    "\n",
    "```python\n",
    "# Subir archivos uno por uno\n",
    "file = client.files.create(\n",
    "    file=open(\"documento.pdf\", \"rb\"),\n",
    "    purpose=\"assistants\"\n",
    ")\n",
    "\n",
    "# Añadir archivo al vector store\n",
    "vector_store_file = client.beta.vector_stores.files.create(\n",
    "    vector_store_id=vector_store.id,\n",
    "    file_id=file.id\n",
    ")\n",
    "\n",
    "# Subida masiva (más eficiente para múltiples archivos)\n",
    "file_paths = [\"doc1.pdf\", \"doc2.txt\", \"doc3.docx\"]\n",
    "file_streams = [open(path, \"rb\") for path in file_paths]\n",
    "\n",
    "file_batch = client.beta.vector_stores.file_batches.upload_and_poll(\n",
    "    vector_store_id=vector_store.id,\n",
    "    files=file_streams\n",
    ")\n",
    "\n",
    "print(f\"Batch procesado: {file_batch.status}\")\n",
    "\n",
    "```\n",
    "\n",
    "### Gestión de estados.\n",
    "```python\n",
    "# Monitorear el progreso de procesamiento\n",
    "import time\n",
    "\n",
    "def wait_for_processing(vector_store_id):\n",
    "    while True:\n",
    "        vs = client.beta.vector_stores.retrieve(vector_store_id)\n",
    "        print(f\"Estado: {vs.status}\")\n",
    "        print(f\"Archivos procesados: {vs.file_counts.completed}/{vs.file_counts.total}\")\n",
    "        \n",
    "        if vs.status == \"completed\":\n",
    "            break\n",
    "        elif vs.status == \"failed\":\n",
    "            raise Exception(\"Error en el procesamiento\")\n",
    "        \n",
    "        time.sleep(2)\n",
    "\n",
    "wait_for_processing(vector_store.id)\n",
    "\n",
    "```\n",
    "\n",
    "### Vinculación con asistentes\n",
    "\n",
    "```python\n",
    "# Crear asistente con vector store\n",
    "assistant = client.beta.assistants.create(\n",
    "    name=\"Asistente Documentos\",\n",
    "    instructions=\"Responde basándote en los documentos adjuntos\",\n",
    "    model=\"gpt-4-turbo\",\n",
    "    tools=[{\"type\": \"file_search\"}],\n",
    "    tool_resources={\n",
    "        \"file_search\": {\n",
    "            \"vector_store_ids\": [vector_store.id]\n",
    "        }\n",
    "    }\n",
    ")\n",
    "\n",
    "# O actualizar asistente existente\n",
    "client.beta.assistants.update(\n",
    "    assistant_id=assistant.id,\n",
    "    tool_resources={\n",
    "        \"file_search\": {\n",
    "            \"vector_store_ids\": [vector_store.id]\n",
    "        }\n",
    "    }\n",
    ")\n",
    "```\n",
    "\n",
    "### Consultas y búsquedas.\n",
    "\n",
    "```python\n",
    "# Crear hilo de conversación\n",
    "thread = client.beta.threads.create()\n",
    "\n",
    "# Hacer pregunta que active la búsqueda\n",
    "message = client.beta.threads.messages.create(\n",
    "    thread_id=thread.id,\n",
    "    role=\"user\",\n",
    "    content=\"¿Cuáles son las políticas de vacaciones según la documentación?\"\n",
    ")\n",
    "\n",
    "# Ejecutar asistente\n",
    "run = client.beta.threads.runs.create_and_poll(\n",
    "    thread_id=thread.id,\n",
    "    assistant_id=assistant.id\n",
    ")\n",
    "\n",
    "# Obtener respuesta\n",
    "messages = client.beta.threads.messages.list(thread_id=thread.id)\n",
    "print(messages.data[0].content[0].text.value)\n",
    "\n",
    "```\n",
    "### Administración de archivos.\n",
    "```python\n",
    "# Listar archivos en el vector store\n",
    "files = client.beta.vector_stores.files.list(\n",
    "    vector_store_id=vector_store.id\n",
    ")\n",
    "\n",
    "for file in files.data:\n",
    "    print(f\"Archivo: {file.id}, Estado: {file.status}\")\n",
    "\n",
    "# Eliminar archivo específico\n",
    "client.beta.vector_stores.files.delete(\n",
    "    vector_store_id=vector_store.id,\n",
    "    file_id=file.id\n",
    ")\n",
    "\n",
    "# Obtener información detallada\n",
    "file_info = client.beta.vector_stores.files.retrieve(\n",
    "    vector_store_id=vector_store.id,\n",
    "    file_id=file.id\n",
    ")\n",
    "\n",
    "```\n",
    "\n",
    "### Gestión completa del ciclo de vida.\n",
    "\n",
    "```python\n",
    "class VectorStoreManager:\n",
    "    def __init__(self, client):\n",
    "        self.client = client\n",
    "    \n",
    "    def create_knowledge_base(self, name, documents):\n",
    "        # Crear vector store\n",
    "        vs = self.client.beta.vector_stores.create(name=name)\n",
    "        \n",
    "        # Subir documentos en lote\n",
    "        file_streams = [open(doc, \"rb\") for doc in documents]\n",
    "        batch = self.client.beta.vector_stores.file_batches.upload_and_poll(\n",
    "            vector_store_id=vs.id,\n",
    "            files=file_streams\n",
    "        )\n",
    "        \n",
    "        # Cerrar streams\n",
    "        for stream in file_streams:\n",
    "            stream.close()\n",
    "        \n",
    "        return vs.id if batch.status == \"completed\" else None\n",
    "    \n",
    "    def cleanup_expired(self):\n",
    "        # Listar todos los vector stores\n",
    "        stores = self.client.beta.vector_stores.list()\n",
    "        \n",
    "        for store in stores.data:\n",
    "            if store.status == \"expired\":\n",
    "                self.client.beta.vector_stores.delete(store.id)\n",
    "                print(f\"Eliminado vector store expirado: {store.id}\")\n",
    "\n",
    "# Uso\n",
    "manager = VectorStoreManager(client)\n",
    "vs_id = manager.create_knowledge_base(\n",
    "    \"Políticas Empresa\", \n",
    "    [\"politicas.pdf\", \"manual.docx\", \"procedimientos.txt\"]\n",
    ")\n",
    "\n",
    "```\n",
    "\n",
    "### Manejo de errores\n",
    "\n",
    "```python\n",
    "try:\n",
    "    # Operación con vector store\n",
    "    result = client.beta.vector_stores.files.create(\n",
    "        vector_store_id=\"vs_invalid\",\n",
    "        file_id=\"file_invalid\"\n",
    "    )\n",
    "except Exception as e:\n",
    "    if \"not found\" in str(e).lower():\n",
    "        print(\"Vector store o archivo no encontrado\")\n",
    "    elif \"limit\" in str(e).lower():\n",
    "        print(\"Límite de archivos alcanzado\")\n",
    "    else:\n",
    "        print(f\"Error inesperado: {e}\")\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "965388a9-e240-419e-95b5-7c0b1038efaf",
   "metadata": {},
   "source": [
    "## Caso de uso con Responses.\n",
    "\n",
    "En este apartado vamos a replicar el apartado anterior de leer un fichero y sacar información del mismo pero en este caso utilizando la API de Responses en lugar de assistant."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53cc8c41-a836-4e7b-8c11-9c7b12df65ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "file = client.files.create(\n",
    "    file = open(\"oidos.pdf\",\"rb\"),\n",
    "    purpose = 'assistants'\n",
    ")\n",
    "\n",
    "# imprimimos el identificador del fichero\n",
    "print(f\"El identificador del fichero es:{file.id}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "001d3908-b7d2-4a75-9b5b-b9b3803ac940",
   "metadata": {},
   "outputs": [],
   "source": [
    "response = client.responses.create(\n",
    "    model=\"gpt-4o-mini\",\n",
    "    input=[\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": [\n",
    "                {\n",
    "                    \"type\": \"input_text\",\n",
    "                    \"text\": \"¿Cuáles son los puntos principales de este documento?\"\n",
    "                },\n",
    "                {\n",
    "                    \"type\": \"input_file\",\n",
    "                    \"file_id\": file.id  # Aquí usas el file.id directamente\n",
    "                }\n",
    "            ]\n",
    "        }\n",
    "    ]\n",
    ")\n",
    "\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc80c401-432c-4e99-865d-db207a57a83f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Borramos el objero response\n",
    "response2 = client.responses.delete(response.id)\n",
    "print(response2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "691c3e40-1855-4585-a45c-9cf8b1c3b79c",
   "metadata": {},
   "source": [
    "Ahora realizamos el mismo procedimiento pero utilizando un vector store"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f911038-3e03-4b25-8e8c-e202e509317c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2. Crear vector store\n",
    "vector_store = client.vector_stores.create(\n",
    "    name=\"Documentos para análisis\"\n",
    ")\n",
    "\n",
    "# 3. Agregar archivo al vector store\n",
    "client.vector_stores.files.create(\n",
    "    vector_store_id=vector_store.id,\n",
    "    file_id=file.id\n",
    ")\n",
    "\n",
    "# 4. Usar el Responses API con el vector store\n",
    "response = client.responses.create(\n",
    "    model=\"gpt-4o-mini\",\n",
    "    tools=[{\n",
    "        \"type\": \"file_search\",\n",
    "        \"vector_store_ids\": [vector_store.id],\n",
    "        \"max_num_results\": 20\n",
    "    }],\n",
    "    input=\"¿Cuáles son los puntos principales de este documento?\",\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21d4f81c-b2c2-43a3-92d2-a20707de01cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c5814df-f7ad-4d1c-9bd4-417855d97094",
   "metadata": {},
   "outputs": [],
   "source": [
    "response.output[1].content[0].text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ffdffb8-17b1-4cc8-8201-220e40e0bf1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# borramos elobjeto response\n",
    "# Borramos el objero response\n",
    "response = client.responses.delete(response.id)\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e030f55-afea-4432-a287-f80ddd40e237",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Veamos cuantos vector Store tenemos\n",
    "vector_stores = client.vector_stores.list()\n",
    "print(vector_stores)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3c48c0a-4c1f-4960-8937-713581a0d008",
   "metadata": {},
   "source": [
    "Un codigo mas completo seria el siguiente:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d39e0dee-ab0f-49b4-8aae-87d7f7f86bae",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "\n",
    "def listar_vector_stores():\n",
    "    try:\n",
    "        # Obtener la lista de vector stores\n",
    "        vector_stores = client.vector_stores.list()\n",
    "        \n",
    "        if not vector_stores.data:\n",
    "            print(\"No tienes vector stores creados.\")\n",
    "            return\n",
    "        \n",
    "        print(f\"Encontrados {len(vector_stores.data)} vector stores:\")\n",
    "        print(\"=\" * 80)\n",
    "        \n",
    "        for i, vs in enumerate(vector_stores.data, 1):\n",
    "            # Convertir timestamp a fecha legible\n",
    "            created_date = datetime.fromtimestamp(vs.created_at).strftime(\"%Y-%m-%d %H:%M:%S\")\n",
    "            \n",
    "            print(f\"{i}. Vector Store:\")\n",
    "            print(f\"   ID: {vs.id}\")\n",
    "            print(f\"   Nombre: {vs.name or 'Sin nombre'}\")\n",
    "            print(f\"   Creado: {created_date}\")\n",
    "            print(f\"   Estado: {vs.status}\")\n",
    "            print(f\"   Archivos:\")\n",
    "            print(f\"     - Total: {vs.file_counts.total}\")\n",
    "            print(f\"     - En progreso: {vs.file_counts.in_progress}\")\n",
    "            print(f\"     - Completados: {vs.file_counts.completed}\")\n",
    "            print(f\"     - Fallidos: {vs.file_counts.failed}\")\n",
    "            print(f\"     - Cancelados: {vs.file_counts.cancelled}\")\n",
    "            print(f\"   Tamaño: {vs.usage_bytes} bytes ({vs.usage_bytes / (1024*1024):.2f} MB)\")\n",
    "            \n",
    "            # Si hay metadatos, mostrarlos\n",
    "            if hasattr(vs, 'metadata') and vs.metadata:\n",
    "                print(f\"   Metadatos: {vs.metadata}\")\n",
    "            \n",
    "            print(\"-\" * 80)\n",
    "            \n",
    "    except Exception as e:\n",
    "        print(f\"Error al obtener vector stores: {e}\")\n",
    "\n",
    "# Ejecutar la función\n",
    "listar_vector_stores()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31937c9b-cfb8-4aea-af47-485c1fd249da",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Borramos los cuatro vector store\n",
    "from datetime import datetime, timedelta\n",
    "import time\n",
    "\n",
    "def eliminar_todos_los_vector_stores():\n",
    "    \"\"\"\n",
    "    Elimina todos los vector stores de tu cuenta OpenAI\n",
    "    PRECAUCIÓN: Esta operación es irreversible\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Obtener lista de todos los vector stores\n",
    "        print(\"Obteniendo lista de vector stores...\")\n",
    "        vector_stores = client.vector_stores.list()\n",
    "        \n",
    "        if not vector_stores.data:\n",
    "            print(\"No se encontraron vector stores para eliminar.\")\n",
    "            return\n",
    "        \n",
    "        total = len(vector_stores.data)\n",
    "        print(f\"Se encontraron {total} vector stores.\")\n",
    "        \n",
    "        # Confirmación de seguridad\n",
    "        confirmacion = input(f\"\\n⚠️  ADVERTENCIA: Esto eliminará TODOS los {total} vector stores de tu cuenta.\\n\"\n",
    "                           \"Esta operación es IRREVERSIBLE.\\n\"\n",
    "                           \"Escribe 'ELIMINAR TODO' para confirmar: \")\n",
    "        \n",
    "        if confirmacion != \"ELIMINAR TODO\":\n",
    "            print(\"Operación cancelada.\")\n",
    "            return\n",
    "        \n",
    "        print(f\"\\nEliminando {total} vector stores...\")\n",
    "        eliminados = 0\n",
    "        errores = 0\n",
    "        \n",
    "        for i, vs in enumerate(vector_stores.data, 1):\n",
    "            try:\n",
    "                print(f\"[{i}/{total}] Eliminando vector store: {vs.id}\")\n",
    "                if vs.name:\n",
    "                    print(f\"    Nombre: {vs.name}\")\n",
    "                \n",
    "                # Eliminar el vector store\n",
    "                response = client.vector_stores.delete(vector_store_id=vs.id)\n",
    "                \n",
    "                if hasattr(response, 'deleted') and response.deleted:\n",
    "                    eliminados += 1\n",
    "                    print(f\"    ✅ Eliminado exitosamente\")\n",
    "                else:\n",
    "                    print(f\"    ❌ No se pudo confirmar la eliminación\")\n",
    "                    errores += 1\n",
    "                \n",
    "                # Pequeña pausa para evitar sobrecarga de la API\n",
    "                time.sleep(0.5)\n",
    "                \n",
    "            except Exception as e:\n",
    "                print(f\"    ❌ Error al eliminar {vs.id}: {e}\")\n",
    "                errores += 1\n",
    "        \n",
    "        print(f\"\\n📊 Resumen:\")\n",
    "        print(f\"   Vector stores eliminados: {eliminados}\")\n",
    "        print(f\"   Errores: {errores}\")\n",
    "        print(f\"   Total procesados: {eliminados + errores}\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Error general: {e}\")\n",
    "\n",
    "# Ejecutar la función\n",
    "eliminar_todos_los_vector_stores()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2664d85a-acb9-45b2-b71b-05cebf5ed9da",
   "metadata": {},
   "outputs": [],
   "source": [
    "# LISTAMOS AHORA TODOS LOS FILES\n",
    "\n",
    "from datetime import datetime\n",
    "\n",
    "\n",
    "def listar_archivos():\n",
    "    \"\"\"\n",
    "    Lista todos los archivos almacenados en tu cuenta OpenAI\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Obtener lista de todos los archivos\n",
    "        print(\"Obteniendo lista de archivos...\")\n",
    "        files = client.files.list()\n",
    "        \n",
    "        if not files.data:\n",
    "            print(\"No se encontraron archivos en tu cuenta.\")\n",
    "            return\n",
    "        \n",
    "        total = len(files.data)\n",
    "        print(f\"Se encontraron {total} archivos:\")\n",
    "        print(\"=\" * 80)\n",
    "        \n",
    "        for i, file in enumerate(files.data, 1):\n",
    "            # Convertir timestamp a fecha legible\n",
    "            created_date = datetime.fromtimestamp(file.created_at).strftime(\"%Y-%m-%d %H:%M:%S\")\n",
    "            size_mb = file.bytes / (1024 * 1024) if file.bytes > 0 else 0\n",
    "            \n",
    "            print(f\"{i}. Archivo:\")\n",
    "            print(f\"   ID: {file.id}\")\n",
    "            print(f\"   Nombre: {file.filename}\")\n",
    "            print(f\"   Propósito: {file.purpose}\")\n",
    "            print(f\"   Creado: {created_date}\")\n",
    "            print(f\"   Tamaño: {file.bytes} bytes ({size_mb:.2f} MB)\")\n",
    "            print(f\"   Estado: {getattr(file, 'status', 'N/A')}\")\n",
    "            print(\"-\" * 80)\n",
    "            \n",
    "    except Exception as e:\n",
    "        print(f\"Error al obtener archivos: {e}\")\n",
    "\n",
    "# Ejecutar la función\n",
    "listar_archivos()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8f8364c-e520-49f2-9abc-5958c9937f95",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Ahora un código para borrar los ficheros\n",
    "from openai import OpenAI\n",
    "from datetime import datetime, timedelta\n",
    "import time\n",
    "\n",
    "def eliminar_archivos_avanzado():\n",
    "    \"\"\"\n",
    "    Versión avanzada que permite filtrar por propósito, fecha, etc.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Obtener todos los archivos\n",
    "        files = client.files.list()\n",
    "        \n",
    "        if not files.data:\n",
    "            print(\"No se encontraron archivos.\")\n",
    "            return\n",
    "        \n",
    "        print(f\"Se encontraron {len(files.data)} archivos:\")\n",
    "        print(\"=\" * 80)\n",
    "        \n",
    "        # Mostrar información detallada agrupada por propósito\n",
    "        archivos_por_proposito = {}\n",
    "        for file in files.data:\n",
    "            purpose = file.purpose\n",
    "            if purpose not in archivos_por_proposito:\n",
    "                archivos_por_proposito[purpose] = []\n",
    "            archivos_por_proposito[purpose].append(file)\n",
    "        \n",
    "        for purpose, file_list in archivos_por_proposito.items():\n",
    "            total_size = sum(f.bytes for f in file_list)\n",
    "            size_mb = total_size / (1024 * 1024)\n",
    "            print(f\"📁 {purpose.upper()}: {len(file_list)} archivos ({size_mb:.2f} MB)\")\n",
    "            \n",
    "            for file in file_list[:3]:  # Mostrar solo los primeros 3\n",
    "                created_date = datetime.fromtimestamp(file.created_at)\n",
    "                print(f\"   • {file.filename} - {created_date.strftime('%Y-%m-%d')}\")\n",
    "            \n",
    "            if len(file_list) > 3:\n",
    "                print(f\"   ... y {len(file_list) - 3} archivos más\")\n",
    "            print()\n",
    "        \n",
    "        print(\"\\nOpciones de eliminación:\")\n",
    "        print(\"1. Eliminar TODOS los archivos\")\n",
    "        print(\"2. Eliminar archivos por propósito específico\")\n",
    "        print(\"3. Eliminar archivos más antiguos que X días\")\n",
    "        print(\"4. Eliminar archivos de un tamaño específico\")\n",
    "        print(\"5. Cancelar\")\n",
    "        \n",
    "        opcion = input(\"\\nSelecciona una opción (1-5): \")\n",
    "        \n",
    "        if opcion == \"1\":\n",
    "            eliminar_todos(files.data)\n",
    "        elif opcion == \"2\":\n",
    "            eliminar_por_proposito(archivos_por_proposito)\n",
    "        elif opcion == \"3\":\n",
    "            eliminar_por_fecha(files.data)\n",
    "        elif opcion == \"4\":\n",
    "            eliminar_por_tamaño(files.data)\n",
    "        else:\n",
    "            print(\"Operación cancelada.\")\n",
    "            \n",
    "    except Exception as e:\n",
    "        print(f\"Error: {e}\")\n",
    "\n",
    "def eliminar_todos(files):\n",
    "    \"\"\"Elimina todos los archivos\"\"\"\n",
    "    confirmacion = input(f\"\\n⚠️  Confirma que quieres eliminar TODOS los {len(files)} archivos.\\n\"\n",
    "                        \"Escribe 'SI ELIMINAR TODO': \")\n",
    "    \n",
    "    if confirmacion == \"SI ELIMINAR TODO\":\n",
    "        procesar_eliminacion(files)\n",
    "    else:\n",
    "        print(\"Cancelado.\")\n",
    "\n",
    "def eliminar_por_proposito(archivos_por_proposito):\n",
    "    \"\"\"Elimina archivos de un propósito específico\"\"\"\n",
    "    print(\"\\nPropósitos disponibles:\")\n",
    "    propositos = list(archivos_por_proposito.keys())\n",
    "    for i, purpose in enumerate(propositos, 1):\n",
    "        count = len(archivos_por_proposito[purpose])\n",
    "        print(f\"{i}. {purpose} ({count} archivos)\")\n",
    "    \n",
    "    try:\n",
    "        seleccion = int(input(\"\\nSelecciona el número del propósito: \")) - 1\n",
    "        if 0 <= seleccion < len(propositos):\n",
    "            purpose_elegido = propositos[seleccion]\n",
    "            archivos_elegidos = archivos_por_proposito[purpose_elegido]\n",
    "            \n",
    "            print(f\"\\nSe eliminarán {len(archivos_elegidos)} archivos con propósito '{purpose_elegido}':\")\n",
    "            for file in archivos_elegidos:\n",
    "                print(f\"  - {file.filename} ({file.id})\")\n",
    "            \n",
    "            confirmacion = input(f\"\\nConfirmar eliminación? (si/no): \")\n",
    "            if confirmacion.lower() == 'si':\n",
    "                procesar_eliminacion(archivos_elegidos)\n",
    "        else:\n",
    "            print(\"Selección inválida.\")\n",
    "    except ValueError:\n",
    "        print(\"Por favor, ingresa un número válido.\")\n",
    "\n",
    "def eliminar_por_fecha(files):\n",
    "    \"\"\"Elimina archivos más antiguos que X días\"\"\"\n",
    "    try:\n",
    "        dias = int(input(\"Eliminar archivos más antiguos que cuántos días? \"))\n",
    "        fecha_limite = datetime.now() - timedelta(days=dias)\n",
    "        \n",
    "        antiguos = [f for f in files \n",
    "                   if datetime.fromtimestamp(f.created_at) < fecha_limite]\n",
    "        \n",
    "        if not antiguos:\n",
    "            print(f\"No hay archivos más antiguos que {dias} días.\")\n",
    "            return\n",
    "        \n",
    "        print(f\"Se eliminarán {len(antiguos)} archivos más antiguos que {dias} días:\")\n",
    "        for file in antiguos:\n",
    "            fecha = datetime.fromtimestamp(file.created_at).strftime('%Y-%m-%d')\n",
    "            print(f\"  - {file.filename} - {fecha}\")\n",
    "        \n",
    "        confirmacion = input(f\"\\nConfirmar eliminación? (si/no): \")\n",
    "        if confirmacion.lower() == 'si':\n",
    "            procesar_eliminacion(antiguos)\n",
    "            \n",
    "    except ValueError:\n",
    "        print(\"Por favor, ingresa un número válido de días.\")\n",
    "\n",
    "def eliminar_por_tamaño(files):\n",
    "    \"\"\"Elimina archivos basado en criterios de tamaño\"\"\"\n",
    "    print(\"\\nOpciones de tamaño:\")\n",
    "    print(\"1. Archivos mayores a X MB\")\n",
    "    print(\"2. Archivos menores a X MB\")\n",
    "    print(\"3. Archivos de 0 bytes (vacíos)\")\n",
    "    \n",
    "    opcion = input(\"Selecciona una opción (1-3): \")\n",
    "    \n",
    "    if opcion == \"1\":\n",
    "        try:\n",
    "            mb_limite = float(input(\"Eliminar archivos mayores a cuántos MB? \"))\n",
    "            bytes_limite = mb_limite * 1024 * 1024\n",
    "            grandes = [f for f in files if f.bytes > bytes_limite]\n",
    "            \n",
    "            if grandes:\n",
    "                print(f\"Se eliminarán {len(grandes)} archivos mayores a {mb_limite} MB:\")\n",
    "                for file in grandes:\n",
    "                    size_mb = file.bytes / (1024 * 1024)\n",
    "                    print(f\"  - {file.filename} ({size_mb:.2f} MB)\")\n",
    "                \n",
    "                confirmacion = input(\"\\nConfirmar eliminación? (si/no): \")\n",
    "                if confirmacion.lower() == 'si':\n",
    "                    procesar_eliminacion(grandes)\n",
    "            else:\n",
    "                print(f\"No hay archivos mayores a {mb_limite} MB.\")\n",
    "        except ValueError:\n",
    "            print(\"Por favor, ingresa un número válido.\")\n",
    "            \n",
    "    elif opcion == \"3\":\n",
    "        vacios = [f for f in files if f.bytes == 0]\n",
    "        if vacios:\n",
    "            print(f\"Se eliminarán {len(vacios)} archivos vacíos:\")\n",
    "            for file in vacios:\n",
    "                print(f\"  - {file.filename}\")\n",
    "            \n",
    "            confirmacion = input(\"\\nConfirmar eliminación? (si/no): \")\n",
    "            if confirmacion.lower() == 'si':\n",
    "                procesar_eliminacion(vacios)\n",
    "        else:\n",
    "            print(\"No hay archivos vacíos.\")\n",
    "\n",
    "def procesar_eliminacion(archivos_a_eliminar):\n",
    "    \"\"\"Procesa la eliminación de los archivos seleccionados\"\"\"\n",
    "    total = len(archivos_a_eliminar)\n",
    "    eliminados = 0\n",
    "    errores = 0\n",
    "    \n",
    "    print(f\"\\nEliminando {total} archivos...\")\n",
    "    \n",
    "    for i, file in enumerate(archivos_a_eliminar, 1):\n",
    "        try:\n",
    "            print(f\"[{i}/{total}] Eliminando: {file.filename}\")\n",
    "            \n",
    "            response = client.files.delete(file.id)\n",
    "            \n",
    "            if hasattr(response, 'deleted') and response.deleted:\n",
    "                eliminados += 1\n",
    "                print(f\"    ✅ Eliminado\")\n",
    "            else:\n",
    "                errores += 1\n",
    "                print(f\"    ❌ Error en la eliminación\")\n",
    "            \n",
    "            # Pausa para no sobrecargar la API\n",
    "            time.sleep(0.2)\n",
    "            \n",
    "        except Exception as e:\n",
    "            errores += 1\n",
    "            print(f\"    ❌ Error: {e}\")\n",
    "    \n",
    "    print(f\"\\n📊 Resultado final:\")\n",
    "    print(f\"   Eliminados exitosamente: {eliminados}\")\n",
    "    print(f\"   Errores: {errores}\")\n",
    "\n",
    "# Ejecutar la versión avanzada\n",
    "eliminar_archivos_avanzado()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9b55298-5895-49d5-8ba8-aeac97586ae4",
   "metadata": {},
   "source": [
    "## Structred Outputs\n",
    "```{index} Structred Outputs,response_format\n",
    "```\n",
    "\n",
    "Los **Structured Outputs** son una funcionalidad que asegura que las salidas del modelo cumplan exactamente con los esquemas JSON Schema proporcionados por el desarrollador. A diferencia del modo JSON tradicional, esta característica garantiza el cumplimiento del esquema al 100%. También puede decirse que los **Structured Output* es una capacidad reciente de la API de OpenAI que permite que los modelos generen salidas que cumplan exactamente un esquema JSON definido por el desarrollador. Es decir, en vez de depender de que el modelo “imite” un formato, podemos forzar que la salida obedezca una estructura dada.\n",
    "\n",
    "OpenAI lo presenta como una mejora respecto al “JSON mode” anterior: con Structured Outputs, el modelo garantiza que el formato siga el esquema proporcionado.\n",
    "\n",
    "Esto se logra mediante decodificación con restricciones (constrained decoding), es decir, el modelo no puede salirse de la gramática/estructura permitida.\n",
    "\n",
    "## Bneficios y casos de uso.\n",
    "\n",
    "**Algunos de los beneficios**:\n",
    "\n",
    "* Fiabilidad en la salida: elimina muchos errores de formato.\n",
    "\n",
    "* Facilidad para integrarse con sistemas que esperan datos estructurados (bases de datos, APIs, frontends).\n",
    "\n",
    "* Menor necesidad de “parsing” manual o correcciones posteriores.\n",
    "\n",
    "* Permite separar la lógica de razonamiento del “empaquetado” del resultado.\n",
    "\n",
    "* Compatible con llamadas a funciones (“tool calling”) para hacer flujos más robustos.\n",
    "\n",
    "Casos de uso típicos:\n",
    "\n",
    "* Extracción de datos de documentos (por ejemplo: nombres, fechas, direcciones).\n",
    "\n",
    "* Respuestas de APIs conversacionales donde se necesita un formato predecible.\n",
    "\n",
    "* Automatización de flujos multi-paso con agentes que llaman funciones internamente.\n",
    "\n",
    "* Generación de interfaces dinámicas basadas en la intención del usuario (por ejemplo: generar un formulario JSON a partir de la consulta).\n",
    "\n",
    "## Modelos compatibles:\n",
    "\n",
    "Los Structured Outputs están disponibles en los siguientes modelos :\n",
    "\n",
    "* gpt-4o-2024-08-06 (más reciente)\n",
    "\n",
    "* gpt-4o-mini\n",
    "\n",
    "* gpt-4o-mini-2024-07-18\n",
    "\n",
    "* Modelos fine-tuned basados en estos\n",
    "\n",
    "## Cómo habilitar Structured Outputs.\n",
    "\n",
    "Hay dos modos principales de usarlo:\n",
    "\n",
    "### Usando response_format + JSON Schema.\n",
    "\n",
    "Puedes pedirle al modelo que responda bajo un esquema JSON concreto, pasando un objeto **response_format**. Este objeto contiene un tipo \"json_schema\" y la especificación del esquema JSON que el modelo debe obedecer.\n",
    "\n",
    "Por ejemplo, en una petición HTTP (o vía SDK):\n",
    "\n",
    "```json\n",
    "{\n",
    "  \"model\": \"gpt-4o-2024-08-06\",\n",
    "  \"messages\": [\n",
    "    { \"role\": \"system\", \"content\": \"...\" },\n",
    "    { \"role\": \"user\", \"content\": \"...\" }\n",
    "  ],\n",
    "  \"response_format\": {\n",
    "    \"type\": \"json_schema\",\n",
    "    \"json_schema\": {\n",
    "      \"name\": \"mi_esquema\",\n",
    "      \"schema\": {\n",
    "        \"type\": \"object\",\n",
    "        \"properties\": {\n",
    "          \"campo1\": { \"type\": \"string\" },\n",
    "          \"campo2\": { \"type\": \"integer\" }\n",
    "        },\n",
    "        \"required\": [\"campo1\", \"campo2\"]\n",
    "      }\n",
    "    }\n",
    "  }\n",
    "}\n",
    "\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26519a4f-6265-4500-834f-9aee62620ed3",
   "metadata": {},
   "source": [
    "Cuando haces esto, el modelo garantiza que la salida será un JSON válido que respete ese esquema (o fallará con un rechazo/refusal).\n",
    "\n",
    "Importante: esta funcionalidad (con response_format) está disponible en modelos como gpt-4o-mini y gpt-4o-2024-08-06 y sus fine-tunes basados en ellos."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4a1484f-133b-4757-9572-da3659a912c8",
   "metadata": {},
   "source": [
    "### Usando llamadas a funciones (tool calling) con strict: true\n",
    "\n",
    "Otra forma poderosa es usar la funcionalidad de function calling (herramientas) de la API, combinada con strict: true en la definición de la función. De esta forma, cuando el modelo “llama” a la función generando argumentos, esos argumentos deben ajustarse al esquema de la función.\n",
    "Por ejemplo, defines una función con parámetros estructurados:\n",
    "\n",
    "```Python\n",
    "from pydantic import BaseModel\n",
    "\n",
    "class WeatherQuery(BaseModel):\n",
    "    location: str\n",
    "    date: str\n",
    "\n",
    "tools = [\n",
    "  openai.pydantic_function_tool(WeatherQuery, strict=True)\n",
    "]\n",
    "\n",
    "response = client.chat.completions.create(\n",
    "  model=\"gpt-4o-2024-08-06\",\n",
    "  messages=[...],\n",
    "  tools=tools\n",
    ")\n",
    "\n",
    "```\n",
    "\n",
    "El modelo generará una llamada a esa función con argumentos que satisfacen el esquema. \n",
    "\n",
    "**Advertencia**: no funciona bien si se permiten llamadas paralelas (parallel tool calls). Cuando se usa strict: true, se recomienda desactivar paralelismo.\n",
    "\n",
    "### Ejemplos prácticos:\n",
    "\n",
    "Supongamos que queremos que el modelo clasifique el sentimiento de una reseña como \"positive\", \"negative\" o \"neutral\", y que la salida sea:\n",
    "\n",
    "```json\n",
    "{\n",
    "  \"sentiment\": \"positive\"\n",
    "}\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "fb85bda2-7e0e-4437-91e6-103cca4501ae",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\"sentiment\":\"neutral\"}\n"
     ]
    }
   ],
   "source": [
    "from pydantic import BaseModel\n",
    "from typing import Literal\n",
    "from openai import OpenAI\n",
    "\n",
    "class SentimentResponse(BaseModel):\n",
    "    sentiment: Literal[\"positive\", \"negative\", \"neutral\"]\n",
    "\n",
    "#client = OpenAI()\n",
    "\n",
    "response = client.chat.completions.parse(\n",
    "    model=\"gpt-4o-2024-08-06\",\n",
    "    messages=[\n",
    "      {\"role\": \"system\", \"content\": \"Eres un clasificador de sentimiento.\"},\n",
    "      {\"role\": \"user\", \"content\": \"The food was amazing but the room was small.\"}\n",
    "    ],\n",
    "    response_format=SentimentResponse\n",
    ")\n",
    "print(response.choices[0].message.content)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4dc38832-f28e-4a81-9346-78640fd0284a",
   "metadata": {},
   "source": [
    "#### Resolver un problema matemático.\n",
    "\n",
    "Imagina que quieres que el modelo devuelva una serie de pasos para resolver un problema:\n",
    "\n",
    "El esquema podría ser:\n",
    "\n",
    "```json\n",
    "{\n",
    "  \"steps\": [\n",
    "    {\n",
    "      \"explanation\": \"texto explicativo\",\n",
    "      \"output\": \"resultado en esa etapa\"\n",
    "    }\n",
    "  ],\n",
    "  \"final_answer\": \"respuesta final\"\n",
    "}\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ee5139a-13e0-4e7a-a424-8ed1418ca648",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Step(BaseModel):\n",
    "    explanation: str\n",
    "    output: str\n",
    "\n",
    "class MathReasoning(BaseModel):\n",
    "    steps: list[Step]\n",
    "    final_answer: str\n",
    "\n",
    "response = client.chat.completions.parse(\n",
    "    model=\"gpt-4o-2024-08-06\",\n",
    "    messages=[...],\n",
    "    response_format=MathReasoning\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b3d36a4-4abf-4f5e-ba86-52c5f698a837",
   "metadata": {},
   "source": [
    "El modelo devolverá un objeto con la propiedad steps (array de objetos con explanation y output) y el campo final_answer.\n",
    "\n",
    "Veamos un ejemplo básico con Pydantic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b60d64e0-30c1-4346-97b2-d85090d6fc58",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Feria de Ciencias\n"
     ]
    }
   ],
   "source": [
    "from pydantic import BaseModel\n",
    "from typing import List\n",
    "\n",
    "class EventoCalendario(BaseModel):\n",
    "    nombre: str\n",
    "    fecha: str\n",
    "    participantes: List[str]\n",
    "\n",
    "# Usando la API Responses\n",
    "response = client.responses.parse(\n",
    "    model=\"gpt-4o-2024-08-06\",\n",
    "    input=[\n",
    "        {\"role\": \"system\", \"content\": \"Extrae la información del evento.\"},\n",
    "        {\n",
    "            \"role\": \"user\", \n",
    "            \"content\": \"Ana y Bob van a una feria de ciencias el viernes.\"\n",
    "        }\n",
    "    ],\n",
    "    text_format=EventoCalendario,\n",
    ")\n",
    "\n",
    "evento = response.output_parsed\n",
    "print(evento.nombre)  # Acceso tipado al resultado\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "44c8fd26-da24-4192-81fe-14a68b10cbe6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "EventoCalendario(nombre='Feria de Ciencias', fecha='Viernes', participantes=['Ana', 'Bob'])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(evento)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ecc0ad1f-e6e6-47e2-8d49-e0186e550ddb",
   "metadata": {},
   "source": [
    "A continuación vemos un ejemplo con curl y JSON Schema\n",
    "```json\n",
    "curl https://api.openai.com/v1/responses \\\n",
    "-H \"Authorization: Bearer $OPENAI_API_KEY\" \\\n",
    "-H \"Content-Type: application/json\" \\\n",
    "-d '{\n",
    "  \"model\": \"gpt-4o-2024-08-06\",\n",
    "  \"input\": [\n",
    "    {\n",
    "      \"role\": \"system\",\n",
    "      \"content\": \"Eres un tutor de matemáticas. Guía al usuario paso a paso.\"\n",
    "    },\n",
    "    {\n",
    "      \"role\": \"user\",\n",
    "      \"content\": \"¿cómo puedo resolver 8x + 7 = -23?\"\n",
    "    }\n",
    "  ],\n",
    "  \"text\": {\n",
    "    \"format\": {\n",
    "      \"type\": \"json_schema\",\n",
    "      \"name\": \"razonamiento_matematico\",\n",
    "      \"schema\": {\n",
    "        \"type\": \"object\",\n",
    "        \"properties\": {\n",
    "          \"pasos\": {\n",
    "            \"type\": \"array\",\n",
    "            \"items\": {\n",
    "              \"type\": \"object\",\n",
    "              \"properties\": {\n",
    "                \"explicacion\": { \"type\": \"string\" },\n",
    "                \"resultado\": { \"type\": \"string\" }\n",
    "              },\n",
    "              \"required\": [\"explicacion\", \"resultado\"],\n",
    "              \"additionalProperties\": false\n",
    "            }\n",
    "          },\n",
    "          \"respuesta_final\": { \"type\": \"string\" }\n",
    "        },\n",
    "        \"required\": [\"pasos\", \"respuesta_final\"],\n",
    "        \"additionalProperties\": false\n",
    "      },\n",
    "      \"strict\": true\n",
    "    }\n",
    "  }\n",
    "}'\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d709b2e-14f3-4d62-8da9-04e59477dcd8",
   "metadata": {},
   "source": [
    "Otro ejemplo de un profesor de matemáticas:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "1532e5d3-8276-4215-bbbf-c2dd6458f395",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Paso: Comienza aislando el término que contiene la variable. Restamos 7 de ambos lados de la ecuación para eliminar el 7 de la izquierda.\n",
      "Resultado: 8x + 7 - 7 = -23 - 7\n",
      "Paso: Simplifica ambos lados de la ecuación después de restar.\n",
      "Resultado: 8x = -30\n",
      "Paso: Ahora, resuelve para x dividiendo ambos lados de la ecuación por 8.\n",
      "Resultado: 8x / 8 = -30 / 8\n",
      "Paso: Simplifica la fracción en el lado derecho de la ecuación.\n",
      "Resultado: x = -15/4 o x = -3.75\n",
      "Respuesta final: x = -3.75\n"
     ]
    }
   ],
   "source": [
    "from pydantic import BaseModel\n",
    "from typing import List\n",
    "\n",
    "class Paso(BaseModel):\n",
    "    explicacion: str\n",
    "    resultado: str\n",
    "\n",
    "class RazonamientoMatematico(BaseModel):\n",
    "    pasos: List[Paso]\n",
    "    respuesta_final: str\n",
    "\n",
    "response = client.responses.parse(\n",
    "    model=\"gpt-4o-2024-08-06\",\n",
    "    input=[\n",
    "        {\n",
    "            \"role\": \"system\",\n",
    "            \"content\": \"Eres un tutor de matemáticas. Guía al usuario paso a paso.\"\n",
    "        },\n",
    "        {\"role\": \"user\", \"content\": \"¿cómo puedo resolver 8x + 7 = -23?\"}\n",
    "    ],\n",
    "    text_format=RazonamientoMatematico,\n",
    ")\n",
    "\n",
    "razonamiento = response.output_parsed\n",
    "for paso in razonamiento.pasos:\n",
    "    print(f\"Paso: {paso.explicacion}\")\n",
    "    print(f\"Resultado: {paso.resultado}\")\n",
    "print(f\"Respuesta final: {razonamiento.respuesta_final}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b3ab684-42c4-41f2-9d50-52837c7b1270",
   "metadata": {},
   "source": [
    "Extracción de datos estructurados"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "b24ab9a7-fe16-4d2f-9138-5841868a06e6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Título: Impacto de la Nutrición en el Desarrollo Cognitivo Infantil: Un Estudio Longitudinal\n",
      "Autores: Dra. Ana López, Dr. Carlos Méndez, Dra. Lucía García\n"
     ]
    }
   ],
   "source": [
    "from pydantic import BaseModel\n",
    "from typing import List\n",
    "\n",
    "class ArticuloInvestigacion(BaseModel):\n",
    "    titulo: str\n",
    "    autores: List[str]\n",
    "    resumen: str\n",
    "    palabras_clave: List[str]\n",
    "\n",
    "response = client.responses.parse(\n",
    "    model=\"gpt-4o-2024-08-06\",\n",
    "    input=[\n",
    "        {\n",
    "            \"role\": \"system\",\n",
    "            \"content\": \"Eres experto en extracción de datos estructurados. Convierte el texto no estructurado de un artículo de investigación en la estructura dada.\"\n",
    "        },\n",
    "        {\n",
    "            \"role\": \"user\", \n",
    "            \"content\": \"Texto del artículo de investigación aquí...\"\n",
    "        }\n",
    "    ],\n",
    "    text_format=ArticuloInvestigacion,\n",
    ")\n",
    "\n",
    "articulo = response.output_parsed\n",
    "print(f\"Título: {articulo.titulo}\")\n",
    "print(f\"Autores: {', '.join(articulo.autores)}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b996262f-f26d-4f45-8000-7f4f76e68ee3",
   "metadata": {},
   "source": [
    "Generación de Interfaces de Usuario"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "ed817ea0-495f-43e7-b642-6b835eff1565",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pydantic import BaseModel\n",
    "from typing import List\n",
    "from enum import Enum\n",
    "\n",
    "class TipoUI(str, Enum):\n",
    "    div = \"div\"\n",
    "    boton = \"button\"\n",
    "    header = \"header\"\n",
    "    seccion = \"section\"\n",
    "    campo = \"field\"\n",
    "    formulario = \"form\"\n",
    "\n",
    "class Atributo(BaseModel):\n",
    "    nombre: str\n",
    "    valor: str\n",
    "\n",
    "class ComponenteUI(BaseModel):\n",
    "    tipo: TipoUI\n",
    "    etiqueta: str\n",
    "    hijos: List['ComponenteUI']\n",
    "    atributos: List[Atributo]\n",
    "\n",
    "ComponenteUI.model_rebuild()  # Necesario para tipos recursivos\n",
    "\n",
    "response = client.responses.parse(\n",
    "    model=\"gpt-4o-2024-08-06\",\n",
    "    input=[\n",
    "        {\n",
    "            \"role\": \"system\",\n",
    "            \"content\": \"Eres un generador de UI. Convierte la entrada del usuario en una UI.\"\n",
    "        },\n",
    "        {\"role\": \"user\", \"content\": \"Crea un formulario de perfil de usuario\"}\n",
    "    ],\n",
    "    text_format=ComponenteUI,\n",
    ")\n",
    "\n",
    "ui = response.output_parsed\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ed0ae78-0fb2-4bdf-99ce-c925f9fa4098",
   "metadata": {},
   "outputs": [],
   "source": [
    "Sistema de Moderación de Contenido"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6658179-3977-415a-a6c4-11508c058bd9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pydantic import BaseModel\n",
    "from typing import Optional\n",
    "from enum import Enum\n",
    "\n",
    "class CategoriaViolacion(str, Enum):\n",
    "    violencia = \"violence\"\n",
    "    sexual = \"sexual\"\n",
    "    autolesion = \"self_harm\"\n",
    "\n",
    "class ComplianceContenido(BaseModel):\n",
    "    es_violacion: bool\n",
    "    categoria: Optional[CategoriaViolacion]\n",
    "    explicacion_si_viola: Optional[str]\n",
    "\n",
    "response = client.responses.parse(\n",
    "    model=\"gpt-4o-2024-08-06\",\n",
    "    input=[\n",
    "        {\n",
    "            \"role\": \"system\",\n",
    "            \"content\": \"Determina si la entrada del usuario viola directrices específicas y explica si las viola.\"\n",
    "        },\n",
    "        {\n",
    "            \"role\": \"user\", \n",
    "            \"content\": \"¿Cómo me preparo para una entrevista de trabajo?\"\n",
    "        }\n",
    "    ],\n",
    "    text_format=ComplianceContenido,\n",
    ")\n",
    "\n",
    "compliance = response.output_parsed\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14ebe411-aef1-4e20-ae87-47a02a07e655",
   "metadata": {},
   "outputs": [],
   "source": [
    "#### Manejo de rechazos y errores\n",
    "\n",
    "Gestión de Rechazos de Seguridad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20b7c04d-8000-4ab1-9305-68ee953aa21f",
   "metadata": {},
   "outputs": [],
   "source": [
    "response = client.responses.parse(\n",
    "    model=\"gpt-4o-2024-08-06\",\n",
    "    input=[\n",
    "        {\"role\": \"system\", \"content\": \"Responde al usuario de forma útil.\"},\n",
    "        {\"role\": \"user\", \"content\": \"Contenido potencialmente problemático\"}\n",
    "    ],\n",
    "    text_format=MiEsquema,\n",
    ")\n",
    "\n",
    "# Verificar si hubo rechazo por seguridad\n",
    "if hasattr(response, 'refusal') and response.refusal:\n",
    "    print(f\"Solicitud rechazada: {response.refusal}\")\n",
    "else:\n",
    "    resultado = response.output_parsed\n",
    "    # Procesar resultado normal\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51f79aea-aa42-49d6-84f8-e57000ce43b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "Manejo de Errores Comunes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb38c4f4-ccb5-4957-89c8-8ba3af61ff74",
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    response = client.responses.parse(\n",
    "        model=\"gpt-4o-2024-08-06\",\n",
    "        input=mensajes,\n",
    "        text_format=MiEsquema,\n",
    "    )\n",
    "    \n",
    "    if response.output_parsed:\n",
    "        datos = response.output_parsed\n",
    "        # Procesar datos exitosos\n",
    "    else:\n",
    "        print(\"No se pudo parsear la respuesta\")\n",
    "        \n",
    "except Exception as e:\n",
    "    print(f\"Error en la llamada a la API: {e}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab6f099a-e662-478c-80bb-fb4d54a9f83c",
   "metadata": {},
   "source": [
    "## Caso de uso: Generación de test\n",
    "\n",
    "En este apartado vamos a utilizar diversas herramientas que nos proporciona la API de OpenAI para obtener código que nos genere una serie de preguntas de tipo test con cuatro posibles respuestas y solo una de ellas la verdadera.\n",
    "\n",
    "Para proceder a conseguir esto, vamos a disponer de dos ficheros de tipo PDF denominados Tema_2_Mortalidad.pdf  y Tema_3_Natalidad.pdf, situados en la misma carpeta donde se encuentra el código y que nos van a servir como referencia para el desarrollo de este apartado.\n",
    "\n",
    "Lo primero que debemos hacer es subir estos ficheros a la plataforma de OpenAI para que los podamos ver en su Dashboard. Para conseguir esto lo hacemos de la siguiente manera (ver apartado anterior: La API de Files): (ver <a href=\"https://platform.openai.com/docs/api-reference/files/create\" target=\"_blank\"> esta referencia </a>)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7184a59-596e-46e1-acc2-94064b754507",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Esta sería una forma de hacerlo para cada uno de los ficheros\n",
    "from openai import OpenAI\n",
    "# Recordemos que en un blok anterior ya hemos creado el cliente\n",
    "\n",
    "\n",
    "#file = client.files.create(\n",
    "#  file=open(\"Tema_2_Mortalidad.pdf\", \"rb\"),\n",
    "#  purpose=\"assistants\",\n",
    "#  expires_after={\n",
    "#    \"anchor\": \"created_at\",\n",
    "#    \"seconds\": 4000\n",
    "  }\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "caf81445-d955-4024-84a8-f06b7e9e64db",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Archivo subido: Tema_2_Mortalidad.pdf\n",
      "  ID: file-FFoSFbTCe1jTuxXLhE13Vq\n",
      "  Filename: Tema_2_Mortalidad.pdf\n",
      "  Bytes: 1057614\n",
      "\n",
      "Archivo subido: Tema_3_Natalidad.pdf\n",
      "  ID: file-CFAhZ6STeupLYfknMKBctd\n",
      "  Filename: Tema_3_Natalidad.pdf\n",
      "  Bytes: 692116\n",
      "\n",
      "Ambos archivos subidos exitosamente. IDs: ['file-FFoSFbTCe1jTuxXLhE13Vq', 'file-CFAhZ6STeupLYfknMKBctd']\n"
     ]
    }
   ],
   "source": [
    "# Pero si queremos subir los dos con un sólo código lo hacemos de la siguiente manera\n",
    "file_paths = [\"Tema_2_Mortalidad.pdf\", \"Tema_3_Natalidad.pdf\"] \n",
    "\n",
    "file_ids = []  # Para guardar IDs si los necesitas después\n",
    "\n",
    "for path in file_paths:\n",
    "    uploaded_file = client.files.create(\n",
    "        file = open(path,\"rb\"),\n",
    "        purpose = \"assistants\"  # Para uso con assistants/tools/vector stores        \n",
    "    )\n",
    "    file_ids.append(uploaded_file.id)\n",
    "    print(f\"Archivo subido: {path}\")\n",
    "    print(f\"  ID: {uploaded_file.id}\")\n",
    "    print(f\"  Filename: {uploaded_file.filename}\")\n",
    "    print(f\"  Bytes: {uploaded_file.bytes}\\n\")    \n",
    "\n",
    "print(\"Ambos archivos subidos exitosamente. IDs:\", file_ids)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86042578-f937-44a2-a02f-a3c01e84a7ea",
   "metadata": {},
   "source": [
    "Si ahora nos vamos al Dashboard de OpenAi y elegimos \"Storage\" como opción del menú, podemos ver estos dos archivos. Hay que tener en cuenta que por la utilización de los vestor stores que usaremos más adelante OpenAi cobra por ello, aunque tiene una parte gratuita (en el momento de redactar estas líneas). En concreto:\n",
    "\n",
    "En OpenAI, mantener archivos subidos en el endpoint Files no tiene coste de almacenamiento publicado; el cargo aparece cuando esos archivos se añaden a un vector store, que cuesta 0,10 USD por GB de almacenamiento vectorial al día **con el primer GB gratis**.​\n",
    "\n",
    "**Archivos en Files**:\n",
    "\n",
    "* La tabla oficial de precios no publica ninguna tarifa por “almacenar” archivos en el endpoint Files; los cargos publicados para documentos aparecen cuando se usa File Search/Vector Stores, no por el mero hecho de subirlos.​\n",
    "\n",
    "* En la práctica, la comunidad y documentación informal confirman que puedes mantener archivos subidos sin facturación por almacenamiento, y los costes empiezan al procesarlos en Vector Stores o usar herramientas sobre ellos.​\n",
    "\n",
    "**Vector store**.\n",
    "\n",
    "El almacenamiento vectorial cuesta 0,10 USD por GB al día, con *1 GB gratuito de almacenamiento vectorial* por cuenta, aplicándose al tamaño procesado (fragmentos + embeddings) que el sistema mantiene para búsqueda.​\n",
    "**Otros cargos relacionados**.\n",
    "\n",
    "Llamadas a la herramienta de File Search cuando se usa via Responses API tienen un coste de 2,50 USD por 2.000 llamadas, además de los tokens del modelo para el contenido recuperado.​\n",
    "\n",
    "Los “search content tokens” que se inyectan al modelo desde el índice se cobran a las tarifas de tokens del modelo elegido.​\n",
    "\n",
    "**Consideraciones útiles**.\n",
    "\n",
    "No hay “doble pago” por tener un archivo en Files y además en un vector store; el cargo relevante es el del vector store y, si aplica, el de llamadas a File Search via Responses.​\n",
    "\n",
    "Si adjuntas el mismo archivo a varios vector stores, el almacenamiento vectorial se factura por cada vector store de forma independiente.​\n",
    "\n",
    "### Cración de un vector store.\n",
    "\n",
    "Una vez subidos esos ficheros a la plataforma  de OpenAI, vamos a <a href=\"https://platform.openai.com/docs/api-reference/vector-stores\" traget=\"_blank\"> crear un vector store </a> donde posteriormente vamos a almacenar  los embedings de esos dos ficheros. A este vector store, lo vamos a llamar \"almacenamiento\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "09c256a8-685f-41ce-8e34-846f12e0a650",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "VectorStore(id='vs_690c2e2f2e608191be88de11bea3c73e', created_at=1762405935, file_counts=FileCounts(cancelled=0, completed=0, failed=0, in_progress=0, total=0), last_active_at=1762405935, metadata={}, name='almacenamiento', object='vector_store', status='completed', usage_bytes=0, expires_after=None, expires_at=None, description=None)\n"
     ]
    }
   ],
   "source": [
    "vector_store = client.vector_stores.create(\n",
    "  name=\"almacenamiento\"\n",
    ")\n",
    "print(vector_store)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7dca5329-b2b6-4348-9086-180f3366e874",
   "metadata": {},
   "source": [
    "Este vector store que acabamos de crear, lo podemos ver en el Dashboard de OpenAI, en el mismo sitio que hemos visto los Files anteriores, pero haciendo clock en la solapa \"Vector stores\".\n",
    "\n",
    "Bien una vez creado este vector stores, que no es más que un lugar de almacenamiento, ahora debemos cargar los dos ficheros anteriores en el vector store \"almacenamiento\", creado anteriormente. La API que rige esto la <a href=\"https://platform.openai.com/docs/api-reference/vector-stores-files/createFile\" target=\"_blank\"> podemos ver en este enlace </a> .\n",
    "\n",
    "Es muy importante tener en cuenta que para procesos posteriores relacionados con búsquedas es muy importante definir *metadatos* con la opción *atributes* de esta clase: Veamos cómo hacemos esto :\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "a050f8b9-f558-4577-a975-2b7fb759a021",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'file_id' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[15], line 17\u001b[0m\n\u001b[0;32m      8\u001b[0m vector_store_file \u001b[38;5;241m=\u001b[39mclient\u001b[38;5;241m.\u001b[39mvector_stores\u001b[38;5;241m.\u001b[39mfiles\u001b[38;5;241m.\u001b[39mcreate(\n\u001b[0;32m      9\u001b[0m     vector_store_id\u001b[38;5;241m=\u001b[39mvector_store_id,\n\u001b[0;32m     10\u001b[0m     file_id\u001b[38;5;241m=\u001b[39mfile_ids[\u001b[38;5;241m0\u001b[39m],\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     14\u001b[0m     }\n\u001b[0;32m     15\u001b[0m )\n\u001b[0;32m     16\u001b[0m vector_store_file_ids\u001b[38;5;241m.\u001b[39mappend(vector_store_file\u001b[38;5;241m.\u001b[39mid)\n\u001b[1;32m---> 17\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mArchivo \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[43mfile_id\u001b[49m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m adjuntado: vector_store_file_id \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mvector_store_file\u001b[38;5;241m.\u001b[39mid\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, status inicial: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mvector_store_file\u001b[38;5;241m.\u001b[39mstatus\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     19\u001b[0m \u001b[38;5;66;03m# Lo hacemos para el segundo FILe\u001b[39;00m\n\u001b[0;32m     20\u001b[0m vector_store_file \u001b[38;5;241m=\u001b[39mclient\u001b[38;5;241m.\u001b[39mvector_stores\u001b[38;5;241m.\u001b[39mfiles\u001b[38;5;241m.\u001b[39mcreate(\n\u001b[0;32m     21\u001b[0m     vector_store_id\u001b[38;5;241m=\u001b[39mvector_store_id,\n\u001b[0;32m     22\u001b[0m     file_id\u001b[38;5;241m=\u001b[39mfile_ids[\u001b[38;5;241m1\u001b[39m],\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     26\u001b[0m     }\n\u001b[0;32m     27\u001b[0m )\n",
      "\u001b[1;31mNameError\u001b[0m: name 'file_id' is not defined"
     ]
    }
   ],
   "source": [
    "# obtenemos el ID del vector store creado anteriomente\n",
    "vector_store_id= vector_store.id\n",
    "\n",
    "vector_store_file_ids = []  # Para trackear\n",
    "#para cada elemento FILE identificadi por su ID\n",
    "# Recordar que file_ids es una lista de ID de los Files creados anteriormente\n",
    "# Lo hacemos para el primer FILe\n",
    "vector_store_file =client.vector_stores.files.create(\n",
    "    vector_store_id=vector_store_id,\n",
    "    file_id=file_ids[0],\n",
    "    attributes={\n",
    "        \"materia\":\"Demografia\",\n",
    "        \"Tema\":\"2\"\n",
    "    }\n",
    ")\n",
    "vector_store_file_ids.append(vector_store_file.id)\n",
    "print(f\"Archivo {file_id} adjuntado: vector_store_file_id {vector_store_file.id}, status inicial: {vector_store_file.status}\")\n",
    "\n",
    "# Lo hacemos para el segundo FILe\n",
    "vector_store_file =client.vector_stores.files.create(\n",
    "    vector_store_id=vector_store_id,\n",
    "    file_id=file_ids[1],\n",
    "    attributes={\n",
    "        \"materia\":\"Demografia\",\n",
    "        \"Tema\":\"3\"\n",
    "    }\n",
    ")\n",
    "\n",
    "vector_store_file_ids.append(vector_store_file.id)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "d6c61455-e7a0-4144-bb4d-db66fef26a65",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Archivo ['file-FFoSFbTCe1jTuxXLhE13Vq', 'file-CFAhZ6STeupLYfknMKBctd'] adjuntado: vector_store_file_id file-FFoSFbTCe1jTuxXLhE13Vq, status inicial: in_progress\n"
     ]
    }
   ],
   "source": [
    "print(f\"Archivo {file_ids} adjuntado: vector_store_file_id {vector_store_file.id}, status inicial: {vector_store_file.status}\")    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "7d8ba3d0-8062-4914-a0f4-36b129133aa1",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Status de file-FFoSFbTCe1jTuxXLhE13Vq (vsf_id: file-FFoSFbTCe1jTuxXLhE13Vq): completed\n",
      "Todos los archivos procesados.\n"
     ]
    }
   ],
   "source": [
    "# Paso 3: Pollear status para cada vector store file (opcional pero recomendado)\n",
    "for vsf_id, file_id in zip(vector_store_file_ids, file_ids):\n",
    "    while True:\n",
    "        vsf = client.vector_stores.files.retrieve(\n",
    "            vector_store_id=vector_store_id,\n",
    "            file_id=vsf_id\n",
    "        )\n",
    "        print(f\"Status de {file_id} (vsf_id: {vsf_id}): {vsf.status}\")\n",
    "        if vsf.status == \"completed\":\n",
    "            break\n",
    "        elif vsf.status == \"failed\":\n",
    "            print(f\"Error procesando {file_id}: {vsf.last_error}\")\n",
    "            break\n",
    "        time.sleep(5)\n",
    "\n",
    "print(\"Todos los archivos procesados.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3cdab05-ac91-4821-9bd3-a9c43d609b8e",
   "metadata": {},
   "source": [
    "Ahora que ya tenemos subidos todos los elementos que necesitamos, podemos crear las preguntas del test que necesitamos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "53e5cdc2-3d39-446a-882c-6c6bec61ec9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from openai import OpenAI\n",
    "from pydantic import BaseModel, Field\n",
    "from typing import List\n",
    "\n",
    "#client = OpenAI(api_key=\"tu_api_key\") # Ya lo tenemos creado de pasos anteriores\n",
    "\n",
    "# Schema JSON para structured outputs (mismo que antes)\n",
    "class Option(BaseModel):\n",
    "    text: str = Field(..., description=\"Una de las 4 opciones posibles\")\n",
    "\n",
    "class MCQ(BaseModel):\n",
    "    question: str = Field(..., description=\"La pregunta de opción múltiple\")\n",
    "    options: List[Option] = Field(..., description=\"Lista de 4 opciones (A, B, C, D)\")\n",
    "    correct_answer: int = Field(..., description=\"Índice de la respuesta correcta (0-3 para A-D)\")\n",
    "    explanation: str = Field(..., description=\"Explicación breve de por qué es correcta\")\n",
    "\n",
    "class MCQsResponse(BaseModel):\n",
    "    questions: List[MCQ] = Field(..., description=\"Lista de exactamente 10 preguntas generadas del vector store\")\n",
    "\n",
    "# Llamada corregida a Responses API\n",
    "response = client.responses.parse(\n",
    "    model=\"gpt-4o-mini\",  # O \"gpt-4o\" para precisión\n",
    "    input=\"Analiza el contenido de los archivos en el vector store y genera exactamente 5 preguntas de opción múltiple. Cada pregunta debe cubrir temas clave de los documentos, con 4 opciones (A, B, C, D) donde solo una es correcta. Asegúrate de que sean educativas y basadas en hechos del contenido.\",\n",
    "    tools=[\n",
    "        {\n",
    "            \"type\": \"file_search\",\n",
    "            \"vector_store_ids\": [\"vs_690c2e2f2e608191be88de11bea3c73e\"]  # ID de tu vector store con los 2 files\n",
    "        }\n",
    "    ],\n",
    "    text_format=MCQsResponse  # Fuerza salida estructurada\n",
    ")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "e1fe439f-b8be-48e8-892d-4ca6643141dd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ParsedResponse[MCQsResponse](id='resp_05043d2df129b7e000690c684226848191a84eb5a239eecf21', created_at=1762420802.0, error=None, incomplete_details=None, instructions=None, metadata={}, model='gpt-4o-mini-2024-07-18', object='response', output=[ResponseFileSearchToolCall(id='fs_05043d2df129b7e000690c684337a88191b595f3b41687ad41', queries=['Principales temas del documento', ' contenido clave del archivo', 'Resumen de los archivos subidos', 'Aspectos destacados de los documentos', 'Puntos importantes en el vector store'], status='completed', type='file_search_call', results=None), ResponseFileSearchToolCall(id='fs_05043d2df129b7e000690c684589188191baf78d3538b0cffb', queries=['mortalidad infantil', 'Tasa de Mortalidad por causas', 'Tablas de mortalidad', 'Análisis de la mortalidad', 'Efectos de envejecimiento en la mortalidad'], status='completed', type='file_search_call', results=None), ResponseFileSearchToolCall(id='fs_05043d2df129b7e000690c684822ec819187f97cdee7c6cdfb', queries=['mortalidad infantil definición', 'tasa de mortalidad', 'tipos de mortalidad', 'esperanza de vida', 'causas de mortalidad'], status='completed', type='file_search_call', results=None), ResponseFileSearchToolCall(id='fs_05043d2df129b7e000690c684a46bc81919ad94ab63d01ffd5', queries=['mortalidad infantil', 'esperanza de vida', 'tasas de mortalidad', 'tablas de mortalidad', 'causas de muerte'], status='completed', type='file_search_call', results=None), ResponseFileSearchToolCall(id='fs_05043d2df129b7e000690c684cf8648191b2871735db8374e3', queries=['Tasa de mortalidad infantil', 'Mortalidad neonatal', 'Esperanza de vida', 'Tablas de mortalidad', 'Causas de mortalidad'], status='completed', type='file_search_call', results=None), ResponseFileSearchToolCall(id='fs_05043d2df129b7e000690c684f520081919abbf06554a51c16', queries=['Mortalidad infantil definición', 'Tasa de mortalidad', 'Espereanza de vida', 'Causas de mortalidad', 'Tablas de mortalidad'], status='completed', type='file_search_call', results=None), ResponseFileSearchToolCall(id='fs_05043d2df129b7e000690c68512e4c8191827fa642de91b090', queries=['esperanza de vida definición', 'tasa de mortalidad infantil', 'análisis de mortalidad infantil', 'tablas de mortalidad', 'mortalidad por causas'], status='completed', type='file_search_call', results=None), ResponseFileSearchToolCall(id='fs_05043d2df129b7e000690c685321688191a74c2bb158745ca3', queries=['mortalidad infantil', 'causas de mortalidad', 'esperanza de vida', 'tablas de mortalidad'], status='completed', type='file_search_call', results=None), ResponseFileSearchToolCall(id='fs_05043d2df129b7e000690c68552734819191a609952ca075dd', queries=['Esperanza de vida en España', 'Causas de mortalidad infantil', 'Mortalidad neonatal y postneonatal', 'Tablas de mortalidad en España', 'Métodos de análisis de mortalidad'], status='completed', type='file_search_call', results=None), ResponseFileSearchToolCall(id='fs_05043d2df129b7e000690c685717ac8191a93e46857146813a', queries=['Esperanza de vida', 'Causas de mortalidad', 'Tasa de mortalidad infantil', 'Mortandad general en España', 'Cohorte de población y mortalidad'], status='completed', type='file_search_call', results=None), ResponseFileSearchToolCall(id='fs_05043d2df129b7e000690c6859b1a48191a81f634428b1ea03', queries=['Tasa de mortalidad infantil', 'Esperanza de vida', 'Tablas de mortalidad', 'Causas de mortalidad'], status='completed', type='file_search_call', results=None), ResponseFileSearchToolCall(id='fs_05043d2df129b7e000690c685c520c819190256259143cbacc', queries=['mortalidad infantil', 'tasa de mortalidad', 'esperanza de vida', 'análisis de mortalidad', 'tablas de mortalidad'], status='completed', type='file_search_call', results=None), ResponseFileSearchToolCall(id='fs_05043d2df129b7e000690c685e72608191a84b354636cd737c', queries=['Evolución de las tasas de mortalidad', 'Causas de mortalidad en España', 'Tasa de mortalidad infantil', 'Esperanza de vida en distintas edades', 'Métodos de análisis de mortalidad'], status='completed', type='file_search_call', results=None), ResponseFileSearchToolCall(id='fs_05043d2df129b7e000690c68606db881918ecf91a4c7e1feac', queries=['temas clave del documento', 'mortalidad infantil', 'esperanza de vida', 'análisis de mortalidad', 'cálculos de tasas de mortalidad'], status='completed', type='file_search_call', results=None), ResponseFileSearchToolCall(id='fs_05043d2df129b7e000690c686256a4819192555d8d36b6d9f2', queries=['concepto de mortalidad infantil', 'tasa de mortalidad', 'esperanza de vida en España', 'mortalidad neonatal', 'causas de defunciones en la infancia'], status='completed', type='file_search_call', results=None), ResponseFileSearchToolCall(id='fs_05043d2df129b7e000690c6864b1c081919681539886c6c64f', queries=['Causas de mortalidad infantil', 'Tasa de mortalidad infantil en España', 'Análisis de mortalidad en España', 'Esperanza de vida', 'Tablas de mortalidad'], status='completed', type='file_search_call', results=None), ResponseFileSearchToolCall(id='fs_05043d2df129b7e000690c68676b408191ba7d78c66f34bfe9', queries=['principales causas de mortalidad', 'mortalidad infantil en España', 'esperanza de vida', 'tablas de mortalidad', 'análisis de tasas de mortalidad'], status='completed', type='file_search_call', results=None), ResponseFileSearchToolCall(id='fs_05043d2df129b7e000690c686a2f988191a29977985993977f', queries=['Esperanza de vida definición', 'Tasa de mortalidad infantil', 'Causas de muerte infantil', 'Tablas de mortalidad en España', 'análisis de mortalidad en diferentes sexos'], status='completed', type='file_search_call', results=None), ResponseFileSearchToolCall(id='fs_05043d2df129b7e000690c686d5bf481918fff4e23b7d138d4', queries=['esperanza de vida', 'tasa de mortalidad infantil', 'tablas de mortalidad', 'mortalidad neonatal', 'análisis de mortalidad', 'causas de mortalidad en España'], status='completed', type='file_search_call', results=None), ResponseFileSearchToolCall(id='fs_05043d2df129b7e000690c68701e3081919280069a6676c945', queries=['Esperanza de vida en España', 'Causas de mortalidad infantil', 'Tasa de mortalidad infantil', 'Tablas de mortalidad en España', 'Métodos de análisis de mortalidad'], status='completed', type='file_search_call', results=None)], parallel_tool_calls=True, temperature=1.0, tool_choice='auto', tools=[FileSearchTool(type='file_search', vector_store_ids=['vs_690c2e2f2e608191be88de11bea3c73e'], filters=None, max_num_results=20, ranking_options=RankingOptions(ranker='auto', score_threshold=0.0))], top_p=1.0, background=False, conversation=None, max_output_tokens=None, max_tool_calls=None, previous_response_id=None, prompt=None, prompt_cache_key=None, reasoning=Reasoning(effort=None, generate_summary=None, summary=None), safety_identifier=None, service_tier='default', status='completed', text=ResponseTextConfig(format=ResponseFormatTextJSONSchemaConfig(name='MCQsResponse', schema_={'$defs': {'MCQ': {'properties': {'question': {'description': 'La pregunta de opción múltiple', 'title': 'Question', 'type': 'string'}, 'options': {'description': 'Lista de 4 opciones (A, B, C, D)', 'items': {'$ref': '#/$defs/Option'}, 'title': 'Options', 'type': 'array'}, 'correct_answer': {'description': 'Índice de la respuesta correcta (0-3 para A-D)', 'title': 'Correct Answer', 'type': 'integer'}, 'explanation': {'description': 'Explicación breve de por qué es correcta', 'title': 'Explanation', 'type': 'string'}}, 'required': ['question', 'options', 'correct_answer', 'explanation'], 'title': 'MCQ', 'type': 'object', 'additionalProperties': False}, 'Option': {'properties': {'text': {'description': 'Una de las 4 opciones posibles', 'title': 'Text', 'type': 'string'}}, 'required': ['text'], 'title': 'Option', 'type': 'object', 'additionalProperties': False}}, 'properties': {'questions': {'description': 'Lista de exactamente 10 preguntas generadas del vector store', 'items': {'$ref': '#/$defs/MCQ'}, 'title': 'Questions', 'type': 'array'}}, 'required': ['questions'], 'title': 'MCQsResponse', 'type': 'object', 'additionalProperties': False}, type='json_schema', description=None, strict=True), verbosity='medium'), top_logprobs=0, truncation='disabled', usage=ResponseUsage(input_tokens=16604, input_tokens_details=InputTokensDetails(cached_tokens=0), output_tokens=901, output_tokens_details=OutputTokensDetails(reasoning_tokens=0), total_tokens=17505), user=None, billing={'payer': 'developer'}, prompt_cache_retention=None, store=True)"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9479e1c-88d8-46aa-8f6a-730d93110642",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extrae y muestra las preguntas (igual que antes)\n",
    "mcqs = response.choices[0].output.parsed\n",
    "for i, q in enumerate(mcqs.questions, 1):\n",
    "    print(f\"Pregunta {i}: {q.question}\")\n",
    "    for j, opt in enumerate(q.options):\n",
    "        letter = chr(65 + j)  # A, B, C, D\n",
    "        print(f\"  {letter}. {opt.text}\")\n",
    "    correct_letter = chr(65 + q.correct_answer)\n",
    "    print(f\"  Respuesta correcta: {correct_letter} ({q.options[q.correct_answer].text})\")\n",
    "    print(f\"  Explicación: {q.explanation}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16f8a37b-2ac9-4c51-950c-a65625a6d49f",
   "metadata": {},
   "source": [
    "## Apéndice:\n",
    "\n",
    "* <a href=\"https://www.datacamp.com/tutorial/open-ai-assistants-api-tutorial\" target=\"_blank\"> Tutorial </a>\n",
    "\n",
    "* <a href=\"https://www.youtube.com/watch?v=wIQePhB5KZI\" target=\"_blank\"> Tutorial en inglés</a>\n",
    "\n",
    "* <a href=\"https://openai.com/es-ES/index/introducing-structured-outputs-in-the-api/\" target='_blank'>Introducción de outputs estructurados en la API </a>\n",
    "\n",
    "* <a href=\"https://www.youtube.com/watch?v=nbtJToGckQM&t=19s\" target=\"_blank\"> Vídeo sobre Structured Output</a>\n",
    "\n",
    "* <a href=\"https://www.youtube.com/watch?v=dIccw6fsuP4\" target=\"_blank\"> Vídeo sobre Structured Output</a>\n",
    "\n",
    "* <a href=\"https://www.youtube.com/watch?v=dIccw6fsuP4\" target=\"_blank\"> Vídeo sobre Structured Output</a>\n",
    "\n",
    "* <a href=\"https://www.youtube.com/watch?v=VUkBgnezdPs\" target=\"_blank\"> Agents SDK y Responses API en acción</a>\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
