{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "edb00af4-ea2a-4ab5-a8d0-6e8153b22fb5",
   "metadata": {},
   "source": [
    "# La Api de OpenAI.\n",
    "```{index} assistants\n",
    "```\n",
    "Los assistants de OpenAI representan una revolución en la integración de inteligencia artificial personalizada y conversacional en aplicaciones y servicios. Son agentes que utilizan modelos de lenguaje avanzados, como GPT-4 Turbo, para automatizar tareas, interactuar con usuarios en lenguaje natural y ejecutar funciones adaptadas según las necesidades específicas del contexto.\n",
    "\n",
    "## ¿Qué son los OpenAI Assistants?\n",
    "Un OpenAI Assistant es un agente digital configurable que utiliza modelos de lenguaje entrenados con grandes cantidades de datos para responder preguntas, ejecutar funciones, analizar información y brindar asistencia en una amplia variedad de tareas. Estos asistentes se configuran a través de instrucciones (que definen personalidad y comportamiento), conjuntos de datos personalizados, y funciones programadas por el desarrollador.\n",
    "\n",
    "Los asistentes son inteligencias artificiales generativas diseñadas para propósitos específicos, utilizando los modelos de OpenAI (como GPT-4 y GPT-3.5, etc.). Estos asistentes tienen acceso a archivos, pueden mantener el contexto de la conversación (historial) y pueden usar herramientas especializadas.\n",
    "\n",
    "## Características principales\n",
    "**Personalización:** Es posible definir el comportamiento y personalidad del asistente mediante instrucciones precisas, así como entrenarlo con datos propios (PDFs, artículos, bases de datos, etc.).\n",
    "\n",
    "**Función de llamada:** Los assistants pueden integrarse con funciones externas y APIs para acceder a información en tiempo real, automatizar cálculos o interactuar con otros servicios digitales.\n",
    "\n",
    "**Gestión de datos:** Capaces de procesar tanto datos estructurados (JSON, hojas de cálculo) como no estructurados (texto libre, documentos), extrayendo y transformando información relevante.\n",
    "\n",
    "**Escalabilidad:** La infraestructura de OpenAI permite atender miles de usuarios en simultáneo sin perder calidad en las respuestas.\n",
    "\n",
    "**Aprendizaje continuo:** Gracias a su naturaleza de entrenamiento, los assistants pueden mejorar su precisión y relevancia conforme reciben nuevos datos o retroalimentación.\n",
    "\n",
    "## Ventajas de utilizar OpenAI Assistants\n",
    "**Automatización y productividad:** Permiten gestionar tareas repetitivas, asistir en proyectos, analizar grandes volúmenes de datos y mejorar la atención al cliente.\n",
    "\n",
    "**Accesibilidad y apertura:** El enfoque suele ser abierto para desarrolladores, lo que facilita la colaboración, la creación de asistentes especializados y la rápida evolución tecnológica.\n",
    "\n",
    "**Integración sencilla:** Herramientas y APIs modernas permiten que los asistentes se integren fácilmente en aplicaciones web, equipos de trabajo y sistemas empresariales.\n",
    "\n",
    "Es de advertir que tal y como figura en su página oficial, en el momento de redactar este documento, esta api está *deprecated* y dejará de estar disponible el 26 de agosto de 2026, siendo sustituida por la api de *responses*. \n",
    "\n",
    "A pesar de todo esto se ha creído conveniente proceder a presentar esta las dos API's, con la finalidad de que el lector sepa de donde estamos y cuales serán los nuevos cambios hacia la nueva API.\n",
    "\n",
    "\n",
    "En la siguiente imagen mostramos una visualización que detalla cómo esta compuesta esta API. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db984c37-4ae4-4a66-8ebd-e16a32726540",
   "metadata": {},
   "source": [
    "![](fig/esquema.PNG/)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fcb9f7ec-ac35-494a-b1de-7900c1d2ffb4",
   "metadata": {},
   "source": [
    "En esa figura, observamos los siguientes componentes que pueden configurar el assitants de OpenAi:\n",
    "\n",
    "* Los modelos LLM, que puede ser cualquiera de los modelos que tiene OpenAi, en esta caso se hace referencia a los modelos GPT 4 ó GPT 3.5 turbo.\n",
    "\n",
    "* Un asistente va a tener un identificador (Id) que es único para cada asistente, un nombre, una descripción e instrucciones.\n",
    "\n",
    "* Herramientas o Tools. Las mas importantes son las de recuperación (retrieval), interprete de Código (para trabajar código python) y la llamada a funciones.\n",
    "\n",
    "* Los Thread o hilos de conversaciones. Cada hilo viene a ser una sesion conversación con un usuario. En cada hilo se guarda los promts del usuario y las respuestas correspondientes del asistente\n",
    "\n",
    "* Los Run son los engranajes entre los asistentes y los thread.\n",
    "\n",
    "\n",
    "A continuación vamos a mostrar un ejemplo de cómo podemos crear un asistente con la herramienta de retrieval.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24be9a54-726a-4840-be5e-e9b97f987175",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install --upgrade openai"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fdd3a8ce-7c1b-4556-a8e4-50f795a0c560",
   "metadata": {},
   "outputs": [],
   "source": [
    "# de esta forma obtenemos los resultados de forma más estética\n",
    "from pprint import pprint\n",
    "# importamos JSON para poder trabajar con este tipo de datos\n",
    "import json\n",
    "# importamos el paqute de openai\n",
    "from openai import OpenAI\n",
    "import time"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "499feea2-2db1-4f5c-973a-e57c8c310cbc",
   "metadata": {},
   "source": [
    "## Ficheros de configuración de claves API\n",
    "\n",
    "A continuación, lo que hacemos es recuperar la clave API de OpenAI, pero para no mostrarla aquí utilizaremos una técnica muy extendida entre los desarrolladores de Python, que consiste en generar un fichero denominado .env y que haciendo una pequeña digresión del tema central de este documento consiste en lo siguiente:\n",
    "Un archivo .env en Python es un archivo de texto plano que almacena variables de entorno, como contraseñas o claves API, para mantener la información sensible fuera del código fuente y del repositorio de control de versiones. Se utiliza con la biblioteca python-dotenv, la cual carga estos pares clave-valor en el entorno de ejecución de la aplicación. Para usarlo, se instala la biblioteca, se crea un archivo .env con las variables, y luego se utiliza os.getenv() para acceder a ellas en el código de Python. \n",
    "\n",
    "```{index} .env, ficheros de claves\n",
    "```\n",
    "### ¿Para qué se usan los archivos .env?\n",
    "\n",
    "* **Seguridad**: Permiten mantener información confidencial, como credenciales de bases de datos o claves de API, separada del código fuente, evitando que se exponga accidentalmente.\n",
    "  \n",
    "* **Configuración**: Facilitan la gestión de diferentes configuraciones para distintos entornos (desarrollo, pruebas, producción) sin tener que modificar el código.\n",
    "\n",
    "  \n",
    "* **Buenas prácticas**: Es una práctica recomendada para separar los datos de configuración del código, lo que hace el proyecto más limpio y seguro.\n",
    "\n",
    "### ¿Cómo se usan en Python?.\n",
    "\n",
    "Primero instalamos la biblioteca:\n",
    "\n",
    "``` Python\n",
    "    pip install python-dotenv\n",
    "```\n",
    "\n",
    "Se crea un archivo denominado .env enla raíz del proyecto y se añaden las variables de entorno en formato CLAVE = VALOR, por ejemplo:\n",
    "\n",
    "```{note}\n",
    "    API_KEY=tu_clave_secreta\n",
    "    DATABASE_URL=postgresql://usuario:contraseña@host:puerto/db\n",
    "```\n",
    " Y ahora ya podemos cargar y usar las variables de entorno en el código de la siguiente manera:\n",
    "\n",
    " ```Python\n",
    "    import os\n",
    "    from dotenv import load_dotenv\n",
    "\n",
    "    # Carga las variables del archivo .env en el entorno\n",
    "    load_dotenv()\n",
    "\n",
    "    # Accede a las variables con os.getenv()\n",
    "    api_key = os.getenv(\"API_KEY\")\n",
    "    db_url = os.getenv(\"DATABASE_URL\")\n",
    "\n",
    "    print(f\"La API Key es: {api_key}\")\n",
    "    print(f\"La URL de la base de datos es: {db_url}\")\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4b8fabb-5ba7-407b-b79f-33c02caac6bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()\n",
    "openai_key = os.getenv(\"openai_key\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38aae82a-69d8-4b0a-8268-339373580738",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Si queremos ver la api key\n",
    "#print(openai_key)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee1c772a-333b-4703-9cc6-4edfdd239ff9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creamos el cliente de OpenAI\n",
    "client = OpenAI(\n",
    "api_key = openai_key\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d1ec46b-6765-4837-94d6-e087bde1a0a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# paso 2: Cargar archivos que nuestro asistente necesita para el tipo retrieval\n",
    "# pueden ser de tipo .pdf, excel, worf, etc\n",
    "#El tamaño máximo del archivo debe ser 512 MB\n",
    "# máximo almacenamiento 100 GB\n",
    "# Cargamos un pequeño fichero .pdf que se encuenta en : https://www.semfyc.es/storage/wp-content/uploads/2021/12/02_Unidad_2020-8.pdf\n",
    "# El contenido del fichero es el dolor de oídos en personas adultas \n",
    "file = client.files.create(\n",
    "    file = open(\"oidos.pdf\",\"rb\"),\n",
    "    purpose = 'assistants'\n",
    ")\n",
    "\n",
    "# imprimimos el identificador del fichero\n",
    "print(f\"El identificador del fichero es:{file.id}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a48c35f-b3c5-4bc5-a940-ce8e7d1c9ab8",
   "metadata": {},
   "source": [
    "Este identificador también lo podemos ver en el *playground de OpenAI* (https://platform.openai.com/chat/edit?models=gpt-5) en el menú denominado: *Storage*.\n",
    "\n",
    "![](fig/Playground.PNG)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26810e14-9e17-4037-87c3-8451cf8ec5f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f47fbb4-0a23-469d-b89f-e775b9891f3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# creamos un asistente sin vector_stores\n",
    "\n",
    "assistant = client.beta.assistants.create(\n",
    "    name=\"Mi Assistant\",\n",
    "    instructions=\"Eres un otorrino de mucho prestigio y con grandes conocimientos sobre el funcionamiento del oído\",\n",
    "    model=\"gpt-4-turbo\",\n",
    "    tools=[{\"type\": \"file_search\"}]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "967a8408-0b2a-408e-b84f-49c127b6037a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3. Crear thread con archivo adjunto\n",
    "thread = client.beta.threads.create(\n",
    "    messages=[\n",
    "        {\n",
    "            \"role\": \"user\", \n",
    "            \"content\": \"¿Cuáles son los puntos principales de este documento?\",\n",
    "            \"attachments\": [\n",
    "                {\n",
    "                    \"file_id\": file.id,\n",
    "                    \"tools\": [{\"type\": \"file_search\"}]\n",
    "                }\n",
    "            ]\n",
    "        }\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3aee7609-c9de-43f0-b2cc-fc59dda62db5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4. Ejecutar \n",
    "run = client.beta.threads.runs.create(\n",
    "    thread_id=thread.id,\n",
    "    assistant_id=assistant.id\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0296a59-4d43-46c3-a111-573951d52119",
   "metadata": {},
   "outputs": [],
   "source": [
    "run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03f14c8e-4f8b-4e2b-ba86-074245ac8a2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Run creado: {run.id}\")\n",
    "print(f\"Estado inicial: {run.status}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c7dff99-a562-4fc7-aec9-f0f44321c3a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. ESPERAR A QUE TERMINE LA EJECUCIÓN\n",
    "while run.status in ['queued', 'in_progress', 'cancelling']:\n",
    "    time.sleep(1)  # Esperar 1 segundo\n",
    "    run = client.beta.threads.runs.retrieve(\n",
    "        thread_id=thread.id,\n",
    "        run_id=run.id\n",
    "    )\n",
    "    print(f\"Estado actual: {run.status}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3eabbdfe-dfae-411a-a1b2-0e74fad5fc2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# 1. ESPERAR A QUE TERMINE LA EJECUCIÓN\n",
    "while run.status in ['queued', 'in_progress', 'cancelling']:\n",
    "    time.sleep(1)  # Esperar 1 segundo\n",
    "    run = client.beta.threads.runs.retrieve(\n",
    "        thread_id=thread.id,\n",
    "        run_id=run.id\n",
    "    )\n",
    "    print(f\"Estado actual: {run.status}\")\n",
    "\n",
    "# 2. VERIFICAR SI COMPLETÓ EXITOSAMENTE\n",
    "if run.status == 'completed':\n",
    "    print(\"✅ Ejecución completada\")\n",
    "    \n",
    "    # 3. OBTENER TODOS LOS MENSAJES DEL THREAD\n",
    "    messages = client.beta.threads.messages.list(\n",
    "        thread_id=thread.id,\n",
    "        order=\"asc\"  # Del más antiguo al más reciente\n",
    "    )\n",
    "    \n",
    "    # 4. EXTRAER LA RESPUESTA DEL ASSISTANT\n",
    "    print(\"\\n\" + \"=\"*50)\n",
    "    print(\"RESPUESTA DEL ASSISTANT:\")\n",
    "    print(\"=\"*50)\n",
    "    \n",
    "    # El último mensaje será la respuesta del assistant\n",
    "    last_message = messages.data[-1]  # Último mensaje\n",
    "    \n",
    "    if last_message.role == 'assistant':\n",
    "        # Extraer el texto de la respuesta\n",
    "        respuesta_texto = last_message.content[0].text.value\n",
    "        print(respuesta_texto)\n",
    "        \n",
    "        # 5. OBTENER CITAS Y ANOTACIONES (si las hay)\n",
    "        annotations = last_message.content[0].text.annotations\n",
    "        \n",
    "        if annotations:\n",
    "            print(\"\\n\" + \"=\"*30)\n",
    "            print(\"CITAS Y REFERENCIAS:\")\n",
    "            print(\"=\"*30)\n",
    "            \n",
    "            for i, annotation in enumerate(annotations):\n",
    "                if hasattr(annotation, 'file_citation'):\n",
    "                    citation = annotation.file_citation\n",
    "                    print(f\"Cita {i+1}: {citation.quote}\")\n",
    "                    print(f\"Archivo ID: {citation.file_id}\")\n",
    "                elif hasattr(annotation, 'file_path'):\n",
    "                    file_path = annotation.file_path\n",
    "                    print(f\"Archivo referenciado {i+1}: {file_path.file_id}\")\n",
    "    \n",
    "elif run.status == 'failed':\n",
    "    print(f\"❌ Error en la ejecución: {run.last_error}\")\n",
    "    \n",
    "elif run.status == 'requires_action':\n",
    "    print(\"⚠️ Requiere acción adicional\")\n",
    "    print(f\"Acción requerida: {run.required_action}\")\n",
    "\n",
    "else:\n",
    "    print(f\"Estado inesperado: {run.status}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35486027-572f-465f-81d6-2736cf3645eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ahora borramos el fichero que hemos almacenado anteriormente, al fin de no incurrir en costos por el almacenamiento\n",
    "client.files.delete(file.id)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53f169b5-6b9d-41b6-aef3-ed055d1d475b",
   "metadata": {},
   "source": [
    "## Llamadas a funciones.\n",
    "\n",
    "Ya hemos visto antes que otra de las herramientas que se pueden utilizar con los assistents, son las llamadas a funciones, cuya documentación la podemos encontrar en el siguiente enlace:\n",
    "\n",
    "https://platform.openai.com/docs/guides/function-calling\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "122f3f18-f636-435b-8b78-58c836cb7662",
   "metadata": {},
   "outputs": [],
   "source": [
    "# cramos nestra primera función\n",
    "# NOMBRE-DESCRPCIÓN-PARÁMETROS\n",
    "sendEmail = {\n",
    "    \"name\" : \"sendEmail\",\n",
    "    \"description\":\"The function allows us to send email by specifying an email address and the title and descrption of the email.\",\n",
    "    \"parameters\":{\n",
    "        \"type\": \"object\",\n",
    "        \"properties\":{\n",
    "            \"email\": {\n",
    "                \"type\":\"string\",\n",
    "                \"description\":\"Email address of the receiver\"\n",
    "            },\n",
    "            \"subject\":{\n",
    "                \"type\":\"string\",\n",
    "                \"description\": \"Subject of the email\"\n",
    "            },\n",
    "            \"textBody\":{\n",
    "                \"type\":\"string\",\n",
    "                \"description\":\"Body of the email\"\n",
    "            }\n",
    "        },\n",
    "         \"required\":[\"email\",\"subject\",\"textBody\"]\n",
    "    }\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e000cdd-3f8c-4b50-b947-3573272d633b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# creamos la segunda función\n",
    "getCurrentWeather ={\n",
    "    \"name\":\"getCurrentWeather\",\n",
    "    \"description\": \"Get the weather in location\",\n",
    "    \"parameters\" :{\n",
    "        \"type\": \"object\",\n",
    "        \"properties\":{\n",
    "            \"location\": {\"type\":\"string\",\"description\":\"The city and e.g. San Francisco , CA\"},\n",
    "            \"unit\":{\"type\":\"string\",\"enum\":[\"c\",\"f\"]}\n",
    "        },\n",
    "        \"required\": [\"location\"]\n",
    "    }\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "045b0ff4-5eba-4321-9de6-d62e6b5e025d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# creamos el asistente\n",
    "assistant = client.beta.assistants.create(\n",
    "    name=\"Chatbot que responde a las preguntas del cliente\",\n",
    "    instructions=\"Eres un chatbot de la Peluqueria LLM Master  y te has especializado en enviar emails. Usa tu concimiento para responder a las preguntas del usuario\",\n",
    "    model = \"gpt-3.5-turbo-1106\",\n",
    "    tools = [\n",
    "        ({'type':'function','function': sendEmail}),\n",
    "        ({'type':'function','function': getCurrentWeather})\n",
    "    ]\n",
    ")\n",
    "\n",
    "print(assistant.id)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5228db95-575f-4d49-a89c-bc9bc7e85bd0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# creamos el hilo\n",
    "thread = client.beta.threads.create()\n",
    "print(thread.id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c235972a-62e3-4615-8411-edd7ad3985d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# creamos el primer mensaje del usuario y lo pasamos al hilo\n",
    "message = client.beta.threads.messages.create(\n",
    "    thread_id = thread.id,\n",
    "    role = \"user\",\n",
    "    content =\"que tiempo hace en Madrid?\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "429a2c93-127a-495a-aade-940c6f8cc560",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ejecutamos el asistente para obtener la respuesta\n",
    "run= client.beta.threads.runs.create(\n",
    "    thread_id = thread.id,\n",
    "        assistant_id = assistant.id\n",
    ")\n",
    "\n",
    "print(run.id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41e0a926-25c1-4d9d-bad9-361ce2e1d0f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Recupera el estado de ejecución (aqui vemos que require_action)\n",
    "# Esto le está diciendo al modelo, que necesita información de nuestra función\n",
    "# es decir, que necesitamos llamar a una función que permita enviar el email\n",
    "run = client.beta.threads.runs.retrieve(\n",
    "    thread_id = thread.id,\n",
    "    run_id=run.id\n",
    ")\n",
    "\n",
    "print(run.status)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e14c6e9-1693-44eb-8bc7-5ca6e8da4953",
   "metadata": {},
   "outputs": [],
   "source": [
    "# como obtenemos requires_action entonces tenemos que llamar a una función\n",
    "# ejemplo una llamada a una api para enciar un mail, titulo y descripción\n",
    "# Ejemplo: Una llamada a un api para obtener la temperatura de Madrid\n",
    "\n",
    "## tools_to_call= a qué función tenemos que llamar\n",
    "tools_to_call = run.required_action.submit_tool_outputs.tool_calls\n",
    "print(len(tools_to_call)) # cuantas llamadas tengoq ue hacer, imaginate que es más de una\n",
    "print(tools_to_call)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27817c67-0671-495d-aca4-1c32f7cef5ea",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "c11ede1e-aced-4a04-a82c-137a5214c75c",
   "metadata": {},
   "source": [
    "Lo dejo en el minuto 9:22. https://www.youtube.com/watch?v=-3-wq_kvc3A  NO LO TERMINO"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38a0c508-77ec-4ec9-82e2-606ed749f847",
   "metadata": {},
   "source": [
    "## Interprete de código.\n",
    "\n",
    "Recordar que esta es otra de las herramientas de los asistentes, según hemos visto al comienzo de este tema. Esta herramienta permite escribir y ejecutar código en python en un entorno seguro y aislado. Este entorno es capaz de procesar archivos de diversos formatos, generar resultados en tiempo real y crear imágenes de gráficos.\n",
    "Veamos a continuación cómo funciona este interprete de código."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b4bac4a-168b-4272-8646-fb4cbb220340",
   "metadata": {},
   "outputs": [],
   "source": [
    "# suponemos que ya hemos creado el cielte "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a0cade0-cac8-49cf-b39e-01c4e5fd53fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "assistant = client.beta.assistants.create(\n",
    "    name = \"profesor matemáticas\",\n",
    "    instructions = \"Eres un chatbot especializado en resolver problemas matemáticos. Escribe y ejecuta código para responder las preguntas de matemáticas\",\n",
    "    model = \"gpt-3.5-turbo-1106\",\n",
    "    tools =[({'type':'code_interpreter'})]\n",
    ")\n",
    "\n",
    "print(assistant.id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1d90230-0e63-4252-9e11-19947d6da586",
   "metadata": {},
   "outputs": [],
   "source": [
    "#creamos el hilo\n",
    "thread = client.beta.threads.create()\n",
    "print(thread.id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86ea7086-d2c4-4cae-afa7-a69ee50e4e23",
   "metadata": {},
   "outputs": [],
   "source": [
    "# creamos el primer mensaje del usuario y lo pasamos al hilo\n",
    "message = client.beta.threads.messages.create(\n",
    "    thread_id=thread.id,\n",
    "    role = \"user\",\n",
    "    content=\"tengo el siguiente problema matemático '5x+5=10'. ¿cual es el valor de x?\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd0809af-1aa8-4432-a29c-d4f7fe58da0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ejecutamos el asistente para obtener la respuesta\n",
    "run = client.beta.threads.runs.create(\n",
    "    thread_id=thread.id,\n",
    "    assistant_id=assistant.id\n",
    ")\n",
    "\n",
    "print(run.id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09b7af61-5277-457f-98d8-4ef38d5a5ff6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# leemos los mensaje\n",
    "messages = client.beta.threads.messages.list(\n",
    "    thread_id=thread.id\n",
    ")\n",
    "\n",
    "with open(\"messages.json\",\"w\") as f:\n",
    "    messages_json = messages.model_dump()\n",
    "    json.dump(messages_json,f,indent=4)\n",
    "\n",
    "print(\"Ver conversación:\")\n",
    "for message in messages.data:\n",
    "    pprint(message.role+\": \"+message.content[0].text.value)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75c0220b-7021-4903-8be7-1d349d736447",
   "metadata": {},
   "source": [
    "# Responses.\n",
    "\n",
    "Como ya se ha dicho anteriormente, la API de asistentes quedará obsoleta a partir del 26  de agosto de 2026, y será sustituida por la API de Responses. Para ayuda de la migración, OpenAi ha creado la página siguiente:\n",
    "\n",
    "https://platform.openai.com/docs/assistants/migration\n",
    "\n",
    "La equivalencia entre una API y la otra se puede ver en el siguiente cuadro:\n",
    "\n",
    "![](fig/cambios.PNG)\n",
    "\n",
    "En lo que sigue vamos a hacer una introducción a esta nueva API denominada **Responses**. Para seguir con este código, vamos a suponer que ya tenemos creado el objeto client como se ha hecho al comienzo de este documento. Comenzamos con un ejemplo bien sencillo para hacer una introducción a este elemento."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73b73ae6-c8e2-447b-ae25-cbadfc858c16",
   "metadata": {},
   "outputs": [],
   "source": [
    "from openai import OpenAI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1b84f03-d4a5-4fc0-935d-b90b526de444",
   "metadata": {},
   "outputs": [],
   "source": [
    "response = client.responses.create(\n",
    "    model =\"gpt-4o-mini\",\n",
    "    input=\"Dime algo sobre la historia de Valladolid\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b47de6ff-209c-4186-bd85-f5c3ce6bbe9a",
   "metadata": {},
   "source": [
    "Lo que se devuelve es un objeto de tipo JSON, cuyo contenido se ve a continuación."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b2ed55c-0a65-4448-a0d8-9986c9d10192",
   "metadata": {},
   "outputs": [],
   "source": [
    "pprint(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5473404-41f4-4720-bde9-1cce0f48eaac",
   "metadata": {},
   "source": [
    "Para obtener el contenido e la respuesta devuelta, lo podemos hace dela siguiente manera:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12b998fe-2ed6-440c-8395-0ec4c14e74c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "response.output[0].content[0].text"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e31640e-f735-40b6-9f07-220ea8284bc0",
   "metadata": {},
   "source": [
    "## Persistencia entre conversaciones.\n",
    "```{index} Responses, Conversation\n",
    "```\n",
    "La API de Responses de OpenAI puede utilizarse con objetos **conversation** para mantener el estado de conversación de manera persistente. Aquí te proporciono un ejemplo completo de cómo utilizarla:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f24e0b5c-257d-4716-9d0b-15914c22e9a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from openai import OpenAI\n",
    "\n",
    "# 1. Crear un objeto conversation\n",
    "conversation = client.conversations.create()\n",
    "\n",
    "# 2. Primera respuesta usando el conversation\n",
    "response = client.responses.create(\n",
    "    model=\"gpt-4o-mini\",\n",
    "    input=[{\"role\": \"user\", \"content\": \"¿Cuáles son los 5 Ds del dodgeball?\"}],\n",
    "    conversation=conversation.id\n",
    ")\n",
    "\n",
    "print(f\"Primera respuesta: {response.output_text}\")\n",
    "\n",
    "# 3. Segunda respuesta usando la misma conversation\n",
    "# El contexto se mantiene automáticamente\n",
    "second_response = client.responses.create(\n",
    "    model=\"gpt-4o-mini\",\n",
    "    input=[{\"role\": \"user\", \"content\": \"¿Puedes explicar el último D?\"}],\n",
    "    conversation=conversation.id\n",
    ")\n",
    "\n",
    "print(f\"Segunda respuesta: {second_response.output_text}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd0695b9-9aeb-4ab6-86dd-af72bf751a5f",
   "metadata": {},
   "source": [
    ":::{note}\n",
    "* La API de Responses la podemos encontrar en el siguiente enlace:\n",
    "\n",
    "https://platform.openai.com/docs/api-reference/responses/create\n",
    "\n",
    "* La API de conversation la podemos encontrar en la siguiente dirección:\n",
    "\n",
    "https://platform.openai.com/docs/api-reference/conversations/create\n",
    "\n",
    "* El mantenimiento del estado de una conversación lo podemos encontrar en el siguiente enlace\n",
    "\n",
    "https://platform.openai.com/docs/guides/conversation-state?api-mode=responses \n",
    "\n",
    ":::"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86165426-1c1b-4bf4-ba45-925730dfeb63",
   "metadata": {},
   "source": [
    "Un ejemplo similar, pero con gestión manual del historial es el siguiente:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e358d8da-9d96-4524-b5a6-d6aafa998c5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from openai import OpenAI\n",
    "\n",
    "#client = OpenAI()\n",
    "\n",
    "# Mantener historial manualmente\n",
    "history = [\n",
    "    {\"role\": \"user\", \"content\": \"cuéntame un chiste\"}\n",
    "]\n",
    "\n",
    "response = client.responses.create(\n",
    "    model=\"gpt-4o-mini\",\n",
    "    input=history,\n",
    "    store=False\n",
    ")\n",
    "\n",
    "print(f\"Chiste: {response.output_text}\")\n",
    "\n",
    "# Agregar la respuesta al historial\n",
    "history += [{\"role\": el.role, \"content\": el.content} for el in response.output]\n",
    "history.append({\"role\": \"user\", \"content\": \"explica por qué es gracioso\"})\n",
    "\n",
    "second_response = client.responses.create(\n",
    "    model=\"gpt-4o-mini\",\n",
    "    input=history,\n",
    "    store=False\n",
    ")\n",
    "\n",
    "print(f\"Explicación: {second_response.output_text}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ddf56f7-5d89-4a4b-af0a-2523426fdbfa",
   "metadata": {},
   "source": [
    "Se puede conseguir un efecto similar utilizando el parámetro *previous_response_id*. Veamos un ejemplo:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5dec9b8-18da-444a-acae-780dbecdf545",
   "metadata": {},
   "outputs": [],
   "source": [
    "from openai import OpenAI\n",
    "\n",
    "#client = OpenAI()\n",
    "\n",
    "# Primera respuesta\n",
    "response = client.responses.create(\n",
    "    model=\"gpt-4o-mini\",\n",
    "    input=\"cuéntame un chiste\",\n",
    "    store=True\n",
    ")\n",
    "\n",
    "print(f\"Chiste: {response.output_text}\")\n",
    "\n",
    "# Segunda respuesta enlazada\n",
    "second_response = client.responses.create(\n",
    "    model=\"gpt-4o-mini\",\n",
    "    previous_response_id=response.id,\n",
    "    input=[{\"role\": \"user\", \"content\": \"explica por qué es gracioso\"}],\n",
    "    store=True\n",
    ")\n",
    "\n",
    "print(f\"Explicación: {second_response.output_text}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5f835b1-8adf-431f-bede-2b6c61739f5a",
   "metadata": {},
   "source": [
    "**Ventajas del objeto conversation:**\n",
    "\n",
    "El objeto conversation ofrece varias ventajas :\n",
    "\n",
    "Persistencia: Mantiene el estado a través de sesiones, dispositivos o trabajos\n",
    "\n",
    "Identificador durable: Cada conversación tiene un ID único persistente\n",
    "\n",
    "Gestión automática: No necesitas manejar manualmente el historial de mensajes\n",
    "\n",
    "Sin límite de 30 días: A diferencia de las respuestas individuales, las conversaciones no tienen TTL\n",
    "\n",
    "**Consideraciones importantes**:\n",
    "\n",
    "Las respuestas individuales se guardan por 30 días por defecto\n",
    "\n",
    "Todos los tokens de entrada previos en la cadena se facturan como tokens de entrada\n",
    "\n",
    "Las conversaciones almacenan items que pueden ser mensajes, llamadas a herramientas y otros datos\n",
    "\n",
    "OpenAI no utiliza datos enviados vía API para entrenar modelos sin consentimiento explícito\n",
    "\n",
    "El **método más recomendado** es usar el objeto conversation ya que simplifica significativamente la gestión del estado y proporciona persistencia a largo plazo.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57864e73-de6b-4240-90c4-94a9c48a0929",
   "metadata": {},
   "source": [
    "## La API de Files.\n",
    "\n",
    "Existe también esta API para poder almacenar ficheros con los que se va a trabajar en OpenAi (tenr en cuenta que mantener estos ficheros tiene un coste). La documentación de esta API se puede encontrar en el siguiente enlace:\n",
    "\n",
    "https://platform.openai.com/docs/api-reference/files/create\n",
    "\n",
    "Existen diferentes posibilidades qur ofrece esta API, como subir ficheros, recuperarlos, borrarlos,etc. Vamos a generar ahora un ejemplo de como subir un fichero:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76cab540-6f38-421f-9e0c-0e0b67b4a4d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from openai import OpenAI\n",
    "client = OpenAI()\n",
    "\n",
    "file = client.files.create(\n",
    "  file=open(\"oidos.pdf\", \"rb\"),\n",
    "  purpose=\"assistants\",\n",
    "  expires_after={\n",
    "    \"anchor\": \"created_at\",\n",
    "    \"seconds\": 4000\n",
    "  }\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9c1d4df-3389-4c06-b3e3-1f8b3ee65b1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Veamos los ficheros existente:\n",
    "client.files.list()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "287b78a9-bd74-429d-a6bf-bd35e3c9fbf4",
   "metadata": {},
   "source": [
    "Como ya hemos indicado en un apartado anterior, podemos ver el fichero en el dashboard de OpenAi y alli borrarrlo, pero otra forma de hacer esto es la siguiente:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e1e4be0-a280-44f5-b293-c450de052c46",
   "metadata": {},
   "outputs": [],
   "source": [
    "client.files.delete(file.id)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1eebcf6f-d4b5-412d-9cf2-31a45dd0047a",
   "metadata": {},
   "source": [
    "## Vector store.\n",
    "\n",
    "Un “vector store” en OpenAI es un almacén de información diseñado para búsqueda semántica y recuperación aumentada por recuperación (RAG), donde los documentos se dividen en fragmentos, se convierten en incrustaciones vectoriales y se indexan para consultas por similitud, de modo que un asistente pueda responder con precisión citando y recuperando los trozos más relevantes de los archivos adjuntos.\n",
    "\n",
    "En la práctica, un objeto vector store actúa como contenedor de archivos procesados para búsqueda: al añadir un archivo, se parsea, se trocea en chunks, se generan embeddings y se guarda en un índice que soporta coincidencias por palabras clave y por semántica, lo que permite recuperar pasajes relevantes dado un prompt o pregunta.\n",
    "\n",
    "Estos almacenes pueden vincularse a asistentes o a conversaciones, habilitando la herramienta de búsqueda de archivos para que el modelo consulte directamente los propios datos; al adjuntarlo, las respuestas del agente pueden fundamentarse en los documentos, mejorando precisión y trazabilidad.\n",
    "\n",
    "Flujo típico:\n",
    "- Ingesta: crear el vector store y subir uno o varios archivos; la plataforma gestiona el análisis, chunking y embeddings de forma asíncrona, con estado visible en contadores de ingesta.\n",
    "- Consulta: la pregunta del usuario se convierte en vector y se ejecuta una búsqueda por similitud (p. ej., distancia coseno) sobre los chunks, devolviendo los más relevantes para que el modelo los use como contexto.\n",
    "- Gestión: se pueden añadir o eliminar archivos individualmente o por lotes (hasta 500 por batch), y hay límites como tamaño máximo por archivo (512 MB) y tope de tokens por archivo.\n",
    "\n",
    "Casos de uso comunes:\n",
    "- Búsqueda semántica y FAQ sobre documentación técnica o bases de conocimiento internas.\n",
    "- Asistentes de soporte que responden con fragmentos de manuales o políticas adjuntas al vector store.\n",
    "- Recomendación, clustering y clasificación basados en proximidad semántica de embeddings.\n",
    "\n",
    "Detalles operativos relevantes:\n",
    "- Adjuntar un vector store otorga capacidad de “file search” al asistente o al hilo, normalmente con un único vector store por asistente/hilo para control de ámbito.\n",
    "- La ingesta es asíncrona; los SDK proporcionan utilidades “create and poll” para esperar a que finalice antes de consultar.\n",
    "- Existen endpoints y objetos específicos para archivos y lotes de archivos dentro del vector store, con propiedades para auditar el estado y conteos de items procesados.\n",
    "\n",
    "En resumen, los objetos vector store son la pieza nativa de OpenAI para convertir archivos en contexto consultable mediante embeddings y similitud, permitiendo construir agentes y flujos RAG sin desplegar una base vectorial externa.\n",
    "\n",
    "El API de estos elementos se puede encontrar en este enlace:\n",
    "\n",
    "https://platform.openai.com/docs/api-reference/vector-stores/create\n",
    "\n",
    " continuación se muestran algunos ejemplos de cómo poder utilizar estos elementos:\n",
    "\n",
    " ### Creación de Vector Store\n",
    "\n",
    " ```python\n",
    "from openai import OpenAI\n",
    "\n",
    "client = OpenAI()\n",
    "\n",
    "# Crear un nuevo vector store\n",
    "vector_store = client.beta.vector_stores.create(\n",
    "    name=\"Mi Base de Conocimientos\",\n",
    "    expires_after={\n",
    "        \"anchor\": \"last_active_at\",\n",
    "        \"days\": 7\n",
    "    }\n",
    ")\n",
    "\n",
    "print(f\"Vector Store creado con ID: {vector_store.id}\")\n",
    "\n",
    "```\n",
    "\n",
    "### Subida de archivos.\n",
    "\n",
    "```python\n",
    "# Subir archivos uno por uno\n",
    "file = client.files.create(\n",
    "    file=open(\"documento.pdf\", \"rb\"),\n",
    "    purpose=\"assistants\"\n",
    ")\n",
    "\n",
    "# Añadir archivo al vector store\n",
    "vector_store_file = client.beta.vector_stores.files.create(\n",
    "    vector_store_id=vector_store.id,\n",
    "    file_id=file.id\n",
    ")\n",
    "\n",
    "# Subida masiva (más eficiente para múltiples archivos)\n",
    "file_paths = [\"doc1.pdf\", \"doc2.txt\", \"doc3.docx\"]\n",
    "file_streams = [open(path, \"rb\") for path in file_paths]\n",
    "\n",
    "file_batch = client.beta.vector_stores.file_batches.upload_and_poll(\n",
    "    vector_store_id=vector_store.id,\n",
    "    files=file_streams\n",
    ")\n",
    "\n",
    "print(f\"Batch procesado: {file_batch.status}\")\n",
    "\n",
    "```\n",
    "\n",
    "### Gestión de estados.\n",
    "```python\n",
    "# Monitorear el progreso de procesamiento\n",
    "import time\n",
    "\n",
    "def wait_for_processing(vector_store_id):\n",
    "    while True:\n",
    "        vs = client.beta.vector_stores.retrieve(vector_store_id)\n",
    "        print(f\"Estado: {vs.status}\")\n",
    "        print(f\"Archivos procesados: {vs.file_counts.completed}/{vs.file_counts.total}\")\n",
    "        \n",
    "        if vs.status == \"completed\":\n",
    "            break\n",
    "        elif vs.status == \"failed\":\n",
    "            raise Exception(\"Error en el procesamiento\")\n",
    "        \n",
    "        time.sleep(2)\n",
    "\n",
    "wait_for_processing(vector_store.id)\n",
    "\n",
    "```\n",
    "\n",
    "### Vinculación con asistentes\n",
    "\n",
    "```python\n",
    "# Crear asistente con vector store\n",
    "assistant = client.beta.assistants.create(\n",
    "    name=\"Asistente Documentos\",\n",
    "    instructions=\"Responde basándote en los documentos adjuntos\",\n",
    "    model=\"gpt-4-turbo\",\n",
    "    tools=[{\"type\": \"file_search\"}],\n",
    "    tool_resources={\n",
    "        \"file_search\": {\n",
    "            \"vector_store_ids\": [vector_store.id]\n",
    "        }\n",
    "    }\n",
    ")\n",
    "\n",
    "# O actualizar asistente existente\n",
    "client.beta.assistants.update(\n",
    "    assistant_id=assistant.id,\n",
    "    tool_resources={\n",
    "        \"file_search\": {\n",
    "            \"vector_store_ids\": [vector_store.id]\n",
    "        }\n",
    "    }\n",
    ")\n",
    "```\n",
    "\n",
    "### Consultas y búsquedas.\n",
    "\n",
    "```python\n",
    "# Crear hilo de conversación\n",
    "thread = client.beta.threads.create()\n",
    "\n",
    "# Hacer pregunta que active la búsqueda\n",
    "message = client.beta.threads.messages.create(\n",
    "    thread_id=thread.id,\n",
    "    role=\"user\",\n",
    "    content=\"¿Cuáles son las políticas de vacaciones según la documentación?\"\n",
    ")\n",
    "\n",
    "# Ejecutar asistente\n",
    "run = client.beta.threads.runs.create_and_poll(\n",
    "    thread_id=thread.id,\n",
    "    assistant_id=assistant.id\n",
    ")\n",
    "\n",
    "# Obtener respuesta\n",
    "messages = client.beta.threads.messages.list(thread_id=thread.id)\n",
    "print(messages.data[0].content[0].text.value)\n",
    "\n",
    "```\n",
    "### Administración de archivos.\n",
    "```python\n",
    "# Listar archivos en el vector store\n",
    "files = client.beta.vector_stores.files.list(\n",
    "    vector_store_id=vector_store.id\n",
    ")\n",
    "\n",
    "for file in files.data:\n",
    "    print(f\"Archivo: {file.id}, Estado: {file.status}\")\n",
    "\n",
    "# Eliminar archivo específico\n",
    "client.beta.vector_stores.files.delete(\n",
    "    vector_store_id=vector_store.id,\n",
    "    file_id=file.id\n",
    ")\n",
    "\n",
    "# Obtener información detallada\n",
    "file_info = client.beta.vector_stores.files.retrieve(\n",
    "    vector_store_id=vector_store.id,\n",
    "    file_id=file.id\n",
    ")\n",
    "\n",
    "```\n",
    "\n",
    "### Gestión completa del ciclo de vida.\n",
    "\n",
    "```python\n",
    "class VectorStoreManager:\n",
    "    def __init__(self, client):\n",
    "        self.client = client\n",
    "    \n",
    "    def create_knowledge_base(self, name, documents):\n",
    "        # Crear vector store\n",
    "        vs = self.client.beta.vector_stores.create(name=name)\n",
    "        \n",
    "        # Subir documentos en lote\n",
    "        file_streams = [open(doc, \"rb\") for doc in documents]\n",
    "        batch = self.client.beta.vector_stores.file_batches.upload_and_poll(\n",
    "            vector_store_id=vs.id,\n",
    "            files=file_streams\n",
    "        )\n",
    "        \n",
    "        # Cerrar streams\n",
    "        for stream in file_streams:\n",
    "            stream.close()\n",
    "        \n",
    "        return vs.id if batch.status == \"completed\" else None\n",
    "    \n",
    "    def cleanup_expired(self):\n",
    "        # Listar todos los vector stores\n",
    "        stores = self.client.beta.vector_stores.list()\n",
    "        \n",
    "        for store in stores.data:\n",
    "            if store.status == \"expired\":\n",
    "                self.client.beta.vector_stores.delete(store.id)\n",
    "                print(f\"Eliminado vector store expirado: {store.id}\")\n",
    "\n",
    "# Uso\n",
    "manager = VectorStoreManager(client)\n",
    "vs_id = manager.create_knowledge_base(\n",
    "    \"Políticas Empresa\", \n",
    "    [\"politicas.pdf\", \"manual.docx\", \"procedimientos.txt\"]\n",
    ")\n",
    "\n",
    "```\n",
    "\n",
    "### Manejo de errores\n",
    "\n",
    "```python\n",
    "try:\n",
    "    # Operación con vector store\n",
    "    result = client.beta.vector_stores.files.create(\n",
    "        vector_store_id=\"vs_invalid\",\n",
    "        file_id=\"file_invalid\"\n",
    "    )\n",
    "except Exception as e:\n",
    "    if \"not found\" in str(e).lower():\n",
    "        print(\"Vector store o archivo no encontrado\")\n",
    "    elif \"limit\" in str(e).lower():\n",
    "        print(\"Límite de archivos alcanzado\")\n",
    "    else:\n",
    "        print(f\"Error inesperado: {e}\")\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "965388a9-e240-419e-95b5-7c0b1038efaf",
   "metadata": {},
   "source": [
    "## Caso de uso con Responses.\n",
    "\n",
    "En este apartado vamos a replicar el apartado anterior de leer un fichero y sacar información del mismo pero en este caso utilizando la API de Responses en lugar de assistant."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53cc8c41-a836-4e7b-8c11-9c7b12df65ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "file = client.files.create(\n",
    "    file = open(\"oidos.pdf\",\"rb\"),\n",
    "    purpose = 'assistants'\n",
    ")\n",
    "\n",
    "# imprimimos el identificador del fichero\n",
    "print(f\"El identificador del fichero es:{file.id}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "001d3908-b7d2-4a75-9b5b-b9b3803ac940",
   "metadata": {},
   "outputs": [],
   "source": [
    "response = client.responses.create(\n",
    "    model=\"gpt-4o-mini\",\n",
    "    input=[\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": [\n",
    "                {\n",
    "                    \"type\": \"input_text\",\n",
    "                    \"text\": \"¿Cuáles son los puntos principales de este documento?\"\n",
    "                },\n",
    "                {\n",
    "                    \"type\": \"input_file\",\n",
    "                    \"file_id\": file.id  # Aquí usas el file.id directamente\n",
    "                }\n",
    "            ]\n",
    "        }\n",
    "    ]\n",
    ")\n",
    "\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc80c401-432c-4e99-865d-db207a57a83f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Borramos el objero response\n",
    "response2 = client.responses.delete(response.id)\n",
    "print(response2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "691c3e40-1855-4585-a45c-9cf8b1c3b79c",
   "metadata": {},
   "source": [
    "Ahora realizamos el mismo procedimiento pero utilizando un vector store"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f911038-3e03-4b25-8e8c-e202e509317c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2. Crear vector store\n",
    "vector_store = client.vector_stores.create(\n",
    "    name=\"Documentos para análisis\"\n",
    ")\n",
    "\n",
    "# 3. Agregar archivo al vector store\n",
    "client.vector_stores.files.create(\n",
    "    vector_store_id=vector_store.id,\n",
    "    file_id=file.id\n",
    ")\n",
    "\n",
    "# 4. Usar el Responses API con el vector store\n",
    "response = client.responses.create(\n",
    "    model=\"gpt-4o-mini\",\n",
    "    tools=[{\n",
    "        \"type\": \"file_search\",\n",
    "        \"vector_store_ids\": [vector_store.id],\n",
    "        \"max_num_results\": 20\n",
    "    }],\n",
    "    input=\"¿Cuáles son los puntos principales de este documento?\",\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21d4f81c-b2c2-43a3-92d2-a20707de01cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c5814df-f7ad-4d1c-9bd4-417855d97094",
   "metadata": {},
   "outputs": [],
   "source": [
    "response.output[1].content[0].text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ffdffb8-17b1-4cc8-8201-220e40e0bf1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# borramos elobjeto response\n",
    "# Borramos el objero response\n",
    "response = client.responses.delete(response.id)\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e030f55-afea-4432-a287-f80ddd40e237",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Veamos cuantos vector Store tenemos\n",
    "vector_stores = client.vector_stores.list()\n",
    "print(vector_stores)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3c48c0a-4c1f-4960-8937-713581a0d008",
   "metadata": {},
   "source": [
    "Un codigo mas completo seria el siguiente:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d39e0dee-ab0f-49b4-8aae-87d7f7f86bae",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "\n",
    "def listar_vector_stores():\n",
    "    try:\n",
    "        # Obtener la lista de vector stores\n",
    "        vector_stores = client.vector_stores.list()\n",
    "        \n",
    "        if not vector_stores.data:\n",
    "            print(\"No tienes vector stores creados.\")\n",
    "            return\n",
    "        \n",
    "        print(f\"Encontrados {len(vector_stores.data)} vector stores:\")\n",
    "        print(\"=\" * 80)\n",
    "        \n",
    "        for i, vs in enumerate(vector_stores.data, 1):\n",
    "            # Convertir timestamp a fecha legible\n",
    "            created_date = datetime.fromtimestamp(vs.created_at).strftime(\"%Y-%m-%d %H:%M:%S\")\n",
    "            \n",
    "            print(f\"{i}. Vector Store:\")\n",
    "            print(f\"   ID: {vs.id}\")\n",
    "            print(f\"   Nombre: {vs.name or 'Sin nombre'}\")\n",
    "            print(f\"   Creado: {created_date}\")\n",
    "            print(f\"   Estado: {vs.status}\")\n",
    "            print(f\"   Archivos:\")\n",
    "            print(f\"     - Total: {vs.file_counts.total}\")\n",
    "            print(f\"     - En progreso: {vs.file_counts.in_progress}\")\n",
    "            print(f\"     - Completados: {vs.file_counts.completed}\")\n",
    "            print(f\"     - Fallidos: {vs.file_counts.failed}\")\n",
    "            print(f\"     - Cancelados: {vs.file_counts.cancelled}\")\n",
    "            print(f\"   Tamaño: {vs.usage_bytes} bytes ({vs.usage_bytes / (1024*1024):.2f} MB)\")\n",
    "            \n",
    "            # Si hay metadatos, mostrarlos\n",
    "            if hasattr(vs, 'metadata') and vs.metadata:\n",
    "                print(f\"   Metadatos: {vs.metadata}\")\n",
    "            \n",
    "            print(\"-\" * 80)\n",
    "            \n",
    "    except Exception as e:\n",
    "        print(f\"Error al obtener vector stores: {e}\")\n",
    "\n",
    "# Ejecutar la función\n",
    "listar_vector_stores()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31937c9b-cfb8-4aea-af47-485c1fd249da",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Borramos los cuatro vector store\n",
    "from datetime import datetime, timedelta\n",
    "import time\n",
    "\n",
    "def eliminar_todos_los_vector_stores():\n",
    "    \"\"\"\n",
    "    Elimina todos los vector stores de tu cuenta OpenAI\n",
    "    PRECAUCIÓN: Esta operación es irreversible\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Obtener lista de todos los vector stores\n",
    "        print(\"Obteniendo lista de vector stores...\")\n",
    "        vector_stores = client.vector_stores.list()\n",
    "        \n",
    "        if not vector_stores.data:\n",
    "            print(\"No se encontraron vector stores para eliminar.\")\n",
    "            return\n",
    "        \n",
    "        total = len(vector_stores.data)\n",
    "        print(f\"Se encontraron {total} vector stores.\")\n",
    "        \n",
    "        # Confirmación de seguridad\n",
    "        confirmacion = input(f\"\\n⚠️  ADVERTENCIA: Esto eliminará TODOS los {total} vector stores de tu cuenta.\\n\"\n",
    "                           \"Esta operación es IRREVERSIBLE.\\n\"\n",
    "                           \"Escribe 'ELIMINAR TODO' para confirmar: \")\n",
    "        \n",
    "        if confirmacion != \"ELIMINAR TODO\":\n",
    "            print(\"Operación cancelada.\")\n",
    "            return\n",
    "        \n",
    "        print(f\"\\nEliminando {total} vector stores...\")\n",
    "        eliminados = 0\n",
    "        errores = 0\n",
    "        \n",
    "        for i, vs in enumerate(vector_stores.data, 1):\n",
    "            try:\n",
    "                print(f\"[{i}/{total}] Eliminando vector store: {vs.id}\")\n",
    "                if vs.name:\n",
    "                    print(f\"    Nombre: {vs.name}\")\n",
    "                \n",
    "                # Eliminar el vector store\n",
    "                response = client.vector_stores.delete(vector_store_id=vs.id)\n",
    "                \n",
    "                if hasattr(response, 'deleted') and response.deleted:\n",
    "                    eliminados += 1\n",
    "                    print(f\"    ✅ Eliminado exitosamente\")\n",
    "                else:\n",
    "                    print(f\"    ❌ No se pudo confirmar la eliminación\")\n",
    "                    errores += 1\n",
    "                \n",
    "                # Pequeña pausa para evitar sobrecarga de la API\n",
    "                time.sleep(0.5)\n",
    "                \n",
    "            except Exception as e:\n",
    "                print(f\"    ❌ Error al eliminar {vs.id}: {e}\")\n",
    "                errores += 1\n",
    "        \n",
    "        print(f\"\\n📊 Resumen:\")\n",
    "        print(f\"   Vector stores eliminados: {eliminados}\")\n",
    "        print(f\"   Errores: {errores}\")\n",
    "        print(f\"   Total procesados: {eliminados + errores}\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Error general: {e}\")\n",
    "\n",
    "# Ejecutar la función\n",
    "eliminar_todos_los_vector_stores()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2664d85a-acb9-45b2-b71b-05cebf5ed9da",
   "metadata": {},
   "outputs": [],
   "source": [
    "# LISTAMOS AHORA TODOS LOS FILES\n",
    "\n",
    "from datetime import datetime\n",
    "\n",
    "\n",
    "def listar_archivos():\n",
    "    \"\"\"\n",
    "    Lista todos los archivos almacenados en tu cuenta OpenAI\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Obtener lista de todos los archivos\n",
    "        print(\"Obteniendo lista de archivos...\")\n",
    "        files = client.files.list()\n",
    "        \n",
    "        if not files.data:\n",
    "            print(\"No se encontraron archivos en tu cuenta.\")\n",
    "            return\n",
    "        \n",
    "        total = len(files.data)\n",
    "        print(f\"Se encontraron {total} archivos:\")\n",
    "        print(\"=\" * 80)\n",
    "        \n",
    "        for i, file in enumerate(files.data, 1):\n",
    "            # Convertir timestamp a fecha legible\n",
    "            created_date = datetime.fromtimestamp(file.created_at).strftime(\"%Y-%m-%d %H:%M:%S\")\n",
    "            size_mb = file.bytes / (1024 * 1024) if file.bytes > 0 else 0\n",
    "            \n",
    "            print(f\"{i}. Archivo:\")\n",
    "            print(f\"   ID: {file.id}\")\n",
    "            print(f\"   Nombre: {file.filename}\")\n",
    "            print(f\"   Propósito: {file.purpose}\")\n",
    "            print(f\"   Creado: {created_date}\")\n",
    "            print(f\"   Tamaño: {file.bytes} bytes ({size_mb:.2f} MB)\")\n",
    "            print(f\"   Estado: {getattr(file, 'status', 'N/A')}\")\n",
    "            print(\"-\" * 80)\n",
    "            \n",
    "    except Exception as e:\n",
    "        print(f\"Error al obtener archivos: {e}\")\n",
    "\n",
    "# Ejecutar la función\n",
    "listar_archivos()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8f8364c-e520-49f2-9abc-5958c9937f95",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Ahora un código para borrar los ficheros\n",
    "from openai import OpenAI\n",
    "from datetime import datetime, timedelta\n",
    "import time\n",
    "\n",
    "def eliminar_archivos_avanzado():\n",
    "    \"\"\"\n",
    "    Versión avanzada que permite filtrar por propósito, fecha, etc.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Obtener todos los archivos\n",
    "        files = client.files.list()\n",
    "        \n",
    "        if not files.data:\n",
    "            print(\"No se encontraron archivos.\")\n",
    "            return\n",
    "        \n",
    "        print(f\"Se encontraron {len(files.data)} archivos:\")\n",
    "        print(\"=\" * 80)\n",
    "        \n",
    "        # Mostrar información detallada agrupada por propósito\n",
    "        archivos_por_proposito = {}\n",
    "        for file in files.data:\n",
    "            purpose = file.purpose\n",
    "            if purpose not in archivos_por_proposito:\n",
    "                archivos_por_proposito[purpose] = []\n",
    "            archivos_por_proposito[purpose].append(file)\n",
    "        \n",
    "        for purpose, file_list in archivos_por_proposito.items():\n",
    "            total_size = sum(f.bytes for f in file_list)\n",
    "            size_mb = total_size / (1024 * 1024)\n",
    "            print(f\"📁 {purpose.upper()}: {len(file_list)} archivos ({size_mb:.2f} MB)\")\n",
    "            \n",
    "            for file in file_list[:3]:  # Mostrar solo los primeros 3\n",
    "                created_date = datetime.fromtimestamp(file.created_at)\n",
    "                print(f\"   • {file.filename} - {created_date.strftime('%Y-%m-%d')}\")\n",
    "            \n",
    "            if len(file_list) > 3:\n",
    "                print(f\"   ... y {len(file_list) - 3} archivos más\")\n",
    "            print()\n",
    "        \n",
    "        print(\"\\nOpciones de eliminación:\")\n",
    "        print(\"1. Eliminar TODOS los archivos\")\n",
    "        print(\"2. Eliminar archivos por propósito específico\")\n",
    "        print(\"3. Eliminar archivos más antiguos que X días\")\n",
    "        print(\"4. Eliminar archivos de un tamaño específico\")\n",
    "        print(\"5. Cancelar\")\n",
    "        \n",
    "        opcion = input(\"\\nSelecciona una opción (1-5): \")\n",
    "        \n",
    "        if opcion == \"1\":\n",
    "            eliminar_todos(files.data)\n",
    "        elif opcion == \"2\":\n",
    "            eliminar_por_proposito(archivos_por_proposito)\n",
    "        elif opcion == \"3\":\n",
    "            eliminar_por_fecha(files.data)\n",
    "        elif opcion == \"4\":\n",
    "            eliminar_por_tamaño(files.data)\n",
    "        else:\n",
    "            print(\"Operación cancelada.\")\n",
    "            \n",
    "    except Exception as e:\n",
    "        print(f\"Error: {e}\")\n",
    "\n",
    "def eliminar_todos(files):\n",
    "    \"\"\"Elimina todos los archivos\"\"\"\n",
    "    confirmacion = input(f\"\\n⚠️  Confirma que quieres eliminar TODOS los {len(files)} archivos.\\n\"\n",
    "                        \"Escribe 'SI ELIMINAR TODO': \")\n",
    "    \n",
    "    if confirmacion == \"SI ELIMINAR TODO\":\n",
    "        procesar_eliminacion(files)\n",
    "    else:\n",
    "        print(\"Cancelado.\")\n",
    "\n",
    "def eliminar_por_proposito(archivos_por_proposito):\n",
    "    \"\"\"Elimina archivos de un propósito específico\"\"\"\n",
    "    print(\"\\nPropósitos disponibles:\")\n",
    "    propositos = list(archivos_por_proposito.keys())\n",
    "    for i, purpose in enumerate(propositos, 1):\n",
    "        count = len(archivos_por_proposito[purpose])\n",
    "        print(f\"{i}. {purpose} ({count} archivos)\")\n",
    "    \n",
    "    try:\n",
    "        seleccion = int(input(\"\\nSelecciona el número del propósito: \")) - 1\n",
    "        if 0 <= seleccion < len(propositos):\n",
    "            purpose_elegido = propositos[seleccion]\n",
    "            archivos_elegidos = archivos_por_proposito[purpose_elegido]\n",
    "            \n",
    "            print(f\"\\nSe eliminarán {len(archivos_elegidos)} archivos con propósito '{purpose_elegido}':\")\n",
    "            for file in archivos_elegidos:\n",
    "                print(f\"  - {file.filename} ({file.id})\")\n",
    "            \n",
    "            confirmacion = input(f\"\\nConfirmar eliminación? (si/no): \")\n",
    "            if confirmacion.lower() == 'si':\n",
    "                procesar_eliminacion(archivos_elegidos)\n",
    "        else:\n",
    "            print(\"Selección inválida.\")\n",
    "    except ValueError:\n",
    "        print(\"Por favor, ingresa un número válido.\")\n",
    "\n",
    "def eliminar_por_fecha(files):\n",
    "    \"\"\"Elimina archivos más antiguos que X días\"\"\"\n",
    "    try:\n",
    "        dias = int(input(\"Eliminar archivos más antiguos que cuántos días? \"))\n",
    "        fecha_limite = datetime.now() - timedelta(days=dias)\n",
    "        \n",
    "        antiguos = [f for f in files \n",
    "                   if datetime.fromtimestamp(f.created_at) < fecha_limite]\n",
    "        \n",
    "        if not antiguos:\n",
    "            print(f\"No hay archivos más antiguos que {dias} días.\")\n",
    "            return\n",
    "        \n",
    "        print(f\"Se eliminarán {len(antiguos)} archivos más antiguos que {dias} días:\")\n",
    "        for file in antiguos:\n",
    "            fecha = datetime.fromtimestamp(file.created_at).strftime('%Y-%m-%d')\n",
    "            print(f\"  - {file.filename} - {fecha}\")\n",
    "        \n",
    "        confirmacion = input(f\"\\nConfirmar eliminación? (si/no): \")\n",
    "        if confirmacion.lower() == 'si':\n",
    "            procesar_eliminacion(antiguos)\n",
    "            \n",
    "    except ValueError:\n",
    "        print(\"Por favor, ingresa un número válido de días.\")\n",
    "\n",
    "def eliminar_por_tamaño(files):\n",
    "    \"\"\"Elimina archivos basado en criterios de tamaño\"\"\"\n",
    "    print(\"\\nOpciones de tamaño:\")\n",
    "    print(\"1. Archivos mayores a X MB\")\n",
    "    print(\"2. Archivos menores a X MB\")\n",
    "    print(\"3. Archivos de 0 bytes (vacíos)\")\n",
    "    \n",
    "    opcion = input(\"Selecciona una opción (1-3): \")\n",
    "    \n",
    "    if opcion == \"1\":\n",
    "        try:\n",
    "            mb_limite = float(input(\"Eliminar archivos mayores a cuántos MB? \"))\n",
    "            bytes_limite = mb_limite * 1024 * 1024\n",
    "            grandes = [f for f in files if f.bytes > bytes_limite]\n",
    "            \n",
    "            if grandes:\n",
    "                print(f\"Se eliminarán {len(grandes)} archivos mayores a {mb_limite} MB:\")\n",
    "                for file in grandes:\n",
    "                    size_mb = file.bytes / (1024 * 1024)\n",
    "                    print(f\"  - {file.filename} ({size_mb:.2f} MB)\")\n",
    "                \n",
    "                confirmacion = input(\"\\nConfirmar eliminación? (si/no): \")\n",
    "                if confirmacion.lower() == 'si':\n",
    "                    procesar_eliminacion(grandes)\n",
    "            else:\n",
    "                print(f\"No hay archivos mayores a {mb_limite} MB.\")\n",
    "        except ValueError:\n",
    "            print(\"Por favor, ingresa un número válido.\")\n",
    "            \n",
    "    elif opcion == \"3\":\n",
    "        vacios = [f for f in files if f.bytes == 0]\n",
    "        if vacios:\n",
    "            print(f\"Se eliminarán {len(vacios)} archivos vacíos:\")\n",
    "            for file in vacios:\n",
    "                print(f\"  - {file.filename}\")\n",
    "            \n",
    "            confirmacion = input(\"\\nConfirmar eliminación? (si/no): \")\n",
    "            if confirmacion.lower() == 'si':\n",
    "                procesar_eliminacion(vacios)\n",
    "        else:\n",
    "            print(\"No hay archivos vacíos.\")\n",
    "\n",
    "def procesar_eliminacion(archivos_a_eliminar):\n",
    "    \"\"\"Procesa la eliminación de los archivos seleccionados\"\"\"\n",
    "    total = len(archivos_a_eliminar)\n",
    "    eliminados = 0\n",
    "    errores = 0\n",
    "    \n",
    "    print(f\"\\nEliminando {total} archivos...\")\n",
    "    \n",
    "    for i, file in enumerate(archivos_a_eliminar, 1):\n",
    "        try:\n",
    "            print(f\"[{i}/{total}] Eliminando: {file.filename}\")\n",
    "            \n",
    "            response = client.files.delete(file.id)\n",
    "            \n",
    "            if hasattr(response, 'deleted') and response.deleted:\n",
    "                eliminados += 1\n",
    "                print(f\"    ✅ Eliminado\")\n",
    "            else:\n",
    "                errores += 1\n",
    "                print(f\"    ❌ Error en la eliminación\")\n",
    "            \n",
    "            # Pausa para no sobrecargar la API\n",
    "            time.sleep(0.2)\n",
    "            \n",
    "        except Exception as e:\n",
    "            errores += 1\n",
    "            print(f\"    ❌ Error: {e}\")\n",
    "    \n",
    "    print(f\"\\n📊 Resultado final:\")\n",
    "    print(f\"   Eliminados exitosamente: {eliminados}\")\n",
    "    print(f\"   Errores: {errores}\")\n",
    "\n",
    "# Ejecutar la versión avanzada\n",
    "eliminar_archivos_avanzado()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16f8a37b-2ac9-4c51-950c-a65625a6d49f",
   "metadata": {},
   "source": [
    "## Apéndice:\n",
    "\n",
    "* <a href=\"https://www.datacamp.com/tutorial/open-ai-assistants-api-tutorial\" target=\"_blank\"> Tutorial </a>\n",
    "\n",
    "* <a href=\"https://www.youtube.com/watch?v=wIQePhB5KZI\" target=\"_blank\"> Tutorial en inglés</a>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
