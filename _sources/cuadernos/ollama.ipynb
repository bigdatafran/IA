{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7ea6939c-bd31-45d0-ac19-78c5f42ae25c",
   "metadata": {},
   "source": [
    "(ollama1)=\n",
    "# Introducción a ollama\n",
    "```{index} ollama\n",
    "```\n",
    "\n",
    "Ollama es una herramienta diseñada para facilitar la ejecución de modelos de inteligencia artificial en dispositivos locales sin depender de servidores en la nube. Se enfoca principalmente en modelos de lenguaje grandes (LLMs), permitiendo a los usuarios ejecutar, personalizar y compartir estos modelos con facilidad.  \n",
    "\n",
    "## **Principales características de Ollama**  \n",
    "\n",
    "1. **Ejecución local de modelos**  \n",
    "   Ollama permite a los usuarios descargar y ejecutar modelos de lenguaje en su propia computadora, reduciendo la latencia y evitando la necesidad de enviar datos a servidores externos.  \n",
    "\n",
    "2. **Simplicidad en el uso**  \n",
    "   Con una interfaz sencilla y comandos directos, es fácil interactuar con modelos sin necesidad de configuraciones complicadas.  \n",
    "\n",
    "3. **Compatibilidad con múltiples modelos**  \n",
    "   Ollama soporta varios modelos de lenguaje, incluyendo versiones optimizadas para diferentes necesidades, como generación de texto, asistencia en programación y más.  \n",
    "\n",
    "4. **Privacidad y seguridad**  \n",
    "   Al ejecutarse localmente, Ollama evita el envío de datos sensibles a terceros, ofreciendo mayor privacidad y control sobre la información.  \n",
    "\n",
    "5. **Personalización y ajuste fino**  \n",
    "   Permite modificar modelos existentes y adaptarlos a tareas específicas mediante técnicas de ajuste fino.  \n",
    "\n",
    "6. **Código abierto y comunidad activa**  \n",
    "   Ollama es un proyecto de código abierto con una comunidad activa que contribuye a su mejora continua.  \n",
    "\n",
    "En resumen, Ollama es una solución potente para aquellos que buscan ejecutar modelos de inteligencia artificial de forma eficiente, segura y personalizable en sus propios dispositivos.\n",
    "\n",
    "Para poder utilizar ollma, lo primero que tenemos que hacer es <a href=\"https://ollama.com/download\" target=\"_blank\"> descargar la aplicación desde su página web</a> (elegir el tipo de sistema operativo para el que se descarga y posteriormente ejecutar el fichero que se ha descargado desde la red.\n",
    "\n",
    "Una vez que el servidor de ollama se encuentre en ejecución, debemos cargar el modelo que queramos La lista de modelos disponibles <a href=\"https://ollama.com/search\" target=\"_blank\"> se pueden encontrar en este enlace </a>.\n",
    "\n",
    "Existen en internet muchos vídeos sobre esta herramienta, uno de ellos lo puedes encontrar <a href=\"https://www.youtube.com/watch?v=WkouIQBB1GI\" target=\"_blank\"> en este enlace </a>. Se debe tener en cuenta que para todas las exposiciones que se hacen en este apartado el modelo utilizado ha sido el denominado *deepseek-r1* que es el último que ha sacado en estos momentos deepseek.\n",
    "\n",
    "Para ejecutar todas las líneas de código que se muestran a continuación, ncesitamos que el servidor de ollama esté activado, y para ello debemos ejecutar los siguientes comandos:\n",
    "\n",
    "```\n",
    "ollama  serve\n",
    "ollama run deepseek-r1:latest\n",
    "```\n",
    "\n",
    "Como es de esperar estos modelos requieren mucho espacio de almacenamiento, por que lo más seguro es que el usuario quiera cambiar la carpeta donde se almacenen estos ficheros tan pesados. Para conseguir esto, se debe crear una variable de entorno denominada 'OLLAMA_MODELS'  e indicar la ruta donde se desee sean almacenados los ficheros. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82911801-7c4d-496b-96fa-905c8c3f2fcf",
   "metadata": {},
   "source": [
    "## Ejecutar ollama con request\n",
    "```{index} request\n",
    "```\n",
    "\n",
    "La forma originaria de utilizar ollama es mediante llamadas al servido mediante una instrucción 'cur'. Un ejemplo de este tipo de instrucciones puede ser el siguiente:\n",
    "\n",
    "```\n",
    "curl http://localhost:11434/api/chat -d '{\n",
    "  \"model\": \"llama3.2\",\n",
    "  \"messages\": [\n",
    "    {\n",
    "      \"role\": \"user\",\n",
    "      \"content\": \"why is the sky blue?\"\n",
    "    }\n",
    "  ]\n",
    "}'\n",
    "\n",
    "```\n",
    " Para ejecutar este tipo de código, existe en python la librería denomianda *requests* que será la que utilicemos para este tipo de instrucciones. \n",
    "\n",
    " Lo primero que debemos hacer es instalarla en nuestro equipo en el supuesto de que no la tengamos instalada ya."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "4a50bc79-4312-4ae8-92e4-7023965dea46",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install requests"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db08752d-9c5d-43c7-b34c-856db99a7d52",
   "metadata": {},
   "source": [
    "Después la traducción de la petición anterior se trasladaría a python de la siguiente manera"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "787ea5d8-df0b-4e5e-81ff-305865d463f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "cd748549-e4d6-4ca9-b948-3eefd7d13c1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "url= \"http://localhost:11434/api/generate\"\n",
    "\n",
    "headers = {\n",
    "    \"Content-Type\": \"application/json\"\n",
    "}\n",
    "\n",
    "data ={\n",
    "    \"model\" : \"deepseek-r1:latest\",\n",
    "    \"prompt\": \"Hablame de Real Madrid\",\n",
    "    \"stream\": False\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "657f9edd-059c-4ad7-b1d0-cadad66d8df5",
   "metadata": {},
   "outputs": [],
   "source": [
    "response = requests.post(url,headers=headers, data=json.dumps(data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "184fb1cd-a4bf-46e5-98d2-a4444966d40b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "*******\n",
      "<think>\n",
      "Okay, the user wants me to talk about Real Madrid. I should start by mentioning how they are one of the most successful teams in Spanish football history.\n",
      "\n",
      "I'll note their rich history dating back to 1899 and how they've won multiple titles over the years.\n",
      "\n",
      "It's important to highlight their current status as a top-tier club with a strong fan base both in Spain and internationally.\n",
      "\n",
      "I should mention some of their famous players like Zidane, Ramos, Benzoni, and Guardiola to give a sense of their legendary teams.\n",
      "\n",
      "Also, emphasizing their commitment to innovation on the field and their global influence would add depth to the response.\n",
      "</think>\n",
      "\n",
      "El Real Madrid CF, fundado el 13 de noviembre de 1899 por Ramón Treserras y Emilio Ferrán, es una equipa argentina que actualmente gira en la Llana (Primera División) de la Liga Spanish. Es uno de los clubes más relevantes del fútbol, con una historia repleta de éxito y celebridad.\n",
      "\n",
      "Entre sus muchos logros se encuentran winos como el Campeonato de España Cup 1930, la Copa del Mundo 1982 (con Zidane) y multos campeones de la Spanish League. Actualmente, Real Madrid está una de las equipas más potentes del mundo, con un fanado worldwide y una ergonomía deportiva que combinaquality y innovación.\n",
      "\n",
      "Además, el club cuenta con un notable pasión de fútbol, lo que lo hace uno de los clubes más celebrados en todo el mundo. Actualmente, Real Madrid está actualmente en la 2ª posición de la Llana de la Liga, con una Campaigna goals de 8-0.\n"
     ]
    }
   ],
   "source": [
    "if response.status_code == 200:\n",
    "    response_text = response.text\n",
    "    data = json.loads(response_text)\n",
    "    actual_response = data[\"response\"]\n",
    "    print(\"*******\")\n",
    "    print(actual_response)\n",
    "else:\n",
    "    print(\"Error: \", response.status_code, response.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9a829c0-ab61-48df-9c24-1531f298bd78",
   "metadata": {},
   "source": [
    "## Ollama desde Python\n",
    "\n",
    "Existe una librería creada para poder utilizar ollama desde python. Esta librería se denomona *ollama* y su api se puede encontrar en este enlace: \n",
    "\n",
    "<a href=\"https://github.com/ollama/ollama-python\" target=\"_blank\">librería python para ollama</a>\n",
    "\n",
    "Para su instalación se debe utilizar el siguiente comando"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d28e7de-41bc-4262-b408-a3d1de850668",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install ollama"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be9e6bb0-0fcb-4306-9664-d29cf838e61e",
   "metadata": {},
   "source": [
    "A continuación se muestra un ejemplo, para su tilización. Como ya se ha dicho antes debemos tener activado en local el servidor de ollama, con el modelo que se indica en el ejemplo:\n",
    "\n",
    "**NOTA:** El campo 'role' puede tener los mismos valores que en la API de OpenAI: \"system\", \"user\",assistant\". Para una ampliación de esto ver el siguiente enlace:\n",
    "\n",
    "<a href=\"https://arize.com/blog-course/mastering-openai-api-tips-and-tricks/#roles-in-messages-and-temperature\" target=\"_blank\">Valores de 'role' permitidos</a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "897a02d6-159d-4023-8cdd-f041df27fefa",
   "metadata": {},
   "outputs": [],
   "source": [
    "from ollama import chat\n",
    "from ollama import ChatResponse\n",
    "\n",
    "response: ChatResponse = chat(model='deepseek-r1', messages=[\n",
    "  {\n",
    "    'role': 'user',\n",
    "    'content': 'Dime algo sobre Valladolid',\n",
    "  },\n",
    "])\n",
    "print(response['message']['content'])\n",
    "# or access fields directly from the response object\n",
    "print(response.message.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e1bf07a-4b73-4fac-951a-f0aca0639cd9",
   "metadata": {},
   "source": [
    "## ollama desde OpenAi.\n",
    "\n",
    "Igualmente ollama se puede utilizar desde la librería de OpenAi. Primero se instala la librería de la siguiente manera"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "42a76714-f389-43ce-93ab-46770896528f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install openai"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17deec84-0a85-4258-9ae1-09677a7246e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "Un ejemplo de utilización de la librería es el siguiente (como siempre debe estar el servidor de ollama levantado):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "418ea661-7eda-46ce-9337-109ce9de769c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<think>\n",
      "Alright, so the user first asked, \"Who won the World Series in 2020?\" and I responded that the LA Dodgers won.\n",
      "\n",
      "Now they're following up with a question about where the series took place. They want to know the location of the game.\n",
      "\n",
      "I should make sure my response is clear and concise, providing both the city or country where it happened and specifically mentioning LA since that's relevant from their previous query about the winning team.\n",
      "\n",
      "Also, using bold might help if they were expecting a specific formatting in the future, but for now, keeping it straightforward.\n",
      "\n",
      "Alright, let me structure this so it's helpful.\n",
      "</think>\n",
      "\n",
      "The 2020 World Series was played in **Los Angeles**, California. Specifically, the final game was hosted at **Brookline Park** in Los Angeles.\n"
     ]
    }
   ],
   "source": [
    "from openai import OpenAI\n",
    "\n",
    "client = OpenAI(\n",
    "    base_url = 'http://localhost:11434/v1',\n",
    "    api_key='ollama', # campo requerido pero no usado\n",
    ")\n",
    "\n",
    "response = client.chat.completions.create(\n",
    "  model=\"deepseek-r1:latest\",\n",
    "  messages=[\n",
    "    {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n",
    "    {\"role\": \"user\", \"content\": \"Who won the world series in 2020?\"},\n",
    "    {\"role\": \"assistant\", \"content\": \"The LA Dodgers won in 2020.\"},\n",
    "    {\"role\": \"user\", \"content\": \"Where was it played?\"}\n",
    "  ]\n",
    ")\n",
    "print(response.choices[0].message.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be900854-ff45-4d35-a8fc-2edd34e358e8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f9b1194a-2847-40cf-bc6c-e501f336b1d9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<think>\n",
      "Okay, the user is asking \"dime cual es la capital de España,\" which means \"Name the capital of Spain.\" \n",
      "\n",
      "I know that the capital of Spain is Madrid. \n",
      "\n",
      "Madrid is a major city in central Spain famous for places like the Príncipe Square and La Latina Street.\n",
      "\n",
      "It's a significant cultural and economic center.\n",
      "</think>\n",
      "\n",
      "La capital de España es **MADRID**.\n",
      "\n",
      "MADRID es una ciudad muy importante en España, conocida por su disponibilidad turística, su ubicación estratégica y por la celebración de multitud de eventos como la Feria de San Fermán."
     ]
    }
   ],
   "source": [
    "from openai import OpenAI\n",
    "\n",
    "client = OpenAI(\n",
    "    base_url = 'http://localhost:11434/v1',\n",
    "    api_key='ollama', # campo requerido pero no usado\n",
    ")\n",
    "\n",
    "stream = client.chat.completions.create(\n",
    "    model=\"deepseek-r1:latest\",\n",
    "    messages=[{\"role\": \"user\", \"content\": \"Dime cual es la capital de España\"}],\n",
    "    stream=True,\n",
    ")\n",
    "for chunk in stream:\n",
    "    if chunk.choices[0].delta.content is not None:\n",
    "        print(chunk.choices[0].delta.content, end=\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53252a92-05dc-49a1-83f6-929ba9a8cbac",
   "metadata": {},
   "source": [
    "## ollama desde LangChain.\n",
    "\n",
    "LangChain es un marco de desarrollo diseñado para crear aplicaciones impulsadas por modelos de lenguaje, como chatbots, agentes autónomos y sistemas de recuperación de información. Su principal objetivo es facilitar la integración de modelos de inteligencia artificial con diversas fuentes de datos, herramientas y entornos de ejecución.\n",
    "\n",
    "Este framework permite conectar modelos de lenguaje como GPT con bases de datos, API externas, documentos y otras herramientas para construir aplicaciones más dinámicas y contextualmente enriquecidas. Además, LangChain proporciona componentes reutilizables y modulares, como *Chains* (cadenas de procesamiento), *Agents* (agentes autónomos) y *Memory* (memoria conversacional), que permiten una mejor gestión de las interacciones con la IA.\n",
    "\n",
    "LangChain es compatible con múltiples entornos y se puede usar tanto en Python como en JavaScript. Su flexibilidad lo hace ideal para desarrollar asistentes virtuales, generación de texto personalizada, automatización de tareas y otras aplicaciones avanzadas de IA.\n",
    "\n",
    "LangChain se puede enfocar para modelos ollama de \"text completion\" o también para modelos \"chat completions\"\n",
    "\n",
    "En esta introducción nos vamos a centrar en los modelo \"text completion\" , para el otro tipo de modelos, se remite al lector <a href=\"https://python.langchain.com/docs/concepts/chat_models/\" target=\"_blank\"> a este enlace </a>. Los distintos modelos de Langchain para ollama se  <a href=\"https://python.langchain.com/api_reference/ollama/index.html2\" target=\"_blank\"> pueden ver en este enlace </a>.\n",
    "\n",
    "### Modelos text completion\n",
    "```{index} langchain, text completion\n",
    "```\n",
    "\n",
    "Lo primero que debemos saber es si tenemos instalado o no en nuestro equipo el paquete 'langcain-ollama'. Si no lo tenemos instalado debemos ejecutar lo siguiente:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d029a2fa-e160-4c0a-b3e9-8d6addf380f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install -U langchain-ollama"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d4904525-891a-40af-a9e7-7c86d160b7e3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"<think>\\nOkay, I need to understand what LangChain is. From the previous introduction, it's an open-source framework developed by Hugging Face. It seems like it's used for building conversational AI systems. The user mentioned it supports chaining different models and custom code through a pipeline interface. That makes sense because building complex NLP tasks often requires combining multiple components.\\n\\nThe key features include modularity, extensibility, multi-modal capabilities, support for distributed training, and integration with other tools like LLMs and Databases. I should probably elaborate on each of these points to show a comprehensive understanding.\\n\\nModularity means you can use pre-trained models or custom ones without much hassle. Extensibility is important because it allows adding new components as needed. Multi-modal support would be beneficial for integrating images, videos, etc., into applications.\\n\\nDistributed training with LangChain probably refers to scaling up AI systems across multiple GPUs or nodes, which is useful for handling larger datasets or more complex models.\\n\\nIntegration with LLMs and databases suggests it's versatile in different application environments, whether standalone or part of a bigger system.\\n\\nI should also mention its use cases like customer support chatbots or personal assistants. Explaining how it helps build scalable applications would be good because scalability is a big concern in AI deployment.\\n\\nIn summary, my answer needs to cover these points clearly and concisely, showing that I understand LangChain's purpose, features, benefits, and applicable scenarios.\\n</think>\\n\\n**Introducción a LangChain**\\n\\nLangChain es un framework abierto fuente desarrollado por Hugging Face, diseñado para construir sistemas de inteligencia artificial (IA) basados en la interacción humano-AI. Este framework permite integrar múltiples modelos y código personalizado a través de una interfaz pipeline, facilitando la construcción de tareas NLP complejas que requieren combinar diversos componentes.\\n\\nCaracterísticas principales de LangChain incluyen su modularidad, extensibilidad, apoyo por modalidades múltiples, posibilidad de entrenamiento distribuido y integración con herramientas adicionales como los modelos Large Language Models (LLMs) y bases de datos. Esta versatile Herramienta es ideal para aplicaciones como asistentes personales, soporte al cliente mediante chatbots, o sistemas más complejos que requieren escalabilidad.\\n\\nLangChain ajuda a construir aplicaciones IA de forma escalable y eficiente, permitiendo el uso de modelos pre-entrenados o personalizados y la integración de datos multimodales para ampliar su funcionalidad.\""
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_ollama.llms import OllamaLLM\n",
    "\n",
    "template = \"\"\"Question: {question}\n",
    "\n",
    "Answer: Let's think step by step.\"\"\"\n",
    "\n",
    "prompt = ChatPromptTemplate.from_template(template)\n",
    "\n",
    "model = OllamaLLM(model=\"deepseek-r1:latest\")\n",
    "\n",
    "chain = prompt | model\n",
    "\n",
    "chain.invoke({\"question\": \"Dame una introducción a Langchain?\"})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cbf8934c-f7c1-4868-946d-2db82485d2d5",
   "metadata": {},
   "source": [
    "### Modelos de tipo chat\n",
    "\n",
    "A continuación un ejemplo de código de tipo chat hecho desde Langchaim. La clase ChatOllama se puede ver <a href=\"https://python.langchain.com/api_reference/ollama/chat_models/langchain_ollama.chat_models.ChatOllama.html\" target=\"_blank\"> sus propiedades en este enlace </a>."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "90a47e7b-d50c-406b-a5a1-597ab54c00ba",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content='Me encanta programar.', additional_kwargs={}, response_metadata={'model': 'llama3.1', 'created_at': '2025-02-14T17:29:53.0197799Z', 'done': True, 'done_reason': 'stop', 'total_duration': 4145183500, 'load_duration': 38339900, 'prompt_eval_count': 32, 'prompt_eval_duration': 2703000000, 'eval_count': 7, 'eval_duration': 1401000000, 'message': Message(role='assistant', content='', images=None, tool_calls=None)}, id='run-de06622e-e44d-435b-9099-fc22576282ee-0', usage_metadata={'input_tokens': 32, 'output_tokens': 7, 'total_tokens': 39})"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_ollama import ChatOllama\n",
    "# Instanciamos la clase\n",
    "llm = ChatOllama(\n",
    "    model = \"llama3.1\",\n",
    "    temperature = 0.8,\n",
    "    num_predict = 256,\n",
    ")\n",
    "\n",
    "messages = [\n",
    "    (\"system\", \"You are a helpful translator. Translate the user sentence to Spanish.\"),\n",
    "    (\"human\", \"I love programming.\"),\n",
    "]\n",
    "llm.invoke(messages)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "148386ae-9e51-42ec-8ada-0a92633db2b7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "content='Hello' additional_kwargs={} response_metadata={} id='run-4d0c55f6-3f8f-4463-885c-d794e931d162'\n",
      "content=' World' additional_kwargs={} response_metadata={} id='run-4d0c55f6-3f8f-4463-885c-d794e931d162'\n",
      "content='!' additional_kwargs={} response_metadata={} id='run-4d0c55f6-3f8f-4463-885c-d794e931d162'\n",
      "content='' additional_kwargs={} response_metadata={'model': 'llama3.1', 'created_at': '2025-02-14T17:30:56.9842802Z', 'done': True, 'done_reason': 'stop', 'total_duration': 3373773000, 'load_duration': 68521000, 'prompt_eval_count': 16, 'prompt_eval_duration': 2580000000, 'eval_count': 4, 'eval_duration': 720000000, 'message': Message(role='assistant', content='', images=None, tool_calls=None)} id='run-4d0c55f6-3f8f-4463-885c-d794e931d162' usage_metadata={'input_tokens': 16, 'output_tokens': 4, 'total_tokens': 20}\n"
     ]
    }
   ],
   "source": [
    "messages = [\n",
    "    (\"human\", \"Return the words Hello World!\"),\n",
    "]\n",
    "for chunk in llm.stream(messages):\n",
    "    print(chunk)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fce829bc-d60c-4181-a5d3-fc335e885964",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "244be744-2e23-4510-8850-abffad6b8603",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"<think>\\nAlright, the user is asking for the capital of Spain. I know that Madrid is the capital.\\n\\nI should confirm it and maybe add a bit more information about why it's important.\\n\\nAlso, mentioning some key facts like its population could give more context.\\n</think>\\n\\nLa capital de España es **Madrid**. Es un centro político, administrativo y cultural de la nation, con una población de más de 3 millones de habitantes.\""
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_community.llms.ollama import Ollama\n",
    "#from langchain.callbacks.manager import CallbackManager \n",
    "#from langchain.callbacks.streaming_stdout import StreamingStdOutCallbackHandler\n",
    "\n",
    "MODEL='deepseek-r1:latest'\n",
    "\n",
    "llm = Ollama(model=MODEL)\n",
    "llm.invoke(\"Dime la capital de España\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3db6c8b6-4500-44db-b83e-abc56b0cfd3f",
   "metadata": {},
   "source": [
    "Otro ejemplo, pero utilizando templates, puede ser el siguiente, en el que se construye una especie de chat-bot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "5fd7103e-1e06-4954-bcda-8d4ef05e1456",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Welcome to the AI ChatBot! Type 'exit to quit.\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "You:  hey how are u doing\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bot:  <think>\n",
      "Alright, so the user greeted me with \"hey how are u doing.\" Hmm, that's pretty casual. I need to respond in a friendly and approachable way. Maybe start with acknowledging their greeting.\n",
      "\n",
      "Wait, but looking back at my response template, it just says \"Let's think step by step.\" That might not be appropriate here since the user asked for an answer, not some problem-solving process.\n",
      "\n",
      "Okay, so I should adjust that. Instead of using the template, I'll craft a more conversational reply. Let me consider the possible ways to respond.\n",
      "\n",
      "I could say something like, \"Hey! I'm just a program, so I don't have feelings, but thanks for asking!\" That's light and fun.\n",
      "\n",
      "Or maybe, \"Not bad, thanks! How about you?\" But since they already asked how I am, that might not be necessary. Alternatively, acknowledge their greeting and then answer the question in a friendly manner.\n",
      "\n",
      "Wait, perhaps \"Hey! I'm just an AI, so I don't have feelings, but thanks for asking!\" That's simple and clear.\n",
      "\n",
      "Alternatively, I could say, \"I'm doing well, thank you! How about you?\" But since they already asked, maybe it's better to answer their question directly. Wait no, they asked how am I, not them. So I should address that in my response.\n",
      "\n",
      "So, putting it all together: \"Hey! I'm just an AI, so I don't have feelings or a body, but thanks for asking!\" That seems appropriate and friendly.\n",
      "</think>\n",
      "\n",
      "Hey! Just an AI here, so no feelings or body, but thanks for asking! How about you?\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "You:  exit\n"
     ]
    }
   ],
   "source": [
    "from langchain_ollama import OllamaLLM\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "\n",
    "template =\"\"\"\n",
    "Answer the question below.\n",
    "\n",
    "Here is the conversation history: {context}\n",
    "\n",
    "Question: {question}\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "model = OllamaLLM(model=MODEL)\n",
    "prompt =ChatPromptTemplate.from_template(template)\n",
    "cahin = prompt | model\n",
    "\n",
    "def handle_conversation():\n",
    "    context =\"\"\n",
    "    print(\"Welcome to the AI ChatBot! Type 'exit' to quit.\")\n",
    "    while True:\n",
    "        user_input =input(\"You: \")\n",
    "        if user_input.lower() == \"exit\":\n",
    "            break\n",
    "        result =chain.invoke({\"context\":context,\"question\":user_input}) \n",
    "        print(\"Bot: \", result)\n",
    "        context += f\"\\nUser: {user_input}\\nAI: {result}\"\n",
    "\n",
    "\n",
    "handle_conversation()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93fce684-abfe-4685-a34d-93daac11f7cc",
   "metadata": {},
   "source": [
    "## Apéndice\n",
    "\n",
    "A continuación se muestra una serie de enlaces donde se explican diferentes aspectos de ollama\n",
    "\n",
    "* <a href=\"https://ollama.com/\" target=\"_blank\">Página oficial ollama</a>\n",
    "* <a href=\"https://github.com/ollama/ollama?tab=readme-ov-file\" target=\"_blank\">Página oficial ollama en github</a>\n",
    "* <a href=\"https://github.com/ollama/ollama/tree/main/docs\" target=\"_blank\">Docs de ollama</a>\n",
    "* <a href=\"https://github.com/ollama/ollama-python\" target=\"_blank\">librería python para ollama</a>\n",
    "* <a href=\"https://platform.openai.com/docs/api-reference/introduction\" target=\"_blank\">Librería OpenAI</a>\n",
    "* <a href=\"https://inside.caratlane.com/apidog-for-qa-professionals-key-features-benefits-and-best-practices-524302172e6c\" target=\"_blank\">APIDog</a>\n",
    "* <a href=\"https://www.youtube.com/watch?v=sGUjmyfof4Q\" target=\"_blank\">Construcción \"deep researcher\" con DeepSeek-R1 y ollama (vídeo)</a>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29c046e7-ff89-43bf-b9cf-afe86dc34a3a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
