{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e01f2b12-00ee-4519-a969-eea5488d1e91",
   "metadata": {},
   "source": [
    "# Introducci√≥n a la generaci√≥n de texto.\n",
    "\n",
    "**NOTA1:** Este documento es una traducci√≥n del documento <a href=\"https://machinelearningmastery.com/auto-completion-style-text-generation-with-gpt-2-model/?utm_source=drip&utm_medium=email&utm_campaign=MLM+Newsletter+February+28%2C+2025&utm_content=Auto-Completion+Style+Text+Generation+with+GPT-2+Model+%E2%80%A2+Your+First+Machine+Learning+Project+in+Python+Step-By-Step\" target=\"_blank\"> que se puede ver en este enlace </a>.\n",
    "\n",
    "**NOTA2**: Se aconseja ejecutar los c√≥digos que aqu√≠ se presentan en google colab si no se dispone de un ordenador potente y con GPU para agilizar la ejecuci√≥n del c√≥digo"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7a2c996-0738-4fdd-96a0-e9d26fdb3219",
   "metadata": {},
   "source": [
    "```{index} transformers\n",
    "```\n",
    "Generar texto sin sentido es un ejercicio de programaci√≥n sencillo para principiantes, pero completar una oraci√≥n de forma significativa requerir√≠a mucho trabajo. El panorama de la tecnolog√≠a de autocompletado se ha transformado dr√°sticamente con la introducci√≥n de enfoques neuronales. Con la biblioteca de transformers de Hugging Face, implementar el autocompletado de texto requiere solo unas pocas l√≠neas de c√≥digo. En este tutorial completo, implementar√° varios ejemplos y explorar√° c√≥mo los sistemas modernos se diferencian de los tradicionales y por qu√© estas diferencias son importantes.\n",
    "\n",
    "## Descripci√≥n general.\n",
    "\n",
    "Este apartado consta de cuatro bloques:\n",
    "\n",
    "* Enfoques tradicionales y enfoques neuronales\n",
    "* Arquitectura de autocompletado\n",
    "* Implementaci√≥n b√°sica de autocompletar\n",
    "* Almacenamiento en cach√© y entrada por lotes\n",
    "\n",
    "## Enfoques tradicionales y enfoques neuronales\n",
    "```{index} N-gram (modelos)\n",
    "```\n",
    "Cuando escribes una palabra en la barra de b√∫squeda de Google, como ‚Äúm√°quina‚Äù, es posible que aparezcan otras palabras adicionales, como ‚Äúaprendizaje‚Äù, para formar ‚Äúaprendizaje autom√°tico‚Äù. Se trata de una tecnolog√≠a de autocompletado . La sugerencia puede no ser la que esperas, pero siempre es coherente.\n",
    "\n",
    "Los sistemas tradicionales de autocompletado se han basado en m√©todos relativamente estad√≠sticos. Los modelos de N-gram predicen la siguiente palabra observando una ventana fija de palabras anteriores y compar√°ndolas con muestras recopiladas. Este m√©todo tiene dificultades con contextos m√°s largos y combinaciones novedosas. Los enfoques basados en diccionarios solo pueden sugerir palabras que ya han visto antes, lo que limita su capacidad para manejar terminolog√≠a nueva. El an√°lisis de frecuencia proporciona sugerencias basadas en patrones comunes, pero a menudo pasa por alto el contexto matizado del texto actual.\n",
    "\n",
    "Los sistemas de autocompletado neuronal, en particular los basados en GPT-2, representan un cambio fundamental en la capacidad. Estos sistemas entienden el contexto en lugar de buscar coincidencias de palabras. Consideran todo el alcance del texto anterior en lugar de solo unas pocas palabras. Captan relaciones sem√°nticas, lo que les permite sugerir opciones de finalizaci√≥n que coinciden no solo con la gram√°tica sino tambi√©n con el significado del texto. La capacidad generativa les permite producir frases u oraciones completas que mantienen la coherencia con el contenido existente.\n",
    "\n",
    "## Arquitectura de autocompletado.\n",
    "\n",
    "Un moderno sistema de autocompletado neuronal integra varios componentes sofisticados que funcionan juntos sin problemas.\n",
    "\n",
    "El modelo de lenguaje funciona como motor cognitivo. Procesa el texto de entrada y mantiene un estado interno para capturar los matices del proceso de generaci√≥n de texto en curso. El componente de tokenizaci√≥n act√∫a como un puente entre el texto legible por humanos y las representaciones num√©ricas del modelo. El controlador de generaci√≥n organiza el proceso, empleando estrategias avanzadas para filtrar y clasificar las posibles finalizaciones. Equilibra cuidadosamente el tiempo de respuesta y la calidad de las sugerencias, lo que garantiza que los usuarios reciban finalizaciones √∫tiles sin demoras notables.\n",
    "\n",
    "El desarrollo de un sistema de autocompletado neuronal eficaz implica superar varios desaf√≠os cr√≠ticos. La latencia es una preocupaci√≥n principal, ya que los usuarios esperan un tiempo de respuesta en milisegundos mientras manejan la complejidad computacional de las operaciones de redes neuronales.\n",
    "\n",
    "El control de calidad es otro desaf√≠o. Se espera que el sistema genere sugerencias relevantes, por lo que se requieren mecanismos de filtrado avanzados para evitar que se completen los campos de forma inapropiada y garantizar que las sugerencias se ajusten al dominio y al estilo de escritura del usuario.\n",
    "\n",
    "La gesti√≥n de recursos es crucial a la hora de escalar el sistema para que admita a varios usuarios. Las importantes demandas de memoria de los modelos neuronales y la intensidad computacional de la generaci√≥n de texto deben equilibrarse cuidadosamente con los recursos del sistema y los requisitos de tiempo de respuesta.\n",
    "\n",
    "## Implementaci√≥n b√°sica de autocompletar\n",
    "\n",
    "Dejemos de lado las consideraciones de un sistema m√°s grande y centr√©monos en una funci√≥n de autocompletado simple para texto parcial. Es f√°cil de implementar utilizando los modelos entrenados previamente de la biblioteca de transformers:\n",
    "\n",
    "(El modelo que utilizamos aqu√≠ lo podemos ver en HugginFace <a href=\"https://huggingface.co/MazharLughmani/GPT2LMHeadModel\" target=\"_blank\"> en este enlace </a> )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c2620bc0-c6ab-492c-8d51-a707cdfb7e43",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\MisTrabajos\\IA_generativa\\venv\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input: The future of artificial\n",
      "Completion:  intelligence is uncertain, but it is becoming increasingly likely that someday, when we become self-aware, we'll be able to \"know\" it.\n",
      "\n",
      "In a recent article, Richard Schmalz and I discussed why self\n"
     ]
    }
   ],
   "source": [
    "from transformers import GPT2LMHeadModel, GPT2Tokenizer\n",
    "import torch\n",
    "\n",
    "class AutoComplete:\n",
    "    def __init__(self, model_name='gpt2'):\n",
    "        \"\"\"Initialize the auto-complete system.\"\"\"\n",
    "        self.tokenizer = GPT2Tokenizer.from_pretrained(model_name)\n",
    "        self.model = GPT2LMHeadModel.from_pretrained(model_name)\n",
    "        self.device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "        self.model.to(self.device)\n",
    "        self.model.eval()  # Set to evaluation mode\n",
    "\n",
    "    def get_completion(self, text, max_length=50):\n",
    "        \"\"\"Generate completion for the input text.\"\"\"\n",
    "        # Encode the input text\n",
    "        inputs = self.tokenizer(text, add_special_tokens=False, return_tensors=\"pt\")\n",
    "        input_ids = inputs[\"input_ids\"].to(self.device)\n",
    "        attn_masks = inputs[\"attention_mask\"].to(self.device)\n",
    "\n",
    "        # Generate completion\n",
    "        with torch.no_grad():\n",
    "            outputs = self.model.generate(\n",
    "                input_ids,\n",
    "                attention_mask=attn_masks,\n",
    "                max_length=max_length,\n",
    "                num_return_sequences=1,\n",
    "                pad_token_id=self.tokenizer.eos_token_id,\n",
    "                do_sample=True,\n",
    "                temperature=0.7\n",
    "            )\n",
    "\n",
    "        # Decode and extract completion\n",
    "        full_text = self.tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "        completion = full_text[len(text):]\n",
    "\n",
    "        return completion\n",
    "\n",
    "# using autocomplete to see what we get\n",
    "auto_complete = AutoComplete()\n",
    "text = \"The future of artificial\"\n",
    "completion = auto_complete.get_completion(text)\n",
    "print(f\"Input: {text}\")\n",
    "print(f\"Completion: {completion}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2259937f-00ce-481e-83f3-ecad9c2af72b",
   "metadata": {},
   "source": [
    "Veamos qu√© hace el c√≥digo anterior. Defini√≥ la clase *AutoComplete*, que se carga GPT2Tokenizercomo tokenizador de texto y GPT2LMHeadModelcomo un modelo GPT-2 entrenado previamente capaz de generar texto. El modelo est√° configurado en modo de evaluaci√≥n ya que est√° utilizando el modelo, no para entrenarlo.\n",
    "\n",
    "La generaci√≥n de texto est√° en la funci√≥n *get_completion()*. El texto de entrada se convierte en tokens antes de pasarlo al modelo. Invoca el modelo con *torch.no_grad()* contexto para omitir el c√°lculo del gradiente y ahorrar tiempo y memoria. Se llama al modelo con temperature=0.7 para lograr una creatividad equilibrada. La salida del modelo debe convertirse nuevamente en texto mediante el tokenizador. Los otros par√°metros en *self.model.generate()* son:\n",
    "\n",
    "* *num_return_sequences=1* para generar solo una finalizaci√≥n. El modelo puede generar m√∫ltiples salidas para la misma entrada.\n",
    "* *pad_token_id=self.tokenizer.eos_token_id* para evitar el relleno innecesario\n",
    "* *do_sample=True* para permitir el muestreo en lugar de la generaci√≥n de texto determinista. Es necesario para la generaci√≥n creativa.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09ad9bb8-a11c-4465-8475-95aadca7dece",
   "metadata": {},
   "source": [
    "## Almacenamiento en cach√© y entrada por lotes.\n",
    "\n",
    "El c√≥digo anterior funciona como un programa simple, pero necesita algo de pulido para ejecutarlo como un servicio.\n",
    "\n",
    "Primero, implementemos un sistema de almacenamiento en cach√© para mejorar el rendimiento de las aplicaciones en tiempo real:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "58a9f9c0-058f-4a2f-a272-146a0f2a7a74",
   "metadata": {},
   "outputs": [],
   "source": [
    "from functools import lru_cache\n",
    "\n",
    "class CachedAutoComplete(AutoComplete):\n",
    "    def __init__(self, cache_size=1000, **kwargs):\n",
    "        \"\"\"Initialize with caching support.\"\"\"\n",
    "        super().__init__(**kwargs)\n",
    "        self.get_completion = lru_cache(maxsize=cache_size)(\n",
    "            self.get_completion\n",
    "        )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7631541-c25b-4bbb-a555-86d21687672d",
   "metadata": {},
   "source": [
    "Esto se basa en la clase anterior al decorar la funci√≥n de generaci√≥n con un cach√© LRU. La biblioteca de Python maneja el almacenamiento en cach√© autom√°ticamente. Simplemente use *CachedAutoComplete* en lugar de *AutoComplete* y todo funcionar√° de la misma manera, excepto que el cach√© devolver√° instant√°neamente los resultados de las entradas procesadas previamente.\n",
    "\n",
    "Ahora, optimicemos a√∫n m√°s el sistema para lograr un mejor rendimiento en tiempo real. Uno de los desaf√≠os de la creaci√≥n de un servicio es manejar varios usuarios simult√°neamente, por lo que resulta beneficioso procesar varias entradas como un lote. Sin embargo, esto aumenta el uso de memoria. Puede mitigar la carga de trabajo adicional reduciendo el tama√±o del modelo mediante n√∫meros flotantes de 16 bits:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0628b0ad-ce80-472a-93d2-d673674b0077",
   "metadata": {},
   "outputs": [],
   "source": [
    "class OptimizedAutoComplete(CachedAutoComplete):\n",
    "    def __init__(self, **kwargs):\n",
    "        \"\"\"Initialize with optimizations.\"\"\"\n",
    "        super().__init__(**kwargs)\n",
    "        self.tokenizer.pad_token = self.tokenizer.eos_token\n",
    "\n",
    "        if self.device == \"cuda\":\n",
    "            self.model = self.model.half()  # Use FP16 on GPU\n",
    "\n",
    "        # use eval mode and cuda graphs\n",
    "        self.model.eval()\n",
    "\n",
    "    def preprocess_batch(self, texts):\n",
    "        \"\"\"Efficiently process multiple texts.\"\"\"\n",
    "        # Tokenize all texts at once\n",
    "        inputs = self.tokenizer(texts, padding=True, truncation=True, return_tensors=\"pt\")\n",
    "        return inputs.to(self.device)\n",
    "\n",
    "    def generate_batch(self, texts, max_length=50):\n",
    "        \"\"\"Generate completions for multiple texts.\"\"\"\n",
    "        # Preprocess batch\n",
    "        inputs = self.preprocess_batch(texts)\n",
    "\n",
    "        # Generate completions\n",
    "        with torch.no_grad():\n",
    "            outputs = self.model.generate(\n",
    "                inputs['input_ids'],\n",
    "                attention_mask=inputs['attention_mask'],\n",
    "                max_length=max_length,\n",
    "                num_return_sequences=1,\n",
    "                pad_token_id=self.tokenizer.eos_token_id,\n",
    "                do_sample=True,\n",
    "                temperature=0.7\n",
    "            )\n",
    "\n",
    "        # Decode completions\n",
    "        completions = self.tokenizer.batch_decode(outputs, skip_special_tokens=True)\n",
    "\n",
    "        # Extract new text\n",
    "        results = []\n",
    "        for text, completion in zip(texts, completions):\n",
    "            results.append(completion[len(text):])\n",
    "\n",
    "        return results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18286cb8-bf65-498c-9a58-b5df3b84ddf4",
   "metadata": {},
   "source": [
    "Convertir un modelo en un valor de punto flotante de 16 bits es tan sencillo como hacerlo *self.model = self.model.half()* en el constructor. La mayor√≠a de las CPU no admiten valores de punto flotante de 16 bits. Por lo tanto, debe hacerlo solo si puede ejecutar el modelo en una GPU. Tenga en cuenta que la funci√≥n *generate_batch()* es b√°sicamente la misma que la funci√≥n  *generate()* anterior, pero debe procesar y colocar la salida por lotes en una lista.\n",
    "\n",
    "A continuaci√≥n se muestra el c√≥digo completo, incluido c√≥mo utilizar la generaci√≥n por lotes:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "262da4db-2014-4cbe-9975-6320328ed235",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Input: Machine learning is\n",
      "Completion:  the development of new applications that integrate with existing technology rather than develop new hardware and techniques. That is a very important distinction. As we continue to build new applications, we need to be able to develop new technologies that are less dependent\n",
      "\n",
      "Input: Deep neural networks can\n",
      "Completion:  create a network of neurons capable of processing different types of information (e.g., the information that is received by a neuron and what is received by an electric field). The neural network that generates this information can then be used to\n",
      "\n",
      "Input: The training process involves\n",
      "Completion:  three stages: the first is the initial setup, the second is the initial implementation, and the third is the final execution. The third stage consists of the execution of the training process, with the final step having been completed. The\n"
     ]
    }
   ],
   "source": [
    "from functools import lru_cache\n",
    "from transformers import GPT2LMHeadModel, GPT2Tokenizer\n",
    "import torch\n",
    "\n",
    "\n",
    "class AutoComplete:\n",
    "    def __init__(self, model_name=\"gpt2\"):\n",
    "        \"\"\"Initialize the auto-complete system.\"\"\"\n",
    "        self.tokenizer = GPT2Tokenizer.from_pretrained(model_name, padding_side=\"left\")\n",
    "        self.model = GPT2LMHeadModel.from_pretrained(model_name)\n",
    "        self.device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "        self.model.to(self.device)\n",
    "        self.model.eval()  # Set to evaluation mode\n",
    "\n",
    "    def get_completion(self, text, max_length=50):\n",
    "        \"\"\"Generate completion for the input text.\"\"\"\n",
    "        print(\"**** Completion:\", text)\n",
    "        # Encode the input text\n",
    "        inputs = self.tokenizer(text, add_special_tokens=False, return_tensors=\"pt\")\n",
    "        input_ids = inputs[\"input_ids\"].to(self.device)\n",
    "        attn_masks = inputs[\"attention_mask\"].to(self.device)\n",
    "\n",
    "        # Generate completion\n",
    "        with torch.no_grad():\n",
    "            outputs = self.model.generate(\n",
    "                input_ids,\n",
    "                attention_mask=attn_masks,\n",
    "                max_length=max_length,\n",
    "                num_return_sequences=1,\n",
    "                pad_token_id=self.tokenizer.eos_token_id,\n",
    "                do_sample=True,\n",
    "                temperature=0.7\n",
    "            )\n",
    "\n",
    "        # Decode and extract completion\n",
    "        full_text = self.tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "        completion = full_text[len(text):]\n",
    "\n",
    "        return completion\n",
    "\n",
    "\n",
    "class CachedAutoComplete(AutoComplete):\n",
    "    def __init__(self, cache_size=1000, **kwargs):\n",
    "        \"\"\"Initialize with caching support.\"\"\"\n",
    "        super().__init__(**kwargs)\n",
    "        self.get_completion = lru_cache(maxsize=cache_size)(\n",
    "            self.get_completion\n",
    "        )\n",
    "\n",
    "\n",
    "class OptimizedAutoComplete(CachedAutoComplete):\n",
    "    def __init__(self, **kwargs):\n",
    "        \"\"\"Initialize with optimizations.\"\"\"\n",
    "        super().__init__(**kwargs)\n",
    "        self.tokenizer.pad_token = self.tokenizer.eos_token\n",
    "\n",
    "        if self.device == \"cuda\":\n",
    "            self.model = self.model.half()  # Use FP16 on GPU\n",
    "\n",
    "        # use eval mode and cuda graphs\n",
    "        self.model.eval()\n",
    "\n",
    "    def preprocess_batch(self, texts):\n",
    "        \"\"\"Efficiently process multiple texts.\"\"\"\n",
    "        # Tokenize all texts at once\n",
    "        inputs = self.tokenizer(texts, padding=True, truncation=True, return_tensors=\"pt\")\n",
    "        return inputs.to(self.device)\n",
    "\n",
    "    def generate_batch(self, texts, max_length=50):\n",
    "        \"\"\"Generate completions for multiple texts.\"\"\"\n",
    "        # Preprocess batch\n",
    "        inputs = self.preprocess_batch(texts)\n",
    "\n",
    "        # Generate completions\n",
    "        with torch.no_grad():\n",
    "            outputs = self.model.generate(\n",
    "                inputs[\"input_ids\"],\n",
    "                attention_mask=inputs[\"attention_mask\"],\n",
    "                max_length=max_length,\n",
    "                num_return_sequences=1,\n",
    "                pad_token_id=self.tokenizer.eos_token_id,\n",
    "                do_sample=True,\n",
    "                temperature=0.7\n",
    "            )\n",
    "\n",
    "        # Decode completions\n",
    "        completions = self.tokenizer.batch_decode(outputs, skip_special_tokens=True)\n",
    "\n",
    "        # Extract new text\n",
    "        results = []\n",
    "        for text, completion in zip(texts, completions):\n",
    "            results.append(completion[len(text):])\n",
    "\n",
    "        return results\n",
    "\n",
    "# Example: Optimized batch completion\n",
    "optimized_complete = OptimizedAutoComplete()\n",
    "texts = [\n",
    "    \"Machine learning is\",\n",
    "    \"Deep neural networks can\",\n",
    "    \"The training process involves\"\n",
    "]\n",
    "completions = optimized_complete.generate_batch(texts)\n",
    "for text, completion in zip(texts, completions):\n",
    "    print(f\"\\nInput: {text}\")\n",
    "    print(f\"Completion: {completion}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ccdae2e-4342-449e-ac44-dd92a3d8778b",
   "metadata": {},
   "source": [
    "## Resumen\n",
    "\n",
    "En este tutorial, se ha visto c√≥mo crear un sistema de autocompletado inteligente utilizando GPT-2. En concreto, se ha expuesto lo siguiente:\n",
    "\n",
    "* La teor√≠a detr√°s de los sistemas de autocompletado neuronal\n",
    "* C√≥mo implementar el autocompletado b√°sico\n",
    "* C√≥mo agregar almacenamiento en cach√© para un mejor rendimiento\n",
    "* C√≥mo hacer sugerencias teniendo en cuenta el contexto\n",
    "* C√≥mo optimizar para el uso en tiempo real"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee2abd4e-758e-4e8c-8b50-cafd177e00da",
   "metadata": {},
   "source": [
    "## Bases de datos vectoriales.\n",
    "\n",
    "üöÄ "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
