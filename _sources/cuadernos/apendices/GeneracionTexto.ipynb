{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e01f2b12-00ee-4519-a969-eea5488d1e91",
   "metadata": {},
   "source": [
    "# Introducción a la generación de texto.\n",
    "\n",
    "**NOTA1:** Este documento es una traducción del documento <a href=\"https://machinelearningmastery.com/auto-completion-style-text-generation-with-gpt-2-model/?utm_source=drip&utm_medium=email&utm_campaign=MLM+Newsletter+February+28%2C+2025&utm_content=Auto-Completion+Style+Text+Generation+with+GPT-2+Model+%E2%80%A2+Your+First+Machine+Learning+Project+in+Python+Step-By-Step\" target=\"_blank\"> que se puede ver en este enlace </a>.\n",
    "\n",
    "**NOTA2**: Se aconseja ejecutar los códigos que aquí se presentan en google colab si no se dispone de un ordenador potente y con GPU para agilizar la ejecución del código"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7a2c996-0738-4fdd-96a0-e9d26fdb3219",
   "metadata": {},
   "source": [
    "```{index} transformers\n",
    "```\n",
    "Generar texto sin sentido es un ejercicio de programación sencillo para principiantes, pero completar una oración de forma significativa requeriría mucho trabajo. El panorama de la tecnología de autocompletado se ha transformado drásticamente con la introducción de enfoques neuronales. Con la biblioteca de transformers de Hugging Face, implementar el autocompletado de texto requiere solo unas pocas líneas de código. En este tutorial completo, implementará varios ejemplos y explorará cómo los sistemas modernos se diferencian de los tradicionales y por qué estas diferencias son importantes.\n",
    "\n",
    "## Descripción general.\n",
    "\n",
    "Este apartado consta de cuatro bloques:\n",
    "\n",
    "* Enfoques tradicionales y enfoques neuronales\n",
    "* Arquitectura de autocompletado\n",
    "* Implementación básica de autocompletar\n",
    "* Almacenamiento en caché y entrada por lotes\n",
    "\n",
    "## Enfoques tradicionales y enfoques neuronales\n",
    "```{index} N-gram (modelos)\n",
    "```\n",
    "Cuando escribes una palabra en la barra de búsqueda de Google, como “máquina”, es posible que aparezcan otras palabras adicionales, como “aprendizaje”, para formar “aprendizaje automático”. Se trata de una tecnología de autocompletado . La sugerencia puede no ser la que esperas, pero siempre es coherente.\n",
    "\n",
    "Los sistemas tradicionales de autocompletado se han basado en métodos relativamente estadísticos. Los modelos de N-gram predicen la siguiente palabra observando una ventana fija de palabras anteriores y comparándolas con muestras recopiladas. Este método tiene dificultades con contextos más largos y combinaciones novedosas. Los enfoques basados en diccionarios solo pueden sugerir palabras que ya han visto antes, lo que limita su capacidad para manejar terminología nueva. El análisis de frecuencia proporciona sugerencias basadas en patrones comunes, pero a menudo pasa por alto el contexto matizado del texto actual.\n",
    "\n",
    "Los sistemas de autocompletado neuronal, en particular los basados en GPT-2, representan un cambio fundamental en la capacidad. Estos sistemas entienden el contexto en lugar de buscar coincidencias de palabras. Consideran todo el alcance del texto anterior en lugar de solo unas pocas palabras. Captan relaciones semánticas, lo que les permite sugerir opciones de finalización que coinciden no solo con la gramática sino también con el significado del texto. La capacidad generativa les permite producir frases u oraciones completas que mantienen la coherencia con el contenido existente.\n",
    "\n",
    "## Arquitectura de autocompletado.\n",
    "\n",
    "Un moderno sistema de autocompletado neuronal integra varios componentes sofisticados que funcionan juntos sin problemas.\n",
    "\n",
    "El modelo de lenguaje funciona como motor cognitivo. Procesa el texto de entrada y mantiene un estado interno para capturar los matices del proceso de generación de texto en curso. El componente de tokenización actúa como un puente entre el texto legible por humanos y las representaciones numéricas del modelo. El controlador de generación organiza el proceso, empleando estrategias avanzadas para filtrar y clasificar las posibles finalizaciones. Equilibra cuidadosamente el tiempo de respuesta y la calidad de las sugerencias, lo que garantiza que los usuarios reciban finalizaciones útiles sin demoras notables.\n",
    "\n",
    "El desarrollo de un sistema de autocompletado neuronal eficaz implica superar varios desafíos críticos. La latencia es una preocupación principal, ya que los usuarios esperan un tiempo de respuesta en milisegundos mientras manejan la complejidad computacional de las operaciones de redes neuronales.\n",
    "\n",
    "El control de calidad es otro desafío. Se espera que el sistema genere sugerencias relevantes, por lo que se requieren mecanismos de filtrado avanzados para evitar que se completen los campos de forma inapropiada y garantizar que las sugerencias se ajusten al dominio y al estilo de escritura del usuario.\n",
    "\n",
    "La gestión de recursos es crucial a la hora de escalar el sistema para que admita a varios usuarios. Las importantes demandas de memoria de los modelos neuronales y la intensidad computacional de la generación de texto deben equilibrarse cuidadosamente con los recursos del sistema y los requisitos de tiempo de respuesta.\n",
    "\n",
    "## Implementación básica de autocompletar\n",
    "\n",
    "Dejemos de lado las consideraciones de un sistema más grande y centrémonos en una función de autocompletado simple para texto parcial. Es fácil de implementar utilizando los modelos entrenados previamente de la biblioteca de transformers:\n",
    "\n",
    "(El modelo que utilizamos aquí lo podemos ver en HugginFace <a href=\"https://huggingface.co/MazharLughmani/GPT2LMHeadModel\" target=\"_blank\"> en este enlace </a> )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c2620bc0-c6ab-492c-8d51-a707cdfb7e43",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\MisTrabajos\\IA_generativa\\venv\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input: The future of artificial\n",
      "Completion:  intelligence is uncertain, but it is becoming increasingly likely that someday, when we become self-aware, we'll be able to \"know\" it.\n",
      "\n",
      "In a recent article, Richard Schmalz and I discussed why self\n"
     ]
    }
   ],
   "source": [
    "from transformers import GPT2LMHeadModel, GPT2Tokenizer\n",
    "import torch\n",
    "\n",
    "class AutoComplete:\n",
    "    def __init__(self, model_name='gpt2'):\n",
    "        \"\"\"Initialize the auto-complete system.\"\"\"\n",
    "        self.tokenizer = GPT2Tokenizer.from_pretrained(model_name)\n",
    "        self.model = GPT2LMHeadModel.from_pretrained(model_name)\n",
    "        self.device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "        self.model.to(self.device)\n",
    "        self.model.eval()  # Set to evaluation mode\n",
    "\n",
    "    def get_completion(self, text, max_length=50):\n",
    "        \"\"\"Generate completion for the input text.\"\"\"\n",
    "        # Encode the input text\n",
    "        inputs = self.tokenizer(text, add_special_tokens=False, return_tensors=\"pt\")\n",
    "        input_ids = inputs[\"input_ids\"].to(self.device)\n",
    "        attn_masks = inputs[\"attention_mask\"].to(self.device)\n",
    "\n",
    "        # Generate completion\n",
    "        with torch.no_grad():\n",
    "            outputs = self.model.generate(\n",
    "                input_ids,\n",
    "                attention_mask=attn_masks,\n",
    "                max_length=max_length,\n",
    "                num_return_sequences=1,\n",
    "                pad_token_id=self.tokenizer.eos_token_id,\n",
    "                do_sample=True,\n",
    "                temperature=0.7\n",
    "            )\n",
    "\n",
    "        # Decode and extract completion\n",
    "        full_text = self.tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "        completion = full_text[len(text):]\n",
    "\n",
    "        return completion\n",
    "\n",
    "# using autocomplete to see what we get\n",
    "auto_complete = AutoComplete()\n",
    "text = \"The future of artificial\"\n",
    "completion = auto_complete.get_completion(text)\n",
    "print(f\"Input: {text}\")\n",
    "print(f\"Completion: {completion}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2259937f-00ce-481e-83f3-ecad9c2af72b",
   "metadata": {},
   "source": [
    "Veamos qué hace el código anterior. Definió la clase *AutoComplete*, que se carga GPT2Tokenizercomo tokenizador de texto y GPT2LMHeadModelcomo un modelo GPT-2 entrenado previamente capaz de generar texto. El modelo está configurado en modo de evaluación ya que está utilizando el modelo, no para entrenarlo.\n",
    "\n",
    "La generación de texto está en la función *get_completion()*. El texto de entrada se convierte en tokens antes de pasarlo al modelo. Invoca el modelo con *torch.no_grad()* contexto para omitir el cálculo del gradiente y ahorrar tiempo y memoria. Se llama al modelo con temperature=0.7 para lograr una creatividad equilibrada. La salida del modelo debe convertirse nuevamente en texto mediante el tokenizador. Los otros parámetros en *self.model.generate()* son:\n",
    "\n",
    "* *num_return_sequences=1* para generar solo una finalización. El modelo puede generar múltiples salidas para la misma entrada.\n",
    "* *pad_token_id=self.tokenizer.eos_token_id* para evitar el relleno innecesario\n",
    "* *do_sample=True* para permitir el muestreo en lugar de la generación de texto determinista. Es necesario para la generación creativa.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09ad9bb8-a11c-4465-8475-95aadca7dece",
   "metadata": {},
   "source": [
    "## Almacenamiento en caché y entrada por lotes.\n",
    "\n",
    "El código anterior funciona como un programa simple, pero necesita algo de pulido para ejecutarlo como un servicio.\n",
    "\n",
    "Primero, implementemos un sistema de almacenamiento en caché para mejorar el rendimiento de las aplicaciones en tiempo real:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "58a9f9c0-058f-4a2f-a272-146a0f2a7a74",
   "metadata": {},
   "outputs": [],
   "source": [
    "from functools import lru_cache\n",
    "\n",
    "class CachedAutoComplete(AutoComplete):\n",
    "    def __init__(self, cache_size=1000, **kwargs):\n",
    "        \"\"\"Initialize with caching support.\"\"\"\n",
    "        super().__init__(**kwargs)\n",
    "        self.get_completion = lru_cache(maxsize=cache_size)(\n",
    "            self.get_completion\n",
    "        )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7631541-c25b-4bbb-a555-86d21687672d",
   "metadata": {},
   "source": [
    "Esto se basa en la clase anterior al decorar la función de generación con un caché LRU. La biblioteca de Python maneja el almacenamiento en caché automáticamente. Simplemente use *CachedAutoComplete* en lugar de *AutoComplete* y todo funcionará de la misma manera, excepto que el caché devolverá instantáneamente los resultados de las entradas procesadas previamente.\n",
    "\n",
    "Ahora, optimicemos aún más el sistema para lograr un mejor rendimiento en tiempo real. Uno de los desafíos de la creación de un servicio es manejar varios usuarios simultáneamente, por lo que resulta beneficioso procesar varias entradas como un lote. Sin embargo, esto aumenta el uso de memoria. Puede mitigar la carga de trabajo adicional reduciendo el tamaño del modelo mediante números flotantes de 16 bits:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0628b0ad-ce80-472a-93d2-d673674b0077",
   "metadata": {},
   "outputs": [],
   "source": [
    "class OptimizedAutoComplete(CachedAutoComplete):\n",
    "    def __init__(self, **kwargs):\n",
    "        \"\"\"Initialize with optimizations.\"\"\"\n",
    "        super().__init__(**kwargs)\n",
    "        self.tokenizer.pad_token = self.tokenizer.eos_token\n",
    "\n",
    "        if self.device == \"cuda\":\n",
    "            self.model = self.model.half()  # Use FP16 on GPU\n",
    "\n",
    "        # use eval mode and cuda graphs\n",
    "        self.model.eval()\n",
    "\n",
    "    def preprocess_batch(self, texts):\n",
    "        \"\"\"Efficiently process multiple texts.\"\"\"\n",
    "        # Tokenize all texts at once\n",
    "        inputs = self.tokenizer(texts, padding=True, truncation=True, return_tensors=\"pt\")\n",
    "        return inputs.to(self.device)\n",
    "\n",
    "    def generate_batch(self, texts, max_length=50):\n",
    "        \"\"\"Generate completions for multiple texts.\"\"\"\n",
    "        # Preprocess batch\n",
    "        inputs = self.preprocess_batch(texts)\n",
    "\n",
    "        # Generate completions\n",
    "        with torch.no_grad():\n",
    "            outputs = self.model.generate(\n",
    "                inputs['input_ids'],\n",
    "                attention_mask=inputs['attention_mask'],\n",
    "                max_length=max_length,\n",
    "                num_return_sequences=1,\n",
    "                pad_token_id=self.tokenizer.eos_token_id,\n",
    "                do_sample=True,\n",
    "                temperature=0.7\n",
    "            )\n",
    "\n",
    "        # Decode completions\n",
    "        completions = self.tokenizer.batch_decode(outputs, skip_special_tokens=True)\n",
    "\n",
    "        # Extract new text\n",
    "        results = []\n",
    "        for text, completion in zip(texts, completions):\n",
    "            results.append(completion[len(text):])\n",
    "\n",
    "        return results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18286cb8-bf65-498c-9a58-b5df3b84ddf4",
   "metadata": {},
   "source": [
    "Convertir un modelo en un valor de punto flotante de 16 bits es tan sencillo como hacerlo *self.model = self.model.half()* en el constructor. La mayoría de las CPU no admiten valores de punto flotante de 16 bits. Por lo tanto, debe hacerlo solo si puede ejecutar el modelo en una GPU. Tenga en cuenta que la función *generate_batch()* es básicamente la misma que la función  *generate()* anterior, pero debe procesar y colocar la salida por lotes en una lista.\n",
    "\n",
    "A continuación se muestra el código completo, incluido cómo utilizar la generación por lotes:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "262da4db-2014-4cbe-9975-6320328ed235",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Input: Machine learning is\n",
      "Completion:  the development of new applications that integrate with existing technology rather than develop new hardware and techniques. That is a very important distinction. As we continue to build new applications, we need to be able to develop new technologies that are less dependent\n",
      "\n",
      "Input: Deep neural networks can\n",
      "Completion:  create a network of neurons capable of processing different types of information (e.g., the information that is received by a neuron and what is received by an electric field). The neural network that generates this information can then be used to\n",
      "\n",
      "Input: The training process involves\n",
      "Completion:  three stages: the first is the initial setup, the second is the initial implementation, and the third is the final execution. The third stage consists of the execution of the training process, with the final step having been completed. The\n"
     ]
    }
   ],
   "source": [
    "from functools import lru_cache\n",
    "from transformers import GPT2LMHeadModel, GPT2Tokenizer\n",
    "import torch\n",
    "\n",
    "\n",
    "class AutoComplete:\n",
    "    def __init__(self, model_name=\"gpt2\"):\n",
    "        \"\"\"Initialize the auto-complete system.\"\"\"\n",
    "        self.tokenizer = GPT2Tokenizer.from_pretrained(model_name, padding_side=\"left\")\n",
    "        self.model = GPT2LMHeadModel.from_pretrained(model_name)\n",
    "        self.device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "        self.model.to(self.device)\n",
    "        self.model.eval()  # Set to evaluation mode\n",
    "\n",
    "    def get_completion(self, text, max_length=50):\n",
    "        \"\"\"Generate completion for the input text.\"\"\"\n",
    "        print(\"**** Completion:\", text)\n",
    "        # Encode the input text\n",
    "        inputs = self.tokenizer(text, add_special_tokens=False, return_tensors=\"pt\")\n",
    "        input_ids = inputs[\"input_ids\"].to(self.device)\n",
    "        attn_masks = inputs[\"attention_mask\"].to(self.device)\n",
    "\n",
    "        # Generate completion\n",
    "        with torch.no_grad():\n",
    "            outputs = self.model.generate(\n",
    "                input_ids,\n",
    "                attention_mask=attn_masks,\n",
    "                max_length=max_length,\n",
    "                num_return_sequences=1,\n",
    "                pad_token_id=self.tokenizer.eos_token_id,\n",
    "                do_sample=True,\n",
    "                temperature=0.7\n",
    "            )\n",
    "\n",
    "        # Decode and extract completion\n",
    "        full_text = self.tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "        completion = full_text[len(text):]\n",
    "\n",
    "        return completion\n",
    "\n",
    "\n",
    "class CachedAutoComplete(AutoComplete):\n",
    "    def __init__(self, cache_size=1000, **kwargs):\n",
    "        \"\"\"Initialize with caching support.\"\"\"\n",
    "        super().__init__(**kwargs)\n",
    "        self.get_completion = lru_cache(maxsize=cache_size)(\n",
    "            self.get_completion\n",
    "        )\n",
    "\n",
    "\n",
    "class OptimizedAutoComplete(CachedAutoComplete):\n",
    "    def __init__(self, **kwargs):\n",
    "        \"\"\"Initialize with optimizations.\"\"\"\n",
    "        super().__init__(**kwargs)\n",
    "        self.tokenizer.pad_token = self.tokenizer.eos_token\n",
    "\n",
    "        if self.device == \"cuda\":\n",
    "            self.model = self.model.half()  # Use FP16 on GPU\n",
    "\n",
    "        # use eval mode and cuda graphs\n",
    "        self.model.eval()\n",
    "\n",
    "    def preprocess_batch(self, texts):\n",
    "        \"\"\"Efficiently process multiple texts.\"\"\"\n",
    "        # Tokenize all texts at once\n",
    "        inputs = self.tokenizer(texts, padding=True, truncation=True, return_tensors=\"pt\")\n",
    "        return inputs.to(self.device)\n",
    "\n",
    "    def generate_batch(self, texts, max_length=50):\n",
    "        \"\"\"Generate completions for multiple texts.\"\"\"\n",
    "        # Preprocess batch\n",
    "        inputs = self.preprocess_batch(texts)\n",
    "\n",
    "        # Generate completions\n",
    "        with torch.no_grad():\n",
    "            outputs = self.model.generate(\n",
    "                inputs[\"input_ids\"],\n",
    "                attention_mask=inputs[\"attention_mask\"],\n",
    "                max_length=max_length,\n",
    "                num_return_sequences=1,\n",
    "                pad_token_id=self.tokenizer.eos_token_id,\n",
    "                do_sample=True,\n",
    "                temperature=0.7\n",
    "            )\n",
    "\n",
    "        # Decode completions\n",
    "        completions = self.tokenizer.batch_decode(outputs, skip_special_tokens=True)\n",
    "\n",
    "        # Extract new text\n",
    "        results = []\n",
    "        for text, completion in zip(texts, completions):\n",
    "            results.append(completion[len(text):])\n",
    "\n",
    "        return results\n",
    "\n",
    "# Example: Optimized batch completion\n",
    "optimized_complete = OptimizedAutoComplete()\n",
    "texts = [\n",
    "    \"Machine learning is\",\n",
    "    \"Deep neural networks can\",\n",
    "    \"The training process involves\"\n",
    "]\n",
    "completions = optimized_complete.generate_batch(texts)\n",
    "for text, completion in zip(texts, completions):\n",
    "    print(f\"\\nInput: {text}\")\n",
    "    print(f\"Completion: {completion}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ccdae2e-4342-449e-ac44-dd92a3d8778b",
   "metadata": {},
   "source": [
    "## Resumen\n",
    "\n",
    "En este tutorial, se ha visto cómo crear un sistema de autocompletado inteligente utilizando GPT-2. En concreto, se ha expuesto lo siguiente:\n",
    "\n",
    "* La teoría detrás de los sistemas de autocompletado neuronal\n",
    "* Cómo implementar el autocompletado básico\n",
    "* Cómo agregar almacenamiento en caché para un mejor rendimiento\n",
    "* Cómo hacer sugerencias teniendo en cuenta el contexto\n",
    "* Cómo optimizar para el uso en tiempo real"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee2abd4e-758e-4e8c-8b50-cafd177e00da",
   "metadata": {},
   "source": [
    "## Bases de datos vectoriales.\n",
    "\n",
    "🚀 "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
