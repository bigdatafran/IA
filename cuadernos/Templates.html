
<!DOCTYPE html>


<html lang="en" data-content_root="../" >

  <head>
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" /><meta name="viewport" content="width=device-width, initial-scale=1" />

    <title>2. Templates con LangChain. &#8212; IA generativa</title>
  
  
  
  <script data-cfasync="false">
    document.documentElement.dataset.mode = localStorage.getItem("mode") || "";
    document.documentElement.dataset.theme = localStorage.getItem("theme") || "";
  </script>
  <!--
    this give us a css class that will be invisible only if js is disabled
  -->
  <noscript>
    <style>
      .pst-js-only { display: none !important; }

    </style>
  </noscript>
  
  <!-- Loaded before other Sphinx assets -->
  <link href="../_static/styles/theme.css?digest=8878045cc6db502f8baf" rel="stylesheet" />
<link href="../_static/styles/pydata-sphinx-theme.css?digest=8878045cc6db502f8baf" rel="stylesheet" />

    <link rel="stylesheet" type="text/css" href="../_static/pygments.css?v=03e43079" />
    <link rel="stylesheet" type="text/css" href="../_static/styles/sphinx-book-theme.css?v=a3416100" />
    <link rel="stylesheet" type="text/css" href="../_static/togglebutton.css?v=13237357" />
    <link rel="stylesheet" type="text/css" href="../_static/copybutton.css?v=76b2166b" />
    <link rel="stylesheet" type="text/css" href="../_static/mystnb.4510f1fc1dee50b3e5859aac5469c37c29e427902b24a333a5f9fcb2f0b3ac41.css?v=be8a1c11" />
    <link rel="stylesheet" type="text/css" href="../_static/sphinx-thebe.css?v=4fa983c6" />
    <link rel="stylesheet" type="text/css" href="../_static/sphinx-design.min.css?v=95c83b7e" />
  
  <!-- So that users can add custom icons -->
  <script src="../_static/scripts/fontawesome.js?digest=8878045cc6db502f8baf"></script>
  <!-- Pre-loaded scripts that we'll load fully later -->
  <link rel="preload" as="script" href="../_static/scripts/bootstrap.js?digest=8878045cc6db502f8baf" />
<link rel="preload" as="script" href="../_static/scripts/pydata-sphinx-theme.js?digest=8878045cc6db502f8baf" />

    <script src="../_static/documentation_options.js?v=9eb32ce0"></script>
    <script src="../_static/doctools.js?v=9a2dae69"></script>
    <script src="../_static/sphinx_highlight.js?v=dc90522c"></script>
    <script src="../_static/clipboard.min.js?v=a7894cd8"></script>
    <script src="../_static/copybutton.js?v=f281be69"></script>
    <script src="../_static/scripts/sphinx-book-theme.js?v=887ef09a"></script>
    <script>let toggleHintShow = 'Click to show';</script>
    <script>let toggleHintHide = 'Click to hide';</script>
    <script>let toggleOpenOnPrint = 'true';</script>
    <script src="../_static/togglebutton.js?v=4a39c7ea"></script>
    <script>var togglebuttonSelector = '.toggle, .admonition.dropdown';</script>
    <script src="../_static/design-tabs.js?v=f930bc37"></script>
    <script>const THEBE_JS_URL = "https://unpkg.com/thebe@0.8.2/lib/index.js"; const thebe_selector = ".thebe,.cell"; const thebe_selector_input = "pre"; const thebe_selector_output = ".output, .cell_output"</script>
    <script async="async" src="../_static/sphinx-thebe.js?v=c100c467"></script>
    <script>var togglebuttonSelector = '.toggle, .admonition.dropdown';</script>
    <script>const THEBE_JS_URL = "https://unpkg.com/thebe@0.8.2/lib/index.js"; const thebe_selector = ".thebe,.cell"; const thebe_selector_input = "pre"; const thebe_selector_output = ".output, .cell_output"</script>
    <script>DOCUMENTATION_OPTIONS.pagename = 'cuadernos/Templates';</script>
    <link rel="index" title="Index" href="../genindex.html" />
    <link rel="search" title="Search" href="../search.html" />
    <link rel="next" title="3. Bases de datos en LangChain." href="basesDatosLangchain.html" />
    <link rel="prev" title="1. Introducción a LangChain" href="langchain.html" />
  <meta name="viewport" content="width=device-width, initial-scale=1"/>
  <meta name="docsearch:language" content="en"/>
  <meta name="docsearch:version" content="" />
  </head>
  
  
  <body data-bs-spy="scroll" data-bs-target=".bd-toc-nav" data-offset="180" data-bs-root-margin="0px 0px -60%" data-default-mode="">

  
  
  <div id="pst-skip-link" class="skip-link d-print-none"><a href="#main-content">Skip to main content</a></div>
  
  <div id="pst-scroll-pixel-helper"></div>
  
  <button type="button" class="btn rounded-pill" id="pst-back-to-top">
    <i class="fa-solid fa-arrow-up"></i>Back to top</button>

  
  <dialog id="pst-search-dialog">
    
<form class="bd-search d-flex align-items-center"
      action="../search.html"
      method="get">
  <i class="fa-solid fa-magnifying-glass"></i>
  <input type="search"
         class="form-control"
         name="q"
         placeholder="Search..."
         aria-label="Search..."
         autocomplete="off"
         autocorrect="off"
         autocapitalize="off"
         spellcheck="false"/>
  <span class="search-button__kbd-shortcut"><kbd class="kbd-shortcut__modifier">Ctrl</kbd>+<kbd>K</kbd></span>
</form>
  </dialog>

  <div class="pst-async-banner-revealer d-none">
  <aside id="bd-header-version-warning" class="d-none d-print-none" aria-label="Version warning"></aside>
</div>

  
    <header class="bd-header navbar navbar-expand-lg bd-navbar d-print-none">
    </header>
  

  <div class="bd-container">
    <div class="bd-container__inner bd-page-width">
      
      
      
      <dialog id="pst-primary-sidebar-modal"></dialog>
      <div id="pst-primary-sidebar" class="bd-sidebar-primary bd-sidebar">
        

  
  <div class="sidebar-header-items sidebar-primary__section">
    
    
    
    
  </div>
  
    <div class="sidebar-primary-items__start sidebar-primary__section">
        <div class="sidebar-primary-item">

  
    
  

<a class="navbar-brand logo" href="../introduccion.html">
  
  
  
  
  
  
    <p class="title logo__title">IA generativa</p>
  
</a></div>
        <div class="sidebar-primary-item">

<button class="btn search-button-field search-button__button pst-js-only" title="Search" aria-label="Search" data-bs-placement="bottom" data-bs-toggle="tooltip">
 <i class="fa-solid fa-magnifying-glass"></i>
 <span class="search-button__default-text">Search</span>
 <span class="search-button__kbd-shortcut"><kbd class="kbd-shortcut__modifier">Ctrl</kbd>+<kbd class="kbd-shortcut__modifier">K</kbd></span>
</button></div>
        <div class="sidebar-primary-item"><nav class="bd-links bd-docs-nav" aria-label="Main">
    <div class="bd-toc-item navbar-nav active">
        <p aria-level="2" class="caption" role="heading"><span class="caption-text">IA generativa</span></p>
<ul class="current nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="langchain.html">1. Langchain</a></li>
<li class="toctree-l1 current active"><a class="current reference internal" href="#">2. Templates en LangChain</a></li>
<li class="toctree-l1"><a class="reference internal" href="basesDatosLangchain.html">3. Conectores a Bases de Datos</a></li>
<li class="toctree-l1"><a class="reference internal" href="cadenas.html">4. Las cadenas de LangChain</a></li>
<li class="toctree-l1"><a class="reference internal" href="memoria.html">5. La memoria en LangChain</a></li>
<li class="toctree-l1"><a class="reference internal" href="Agenteslangchain.html">6. Los Agentes en LangChain</a></li>
<li class="toctree-l1"><a class="reference internal" href="Agentes_RAG.html">7. Ejemplo de agente RAG</a></li>
<li class="toctree-l1"><a class="reference internal" href="ollama.html">8. Ollama</a></li>
<li class="toctree-l1"><a class="reference internal" href="embeddings.html">9. Cómo hacer embeding's</a></li>
<li class="toctree-l1"><a class="reference internal" href="assistants/introAsistentes.html">10. La Api de OpenAI.</a></li>

<li class="toctree-l1"><a class="reference internal" href="conpago.html">12. Métodos de pago</a></li>
</ul>
<p aria-level="2" class="caption" role="heading"><span class="caption-text">IA generativa (Apéndices)</span></p>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="apendice.html">13. Apéndice</a></li>
<li class="toctree-l1"><a class="reference internal" href="videos.html">14. Vídeos interesantes</a></li>
<li class="toctree-l1"><a class="reference internal" href="apendices/Llama_2.html">15. Llama 2 en Colab</a></li>
<li class="toctree-l1"><a class="reference internal" href="apendices/ModelosNER.html">16. Named Entity Recognition(NER)</a></li>
<li class="toctree-l1"><a class="reference internal" href="apendices/GeneracionTexto.html">17. Auto-Completado de texto</a></li>
<li class="toctree-l1"><a class="reference internal" href="apendices/gradio.html">18. Gradio</a></li>
</ul>
<p aria-level="2" class="caption" role="heading"><span class="caption-text">Casos de uso</span></p>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="casosUso/EjemploLangGraph.html">19. Ejemplo con LangGraph</a></li>
</ul>
<p aria-level="2" class="caption" role="heading"><span class="caption-text">Índice de términos</span></p>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="../genindex.html">Índice de términos</a></li>
</ul>

    </div>
</nav></div>
    </div>
  
  
  <div class="sidebar-primary-items__end sidebar-primary__section">
      <div class="sidebar-primary-item">
<div id="ethical-ad-placement"
      class="flat"
      data-ea-publisher="readthedocs"
      data-ea-type="readthedocs-sidebar"
      data-ea-manual="true">
</div></div>
  </div>


      </div>
      
      <main id="main-content" class="bd-main" role="main">
        
        

<div class="sbt-scroll-pixel-helper"></div>

          <div class="bd-content">
            <div class="bd-article-container">
              
              <div class="bd-header-article d-print-none">
<div class="header-article-items header-article__inner">
  
    <div class="header-article-items__start">
      
        <div class="header-article-item"><button class="sidebar-toggle primary-toggle btn btn-sm" title="Toggle primary sidebar" data-bs-placement="bottom" data-bs-toggle="tooltip">
  <span class="fa-solid fa-bars"></span>
</button></div>
      
    </div>
  
  
    <div class="header-article-items__end">
      
        <div class="header-article-item">

<div class="article-header-buttons">



<button onclick="toggleFullScreen()"
  class="btn btn-sm btn-fullscreen-button"
  title="Fullscreen mode"
  data-bs-placement="bottom" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-expand"></i>
  </span>

</button>



<button class="btn btn-sm nav-link pst-navbar-icon theme-switch-button pst-js-only" aria-label="Color mode" data-bs-title="Color mode"  data-bs-placement="bottom" data-bs-toggle="tooltip">
  <i class="theme-switch fa-solid fa-sun                fa-lg" data-mode="light" title="Light"></i>
  <i class="theme-switch fa-solid fa-moon               fa-lg" data-mode="dark"  title="Dark"></i>
  <i class="theme-switch fa-solid fa-circle-half-stroke fa-lg" data-mode="auto"  title="System Settings"></i>
</button>


<button class="btn btn-sm pst-navbar-icon search-button search-button__button pst-js-only" title="Search" aria-label="Search" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <i class="fa-solid fa-magnifying-glass fa-lg"></i>
</button>
<button class="sidebar-toggle secondary-toggle btn btn-sm" title="Toggle secondary sidebar" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <span class="fa-solid fa-list"></span>
</button>
</div></div>
      
    </div>
  
</div>
</div>
              
              

<div id="jb-print-docs-body" class="onlyprint">
    <h1>Templates con LangChain.</h1>
    <!-- Table of contents -->
    <div id="print-main-content">
        <div id="jb-print-toc">
            
            <div>
                <h2> Contents </h2>
            </div>
            <nav aria-label="Page">
                <ul class="visible nav section-nav flex-column">
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#usos-de-prompttemplate">2.1. Usos de PromptTemplate.</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#usos-de-chatprompttemplate">2.2. Usos de ChatPromptTemplate.</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#diferencia-entre-prompttemplate-y-chatprompttemplate">2.3. Diferencia entre PromptTemplate y ChatPromptTemplate.</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#ejemplo-basico-de-prompttemplate">2.4. Ejemplo Básico de PromptTemplate.</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#codigo-de-ejemplo">2.4.1. Código de Ejemplo.</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#ejemplo-basico-de-chatprompttemplate">2.5. Ejemplo Básico de ChatPromptTemplate.</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#id1">2.5.1. Código de Ejemplo.</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#ejemplo-complejo-de-chatprompttemplate">2.5.2. Ejemplo Complejo de ChatPromptTemplate.</a></li>
</ul>
</li>
</ul>
            </nav>
        </div>
    </div>
</div>

              
                
<div id="searchbox"></div>
                <article class="bd-article">
                  
  <section class="tex2jax_ignore mathjax_ignore" id="templates-con-langchain">
<h1><span class="section-number">2. </span>Templates con LangChain.<a class="headerlink" href="#templates-con-langchain" title="Link to this heading">#</a></h1>
<p>Los templates en LangChain, permiten crear plantillas con variables, muy útiles para generar diferentes entornos de AI. Vamos a distinguir don tipos de templates: PromptTemplate y ChatPromptTemplate.</p>
<section id="usos-de-prompttemplate">
<h2><span class="section-number">2.1. </span>Usos de PromptTemplate.<a class="headerlink" href="#usos-de-prompttemplate" title="Link to this heading">#</a></h2>
<p id="index-0">PromptTemplate se utiliza para crear plantillas de prompts simples y lineales en LangChain, permitiendo formatear entradas de texto con variables para generar instrucciones directas para modelos de lenguaje. Es ideal para tareas básicas donde se necesita claridad y facilidad de uso, como generar respuestas basadas en una sola cadena de texto sin contexto conversacional complejo. Por ejemplo, se puede definir una plantilla como “Dime un chiste sobre {tema}” y reemplazar {tema} con un valor específico para invocar un modelo.</p>
</section>
<section id="usos-de-chatprompttemplate">
<h2><span class="section-number">2.2. </span>Usos de ChatPromptTemplate.<a class="headerlink" href="#usos-de-chatprompttemplate" title="Link to this heading">#</a></h2>
<p id="index-1">ChatPromptTemplate se emplea para construir prompts en formato de chat, manejando listas de mensajes con roles específicos como sistema, humano o AI, lo que facilita interacciones dinámicas y conversacionales. Es útil en aplicaciones que requieren mantener contexto a lo largo de múltiples intercambios, como chatbots o diálogos continuos. Por instancia, permite separar mensajes del sistema (instrucciones generales) de los del usuario, mejorando la estructura para modelos de chat como GPT.</p>
</section>
<section id="diferencia-entre-prompttemplate-y-chatprompttemplate">
<h2><span class="section-number">2.3. </span>Diferencia entre PromptTemplate y ChatPromptTemplate.<a class="headerlink" href="#diferencia-entre-prompttemplate-y-chatprompttemplate" title="Link to this heading">#</a></h2>
<p>La principal diferencia radica en su estructura: PromptTemplate genera una sola cadena de texto formateada, adecuada para prompts lineales y simples, mientras que ChatPromptTemplate crea una lista de mensajes con roles, optimizada para conversaciones multi-turno y modelos de chat. PromptTemplate es más directo y menos flexible para diálogos complejos, pero requiere menos configuración; en cambio, ChatPromptTemplate ofrece mayor capacidad para retener contexto, aunque puede ser más complejo de implementar. En resumen, elige PromptTemplate para tareas no conversacionales y ChatPromptTemplate para flujos interactivos.</p>
</section>
<section id="ejemplo-basico-de-prompttemplate">
<h2><span class="section-number">2.4. </span>Ejemplo Básico de PromptTemplate.<a class="headerlink" href="#ejemplo-basico-de-prompttemplate" title="Link to this heading">#</a></h2>
<p>Un ejemplo común de uso de PromptTemplate en LangChain es crear una plantilla para generar sugerencias de nombres de empresas basadas en un producto, donde se define una variable de entrada para personalizar el prompt antes de enviarlo a un modelo de lenguaje. Esto permite formatear dinámicamente el texto, reemplazando placeholders como {product} con valores específicos para producir instrucciones claras y reutilizables.​</p>
<section id="codigo-de-ejemplo">
<h3><span class="section-number">2.4.1. </span>Código de Ejemplo.<a class="headerlink" href="#codigo-de-ejemplo" title="Link to this heading">#</a></h3>
<p>Aquí va un snippet de código en Python que ilustra cómo implementar y formatear un PromptTemplate simple:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span><span class="w"> </span><span class="nn">langchain</span><span class="w"> </span><span class="kn">import</span> <span class="n">PromptTemplate</span>

<span class="c1"># Definir la plantilla con una variable de entrada</span>
<span class="n">template</span> <span class="o">=</span> <span class="s2">&quot;&quot;&quot;</span>
<span class="s2">Actúa como consultor de nombres para nuevas empresas.</span>
<span class="s2">¿Qué nombre sería bueno para una empresa que fabrica </span><span class="si">{producto}</span><span class="s2">?</span>
<span class="s2">&quot;&quot;&quot;</span>

<span class="n">prompt</span> <span class="o">=</span> <span class="n">PromptTemplate</span><span class="p">(</span>
    <span class="n">input_variables</span><span class="o">=</span><span class="p">[</span><span class="s2">&quot;producto&quot;</span><span class="p">],</span>
    <span class="n">template</span><span class="o">=</span><span class="n">template</span>
<span class="p">)</span>

<span class="c1"># Formatear el prompt con un valor específico</span>
<span class="n">prompt_formateado</span> <span class="o">=</span> <span class="n">prompt</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">producto</span><span class="o">=</span><span class="s2">&quot;calcetines coloridos&quot;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="n">prompt_formateado</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stderr highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>D:\MisTrabajos\IA_generativa\venv\Lib\site-packages\tqdm\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html
  from .autonotebook import tqdm as notebook_tqdm
</pre></div>
</div>
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Actúa como consultor de nombres para nuevas empresas.
¿Qué nombre sería bueno para una empresa que fabrica calcetines coloridos?
</pre></div>
</div>
</div>
</div>
<p>Otra forma de hacer lo siguiente sería de esta manera:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># Definir la plantilla con una variable de entrada</span>
<span class="n">template</span> <span class="o">=</span> <span class="s2">&quot;&quot;&quot;</span>
<span class="s2">Actúa como consultor de nombres para nuevas empresas.</span>
<span class="s2">¿Qué nombre sería bueno para una empresa que fabrica </span><span class="si">{producto}</span><span class="s2">?</span>
<span class="s2">&quot;&quot;&quot;</span>

<span class="n">prompt</span> <span class="o">=</span> <span class="n">PromptTemplate</span><span class="o">.</span><span class="n">from_template</span><span class="p">(</span>
    <span class="n">template</span><span class="o">=</span><span class="n">template</span>
<span class="p">)</span>

<span class="c1"># Formatear el prompt con un valor específico</span>
<span class="n">prompt_formateado</span> <span class="o">=</span> <span class="n">prompt</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">producto</span><span class="o">=</span><span class="s2">&quot;calcetines coloridos&quot;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="n">prompt_formateado</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Actúa como consultor de nombres para nuevas empresas.
¿Qué nombre sería bueno para una empresa que fabrica calcetines coloridos?
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">Ahora</span> <span class="n">lo</span> <span class="n">utilizariamos</span> <span class="n">de</span> <span class="n">la</span> <span class="n">siguiente</span> <span class="n">manera</span><span class="o">.</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span><span class="w"> </span><span class="nn">os</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">dotenv</span><span class="w"> </span><span class="kn">import</span> <span class="n">load_dotenv</span>

<span class="n">load_dotenv</span><span class="p">()</span>

<span class="kn">from</span><span class="w"> </span><span class="nn">langchain_openai</span><span class="w"> </span><span class="kn">import</span> <span class="n">OpenAI</span>


<span class="n">llm</span> <span class="o">=</span> <span class="n">OpenAI</span><span class="p">(</span><span class="n">model</span><span class="o">=</span><span class="s2">&quot;gpt-3.5-turbo-instruct&quot;</span><span class="p">,</span> <span class="n">temperature</span><span class="o">=</span><span class="mf">0.7</span><span class="p">,</span><span class="n">api_key</span><span class="o">=</span><span class="n">os</span><span class="o">.</span><span class="n">getenv</span><span class="p">(</span><span class="s2">&quot;OPENAI_API_KEY&quot;</span><span class="p">)</span> <span class="p">)</span>  <span class="c1"># Usa modelo de completación</span>
<span class="n">resumen</span> <span class="o">=</span> <span class="n">llm</span><span class="o">.</span><span class="n">invoke</span><span class="p">(</span><span class="n">prompt_formateado</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="n">resumen</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>&quot;RainbowSock Co.&quot;
</pre></div>
</div>
</div>
</div>
<p>Otra forma utilizando LLMChain</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span><span class="w"> </span><span class="nn">langchain.chains</span><span class="w"> </span><span class="kn">import</span> <span class="n">LLMChain</span>

<span class="n">llm</span> <span class="o">=</span> <span class="n">OpenAI</span><span class="p">(</span><span class="n">model</span><span class="o">=</span><span class="s2">&quot;gpt-3.5-turbo-instruct&quot;</span><span class="p">,</span> <span class="n">temperature</span><span class="o">=</span><span class="mf">0.7</span><span class="p">,</span><span class="n">api_key</span><span class="o">=</span><span class="n">os</span><span class="o">.</span><span class="n">getenv</span><span class="p">(</span><span class="s2">&quot;OPENAI_API_KEY&quot;</span><span class="p">)</span> <span class="p">)</span>
<span class="n">chain</span> <span class="o">=</span> <span class="n">LLMChain</span><span class="p">(</span><span class="n">llm</span><span class="o">=</span><span class="n">llm</span><span class="p">,</span> <span class="n">prompt</span><span class="o">=</span><span class="n">prompt</span><span class="p">)</span>

<span class="n">respuesta</span> <span class="o">=</span> <span class="n">chain</span><span class="o">.</span><span class="n">invoke</span><span class="p">(</span><span class="nb">input</span><span class="o">=</span><span class="p">{</span><span class="s2">&quot;producto&quot;</span><span class="p">:</span><span class="s2">&quot;calcetines coloridos&quot;</span><span class="p">})</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="nb">print</span><span class="p">(</span><span class="n">respuesta</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;-----------------------&quot;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="n">respuesta</span><span class="p">[</span><span class="s2">&quot;text&quot;</span><span class="p">])</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>{&#39;producto&#39;: &#39;calcetines coloridos&#39;, &#39;text&#39;: &#39;\n&quot;Rainbow Socks Co.&quot; o &quot;Vibrant Footwear Inc.&quot;&#39;}
-----------------------

&quot;Rainbow Socks Co.&quot; o &quot;Vibrant Footwear Inc.&quot;
</pre></div>
</div>
</div>
</div>
</section>
</section>
<section id="ejemplo-basico-de-chatprompttemplate">
<h2><span class="section-number">2.5. </span>Ejemplo Básico de ChatPromptTemplate.<a class="headerlink" href="#ejemplo-basico-de-chatprompttemplate" title="Link to this heading">#</a></h2>
<p>Un ejemplo típico de ChatPromptTemplate en LangChain es construir una plantilla para un tutor que explica conceptos simples, utilizando mensajes con roles como sistema y humano para estructurar una interacción conversacional dinámica. Esto permite formatear una lista de mensajes con variables personalizables, ideal para modelos de chat que manejan contextos multi-turno.​</p>
<section id="id1">
<h3><span class="section-number">2.5.1. </span>Código de Ejemplo.<a class="headerlink" href="#id1" title="Link to this heading">#</a></h3>
<p>Aquí un snippet en Python que muestra cómo definir y formatear un ChatPromptTemplate:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span><span class="w"> </span><span class="nn">langchain_core.prompts</span><span class="w"> </span><span class="kn">import</span> <span class="n">ChatPromptTemplate</span>

<span class="c1"># Definir la plantilla con mensajes de sistema y humano</span>
<span class="n">chat_prompt</span> <span class="o">=</span> <span class="n">ChatPromptTemplate</span><span class="o">.</span><span class="n">from_messages</span><span class="p">([</span>
    <span class="p">(</span><span class="s2">&quot;system&quot;</span><span class="p">,</span> <span class="s2">&quot;Eres un tutor paciente que explica las cosas de manera clara.&quot;</span><span class="p">),</span>
    <span class="p">(</span><span class="s2">&quot;human&quot;</span><span class="p">,</span> <span class="s2">&quot;¿Puedes explicar </span><span class="si">{concepto}</span><span class="s2"> como si yo tuviera cinco años?&quot;</span><span class="p">)</span>
<span class="p">])</span>

<span class="c1"># Formatear los mensajes con un valor específico</span>
<span class="n">mensajes_formateados</span> <span class="o">=</span> <span class="n">chat_prompt</span><span class="o">.</span><span class="n">format_messages</span><span class="p">(</span><span class="n">concepto</span><span class="o">=</span><span class="s2">&quot;gravedad&quot;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="n">mensajes_formateados</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>[SystemMessage(content=&#39;Eres un tutor paciente que explica las cosas de manera clara.&#39;, additional_kwargs={}, response_metadata={}), HumanMessage(content=&#39;¿Puedes explicar gravedad como si yo tuviera cinco años?&#39;, additional_kwargs={}, response_metadata={})]
</pre></div>
</div>
</div>
</div>
<p>Este código produce una lista de mensajes: el primero como SystemMessage con las instrucciones del sistema, y el segundo como HumanMessage con la pregunta personalizada sobre “gravedad”. Puedes conectar este template a un modelo de chat como GPT para generar respuestas interactivas en una cadena.</p>
<p>Si queremos ejecutar el ejemplo anterior con ChatOpenAI, lo podemos hacer de la siguiente manera</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># Instanciar el modelo ChatOpenAI (ajusta el modelo y temperatura según necesites)</span>
<span class="n">llm</span> <span class="o">=</span> <span class="n">ChatOpenAI</span><span class="p">(</span>
    <span class="n">model</span><span class="o">=</span><span class="s2">&quot;gpt-3.5-turbo&quot;</span><span class="p">,</span>
    <span class="n">temperature</span><span class="o">=</span><span class="mf">0.7</span>  <span class="c1"># Controla la creatividad; 0 para más determinístico</span>
<span class="p">)</span>

<span class="c1"># Invocar el modelo con los mensajes formateados</span>
<span class="n">respuesta</span> <span class="o">=</span> <span class="n">llm</span><span class="o">.</span><span class="n">invoke</span><span class="p">(</span><span class="n">mensajes_formateados</span><span class="p">)</span>

<span class="c1"># Imprimir los mensajes formateados y la respuesta del modelo</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Mensajes formateados:&quot;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="n">mensajes_formateados</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;</span><span class="se">\n</span><span class="s2">Respuesta del modelo:&quot;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="n">respuesta</span><span class="o">.</span><span class="n">content</span><span class="p">)</span>  <span class="c1"># .content extrae el texto de la AIMessage</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Mensajes formateados:
[SystemMessage(content=&#39;Eres un asistente que resume conversaciones en español.&#39;, additional_kwargs={}, response_metadata={}), HumanMessage(content=&#39;Hola, ¿qué es la IA?&#39;, additional_kwargs={}, response_metadata={}), AIMessage(content=&#39;La IA es inteligencia artificial que simula procesos humanos.&#39;, additional_kwargs={}, response_metadata={}), HumanMessage(content=&#39;Explícame más sobre machine learning.&#39;, additional_kwargs={}, response_metadata={}), HumanMessage(content=&#39;Resume nuestra conversación en 50 palabras.\nReturn a JSON object.&#39;, additional_kwargs={}, response_metadata={})]

Respuesta del modelo:
{
  &quot;resumen&quot;: &quot;La IA es inteligencia artificial que simula procesos humanos. Machine learning es una rama de la IA que se centra en desarrollar algoritmos que permiten a las máquinas aprender a partir de datos y mejorar con la experiencia.&quot;
}
</pre></div>
</div>
</div>
</div>
</section>
<section id="ejemplo-complejo-de-chatprompttemplate">
<h3><span class="section-number">2.5.2. </span>Ejemplo Complejo de ChatPromptTemplate.<a class="headerlink" href="#ejemplo-complejo-de-chatprompttemplate" title="Link to this heading">#</a></h3>
<p>Un ejemplo más complejo de ChatPromptTemplate en LangChain involucra la creación de una plantilla para un asistente de traducción que maneja mensajes de sistema, humano y placeholders para historial de conversación, permitiendo interacciones multi-turno con variables parciales y parsers de salida. Esto es útil para aplicaciones conversacionales avanzadas, como chatbots que mantienen contexto y formatean respuestas en JSON para procesamiento estructurado. La plantilla se puede encadenar con un LLM y un parser para automatizar flujos dinámicos, reduciendo la necesidad de prompts manuales en cada turno</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span><span class="w"> </span><span class="nn">langchain_core.prompts</span><span class="w"> </span><span class="kn">import</span> <span class="n">ChatPromptTemplate</span><span class="p">,</span> <span class="n">MessagesPlaceholder</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">langchain_core.output_parsers</span><span class="w"> </span><span class="kn">import</span> <span class="n">JsonOutputParser</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">langchain_core.messages</span><span class="w"> </span><span class="kn">import</span> <span class="n">SystemMessage</span><span class="p">,</span> <span class="n">HumanMessage</span><span class="p">,</span> <span class="n">AIMessage</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">langchain_openai</span><span class="w"> </span><span class="kn">import</span> <span class="n">ChatOpenAI</span>  <span class="c1"># Asumiendo un LLM como OpenAI</span>

<span class="c1"># Parser para salida JSON</span>
<span class="n">json_parser</span> <span class="o">=</span> <span class="n">JsonOutputParser</span><span class="p">()</span>

<span class="c1"># Plantilla parcial con instrucciones de formato</span>
<span class="n">prompt_template</span> <span class="o">=</span> <span class="n">ChatPromptTemplate</span><span class="o">.</span><span class="n">from_messages</span><span class="p">([</span>
    <span class="n">SystemMessage</span><span class="p">(</span><span class="n">content</span><span class="o">=</span><span class="s2">&quot;Eres un asistente que resume conversaciones en español.&quot;</span><span class="p">),</span>
    <span class="n">MessagesPlaceholder</span><span class="p">(</span><span class="n">variable_name</span><span class="o">=</span><span class="s2">&quot;conversation&quot;</span><span class="p">),</span>  <span class="c1"># Placeholder para historial</span>
    <span class="p">(</span><span class="s2">&quot;human&quot;</span><span class="p">,</span> <span class="s2">&quot;Resume nuestra conversación en </span><span class="si">{word_count}</span><span class="s2"> palabras.</span><span class="se">\n</span><span class="si">{format_instructions}</span><span class="s2">&quot;</span><span class="p">)</span>
<span class="p">])</span>

<span class="c1"># Variables parciales para el parser</span>
<span class="n">prompt_template_partial</span> <span class="o">=</span> <span class="n">prompt_template</span><span class="o">.</span><span class="n">partial</span><span class="p">(</span>
    <span class="n">format_instructions</span><span class="o">=</span><span class="n">json_parser</span><span class="o">.</span><span class="n">get_format_instructions</span><span class="p">(),</span>
    <span class="n">word_count</span><span class="o">=</span><span class="mi">50</span>
<span class="p">)</span>

<span class="c1"># Ejemplo de historial de conversación</span>
<span class="n">conversation</span> <span class="o">=</span> <span class="p">[</span>
    <span class="n">HumanMessage</span><span class="p">(</span><span class="n">content</span><span class="o">=</span><span class="s2">&quot;Hola, ¿qué es la IA?&quot;</span><span class="p">),</span>
    <span class="n">AIMessage</span><span class="p">(</span><span class="n">content</span><span class="o">=</span><span class="s2">&quot;La IA es inteligencia artificial que simula procesos humanos.&quot;</span><span class="p">),</span>
    <span class="n">HumanMessage</span><span class="p">(</span><span class="n">content</span><span class="o">=</span><span class="s2">&quot;Explícame más sobre machine learning.&quot;</span><span class="p">)</span>
<span class="p">]</span>

<span class="c1"># Formatear con el historial</span>
<span class="n">mensajes_formateados</span> <span class="o">=</span> <span class="n">prompt_template_partial</span><span class="o">.</span><span class="n">format_messages</span><span class="p">(</span><span class="n">conversation</span><span class="o">=</span><span class="n">conversation</span><span class="p">)</span>

<span class="c1"># Encadenar con LLM (ejemplo)</span>
<span class="n">llm</span> <span class="o">=</span> <span class="n">ChatOpenAI</span><span class="p">(</span><span class="n">model</span><span class="o">=</span><span class="s2">&quot;gpt-3.5-turbo&quot;</span><span class="p">)</span>
<span class="n">chain</span> <span class="o">=</span> <span class="n">prompt_template_partial</span> <span class="o">|</span> <span class="n">llm</span> <span class="o">|</span> <span class="n">json_parser</span>
<span class="n">resumen</span> <span class="o">=</span> <span class="n">chain</span><span class="o">.</span><span class="n">invoke</span><span class="p">({</span><span class="s2">&quot;conversation&quot;</span><span class="p">:</span> <span class="n">conversation</span><span class="p">})</span>
<span class="nb">print</span><span class="p">(</span><span class="n">resumen</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>{&#39;conversacion&#39;: &#39;Hablamos sobre IA, que es inteligencia artificial que simula procesos humanos. También discutimos sobre machine learning, que es una rama de la IA donde la máquina aprende de datos para tomar decisiones sin ser programada específicamente.&#39;, &#39;palabras&#39;: 50}
</pre></div>
</div>
</div>
</div>
<p>Este código genera una lista de mensajes que incluye el mensaje de sistema, el historial de conversación insertado en el placeholder, y la consulta humana con variables parciales para el conteo de palabras y formato JSON. Al invocar la cadena, el LLM produce un resumen estructurado en JSON, como {“resumen”: “La conversación introduce IA y machine learning…”} con aproximadamente 50 palabras, facilitando el manejo de contextos complejos en aplicaciones reales.</p>
</section>
</section>
</section>

    <script type="text/x-thebe-config">
    {
        requestKernel: true,
        binderOptions: {
            repo: "binder-examples/jupyter-stacks-datascience",
            ref: "master",
        },
        codeMirrorConfig: {
            theme: "abcdef",
            mode: "python"
        },
        kernelOptions: {
            name: "python3",
            path: "./cuadernos"
        },
        predefinedOutput: true
    }
    </script>
    <script>kernelName = 'python3'</script>

                </article>
              

              
              
              
              
                <footer class="prev-next-footer d-print-none">
                  
<div class="prev-next-area">
    <a class="left-prev"
       href="langchain.html"
       title="previous page">
      <i class="fa-solid fa-angle-left"></i>
      <div class="prev-next-info">
        <p class="prev-next-subtitle">previous</p>
        <p class="prev-next-title"><span class="section-number">1. </span>Introducción a LangChain</p>
      </div>
    </a>
    <a class="right-next"
       href="basesDatosLangchain.html"
       title="next page">
      <div class="prev-next-info">
        <p class="prev-next-subtitle">next</p>
        <p class="prev-next-title"><span class="section-number">3. </span>Bases de datos en LangChain.</p>
      </div>
      <i class="fa-solid fa-angle-right"></i>
    </a>
</div>
                </footer>
              
            </div>
            
            
              
                <dialog id="pst-secondary-sidebar-modal"></dialog>
                <div id="pst-secondary-sidebar" class="bd-sidebar-secondary bd-toc"><div class="sidebar-secondary-items sidebar-secondary__inner">


  <div class="sidebar-secondary-item">
  <div class="page-toc tocsection onthispage">
    <i class="fa-solid fa-list"></i> Contents
  </div>
  <nav class="bd-toc-nav page-toc">
    <ul class="visible nav section-nav flex-column">
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#usos-de-prompttemplate">2.1. Usos de PromptTemplate.</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#usos-de-chatprompttemplate">2.2. Usos de ChatPromptTemplate.</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#diferencia-entre-prompttemplate-y-chatprompttemplate">2.3. Diferencia entre PromptTemplate y ChatPromptTemplate.</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#ejemplo-basico-de-prompttemplate">2.4. Ejemplo Básico de PromptTemplate.</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#codigo-de-ejemplo">2.4.1. Código de Ejemplo.</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#ejemplo-basico-de-chatprompttemplate">2.5. Ejemplo Básico de ChatPromptTemplate.</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#id1">2.5.1. Código de Ejemplo.</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#ejemplo-complejo-de-chatprompttemplate">2.5.2. Ejemplo Complejo de ChatPromptTemplate.</a></li>
</ul>
</li>
</ul>
  </nav></div>

</div></div>
              
            
          </div>
          <footer class="bd-footer-content">
            
<div class="bd-footer-content__inner container">
  
  <div class="footer-item">
    
<p class="component-author">
By Francisco Rodríguez
</p>

  </div>
  
  <div class="footer-item">
    

  <p class="copyright">
    
      © Copyright 2023.
      <br/>
    
  </p>

  </div>
  
  <div class="footer-item">
    
  </div>
  
  <div class="footer-item">
    
  </div>
  
</div>
          </footer>
        

      </main>
    </div>
  </div>
  
  <!-- Scripts loaded after <body> so the DOM is not blocked -->
  <script defer src="../_static/scripts/bootstrap.js?digest=8878045cc6db502f8baf"></script>
<script defer src="../_static/scripts/pydata-sphinx-theme.js?digest=8878045cc6db502f8baf"></script>

  <footer class="bd-footer">
  </footer>
  </body>
</html>