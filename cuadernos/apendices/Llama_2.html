
<!DOCTYPE html>


<html lang="en" data-content_root="../../" >

  <head>
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" /><meta name="viewport" content="width=device-width, initial-scale=1" />

    <title>15. Llama 2 &#8212; IA generativa</title>
  
  
  
  <script data-cfasync="false">
    document.documentElement.dataset.mode = localStorage.getItem("mode") || "";
    document.documentElement.dataset.theme = localStorage.getItem("theme") || "";
  </script>
  <!--
    this give us a css class that will be invisible only if js is disabled
  -->
  <noscript>
    <style>
      .pst-js-only { display: none !important; }

    </style>
  </noscript>
  
  <!-- Loaded before other Sphinx assets -->
  <link href="../../_static/styles/theme.css?digest=8878045cc6db502f8baf" rel="stylesheet" />
<link href="../../_static/styles/pydata-sphinx-theme.css?digest=8878045cc6db502f8baf" rel="stylesheet" />

    <link rel="stylesheet" type="text/css" href="../../_static/pygments.css?v=03e43079" />
    <link rel="stylesheet" type="text/css" href="../../_static/styles/sphinx-book-theme.css?v=a3416100" />
    <link rel="stylesheet" type="text/css" href="../../_static/togglebutton.css?v=13237357" />
    <link rel="stylesheet" type="text/css" href="../../_static/copybutton.css?v=76b2166b" />
    <link rel="stylesheet" type="text/css" href="../../_static/mystnb.4510f1fc1dee50b3e5859aac5469c37c29e427902b24a333a5f9fcb2f0b3ac41.css?v=be8a1c11" />
    <link rel="stylesheet" type="text/css" href="../../_static/sphinx-thebe.css?v=4fa983c6" />
    <link rel="stylesheet" type="text/css" href="../../_static/sphinx-design.min.css?v=95c83b7e" />
  
  <!-- So that users can add custom icons -->
  <script src="../../_static/scripts/fontawesome.js?digest=8878045cc6db502f8baf"></script>
  <!-- Pre-loaded scripts that we'll load fully later -->
  <link rel="preload" as="script" href="../../_static/scripts/bootstrap.js?digest=8878045cc6db502f8baf" />
<link rel="preload" as="script" href="../../_static/scripts/pydata-sphinx-theme.js?digest=8878045cc6db502f8baf" />

    <script src="../../_static/documentation_options.js?v=9eb32ce0"></script>
    <script src="../../_static/doctools.js?v=9a2dae69"></script>
    <script src="../../_static/sphinx_highlight.js?v=dc90522c"></script>
    <script src="../../_static/clipboard.min.js?v=a7894cd8"></script>
    <script src="../../_static/copybutton.js?v=f281be69"></script>
    <script src="../../_static/scripts/sphinx-book-theme.js?v=887ef09a"></script>
    <script>let toggleHintShow = 'Click to show';</script>
    <script>let toggleHintHide = 'Click to hide';</script>
    <script>let toggleOpenOnPrint = 'true';</script>
    <script src="../../_static/togglebutton.js?v=4a39c7ea"></script>
    <script>var togglebuttonSelector = '.toggle, .admonition.dropdown';</script>
    <script src="../../_static/design-tabs.js?v=f930bc37"></script>
    <script>const THEBE_JS_URL = "https://unpkg.com/thebe@0.8.2/lib/index.js"; const thebe_selector = ".thebe,.cell"; const thebe_selector_input = "pre"; const thebe_selector_output = ".output, .cell_output"</script>
    <script async="async" src="../../_static/sphinx-thebe.js?v=c100c467"></script>
    <script>var togglebuttonSelector = '.toggle, .admonition.dropdown';</script>
    <script>const THEBE_JS_URL = "https://unpkg.com/thebe@0.8.2/lib/index.js"; const thebe_selector = ".thebe,.cell"; const thebe_selector_input = "pre"; const thebe_selector_output = ".output, .cell_output"</script>
    <script>DOCUMENTATION_OPTIONS.pagename = 'cuadernos/apendices/Llama_2';</script>
    <link rel="index" title="Index" href="../../genindex.html" />
    <link rel="search" title="Search" href="../../search.html" />
    <link rel="next" title="16. Introducción a los modelos NER (Named Entity Recognition)" href="ModelosNER.html" />
    <link rel="prev" title="14. Videos sobre IA." href="../videos.html" />
  <meta name="viewport" content="width=device-width, initial-scale=1"/>
  <meta name="docsearch:language" content="en"/>
  <meta name="docsearch:version" content="" />
  </head>
  
  
  <body data-bs-spy="scroll" data-bs-target=".bd-toc-nav" data-offset="180" data-bs-root-margin="0px 0px -60%" data-default-mode="">

  
  
  <div id="pst-skip-link" class="skip-link d-print-none"><a href="#main-content">Skip to main content</a></div>
  
  <div id="pst-scroll-pixel-helper"></div>
  
  <button type="button" class="btn rounded-pill" id="pst-back-to-top">
    <i class="fa-solid fa-arrow-up"></i>Back to top</button>

  
  <dialog id="pst-search-dialog">
    
<form class="bd-search d-flex align-items-center"
      action="../../search.html"
      method="get">
  <i class="fa-solid fa-magnifying-glass"></i>
  <input type="search"
         class="form-control"
         name="q"
         placeholder="Search..."
         aria-label="Search..."
         autocomplete="off"
         autocorrect="off"
         autocapitalize="off"
         spellcheck="false"/>
  <span class="search-button__kbd-shortcut"><kbd class="kbd-shortcut__modifier">Ctrl</kbd>+<kbd>K</kbd></span>
</form>
  </dialog>

  <div class="pst-async-banner-revealer d-none">
  <aside id="bd-header-version-warning" class="d-none d-print-none" aria-label="Version warning"></aside>
</div>

  
    <header class="bd-header navbar navbar-expand-lg bd-navbar d-print-none">
    </header>
  

  <div class="bd-container">
    <div class="bd-container__inner bd-page-width">
      
      
      
      <dialog id="pst-primary-sidebar-modal"></dialog>
      <div id="pst-primary-sidebar" class="bd-sidebar-primary bd-sidebar">
        

  
  <div class="sidebar-header-items sidebar-primary__section">
    
    
    
    
  </div>
  
    <div class="sidebar-primary-items__start sidebar-primary__section">
        <div class="sidebar-primary-item">

  
    
  

<a class="navbar-brand logo" href="../../introduccion.html">
  
  
  
  
  
  
    <p class="title logo__title">IA generativa</p>
  
</a></div>
        <div class="sidebar-primary-item">

<button class="btn search-button-field search-button__button pst-js-only" title="Search" aria-label="Search" data-bs-placement="bottom" data-bs-toggle="tooltip">
 <i class="fa-solid fa-magnifying-glass"></i>
 <span class="search-button__default-text">Search</span>
 <span class="search-button__kbd-shortcut"><kbd class="kbd-shortcut__modifier">Ctrl</kbd>+<kbd class="kbd-shortcut__modifier">K</kbd></span>
</button></div>
        <div class="sidebar-primary-item"><nav class="bd-links bd-docs-nav" aria-label="Main">
    <div class="bd-toc-item navbar-nav active">
        <p aria-level="2" class="caption" role="heading"><span class="caption-text">IA generativa</span></p>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="../langchain.html">1. Langchain</a></li>
<li class="toctree-l1"><a class="reference internal" href="../Templates.html">2. Templates en LangChain</a></li>
<li class="toctree-l1"><a class="reference internal" href="../basesDatosLangchain.html">3. Conectores a Bases de Datos</a></li>
<li class="toctree-l1"><a class="reference internal" href="../cadenas.html">4. Las cadenas de LangChain</a></li>
<li class="toctree-l1"><a class="reference internal" href="../memoria.html">5. La memoria en LangChain</a></li>
<li class="toctree-l1"><a class="reference internal" href="../Agenteslangchain.html">6. Los Agentes en LangChain</a></li>
<li class="toctree-l1"><a class="reference internal" href="../Agentes_RAG.html">7. Ejemplo de agente RAG</a></li>
<li class="toctree-l1"><a class="reference internal" href="../ollama.html">8. Ollama</a></li>
<li class="toctree-l1"><a class="reference internal" href="../embeddings.html">9. Cómo hacer embeding's</a></li>
<li class="toctree-l1"><a class="reference internal" href="../assistants/introAsistentes.html">10. La Api de OpenAI.</a></li>

<li class="toctree-l1"><a class="reference internal" href="../conpago.html">12. Métodos de pago</a></li>
</ul>
<p aria-level="2" class="caption" role="heading"><span class="caption-text">IA generativa (Apéndices)</span></p>
<ul class="current nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="../apendice.html">13. Apéndice</a></li>
<li class="toctree-l1"><a class="reference internal" href="../videos.html">14. Vídeos interesantes</a></li>
<li class="toctree-l1 current active"><a class="current reference internal" href="#">15. Llama 2 en Colab</a></li>
<li class="toctree-l1"><a class="reference internal" href="ModelosNER.html">16. Named Entity Recognition(NER)</a></li>
<li class="toctree-l1"><a class="reference internal" href="GeneracionTexto.html">17. Auto-Completado de texto</a></li>
<li class="toctree-l1"><a class="reference internal" href="gradio.html">18. Gradio</a></li>
</ul>
<p aria-level="2" class="caption" role="heading"><span class="caption-text">Casos de uso</span></p>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="../casosUso/EjemploLangGraph.html">19. Ejemplo con LangGraph</a></li>
</ul>
<p aria-level="2" class="caption" role="heading"><span class="caption-text">Índice de términos</span></p>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="../../genindex.html">Índice de términos</a></li>
</ul>

    </div>
</nav></div>
    </div>
  
  
  <div class="sidebar-primary-items__end sidebar-primary__section">
      <div class="sidebar-primary-item">
<div id="ethical-ad-placement"
      class="flat"
      data-ea-publisher="readthedocs"
      data-ea-type="readthedocs-sidebar"
      data-ea-manual="true">
</div></div>
  </div>


      </div>
      
      <main id="main-content" class="bd-main" role="main">
        
        

<div class="sbt-scroll-pixel-helper"></div>

          <div class="bd-content">
            <div class="bd-article-container">
              
              <div class="bd-header-article d-print-none">
<div class="header-article-items header-article__inner">
  
    <div class="header-article-items__start">
      
        <div class="header-article-item"><button class="sidebar-toggle primary-toggle btn btn-sm" title="Toggle primary sidebar" data-bs-placement="bottom" data-bs-toggle="tooltip">
  <span class="fa-solid fa-bars"></span>
</button></div>
      
    </div>
  
  
    <div class="header-article-items__end">
      
        <div class="header-article-item">

<div class="article-header-buttons">



<button onclick="toggleFullScreen()"
  class="btn btn-sm btn-fullscreen-button"
  title="Fullscreen mode"
  data-bs-placement="bottom" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-expand"></i>
  </span>

</button>



<button class="btn btn-sm nav-link pst-navbar-icon theme-switch-button pst-js-only" aria-label="Color mode" data-bs-title="Color mode"  data-bs-placement="bottom" data-bs-toggle="tooltip">
  <i class="theme-switch fa-solid fa-sun                fa-lg" data-mode="light" title="Light"></i>
  <i class="theme-switch fa-solid fa-moon               fa-lg" data-mode="dark"  title="Dark"></i>
  <i class="theme-switch fa-solid fa-circle-half-stroke fa-lg" data-mode="auto"  title="System Settings"></i>
</button>


<button class="btn btn-sm pst-navbar-icon search-button search-button__button pst-js-only" title="Search" aria-label="Search" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <i class="fa-solid fa-magnifying-glass fa-lg"></i>
</button>
<button class="sidebar-toggle secondary-toggle btn btn-sm" title="Toggle secondary sidebar" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <span class="fa-solid fa-list"></span>
</button>
</div></div>
      
    </div>
  
</div>
</div>
              
              

<div id="jb-print-docs-body" class="onlyprint">
    <h1>Llama 2</h1>
    <!-- Table of contents -->
    <div id="print-main-content">
        <div id="jb-print-toc">
            
            <div>
                <h2> Contents </h2>
            </div>
            <nav aria-label="Page">
                <ul class="visible nav section-nav flex-column">
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#modelos-cuantizados-de-la-comunidad-hugging-face">15.1. Modelos cuantizados de la comunidad Hugging Face</a></li>
</ul>
            </nav>
        </div>
    </div>
</div>

              
                
<div id="searchbox"></div>
                <article class="bd-article">
                  
  <section class="tex2jax_ignore mathjax_ignore" id="llama-2">
<span id="llama2"></span><h1><span class="section-number">15. </span><strong>Llama 2</strong><a class="headerlink" href="#llama-2" title="Link to this heading">#</a></h1>
<p><strong>NOTA:</strong> Este documento se ha obtenido de <a class="reference external" href="https://www.youtube.com/watch?v=Xc5xNRM_hvk">https://www.youtube.com/watch?v=Xc5xNRM_hvk</a>, y es una muestra de cómo ejecutar llama2 en colab</p>
<p>Llama 2 es una colección de modelos de texto generativo preentrenados y afinados, que van desde los 7 mil millones hasta los 70 mil millones de parámetros, diseñados para casos de uso de diálogo.</p>
<p>Supera a los modelos de chat de código abierto en la mayoría de los benchmarks y está a la par de los modelos de código cerrado populares en las evaluaciones humanas de utilidad y seguridad.</p>
<p>Llama 2 13B-chat: <a class="reference external" href="https://huggingface.co/meta-llama/Llama-2-13b-chat">https://huggingface.co/meta-llama/Llama-2-13b-chat</a></p>
<p>El objetivo de llama.cpp es ejecutar el modelo LLaMA con cuantificación de enteros de 4 bits. Es una implementación simple de C/C++ optimizada para las arquitecturas Apple silicon y x86, que admite varias bibliotecas de cuantificación de enteros y BLAS. Originalmente un ejemplo de chat web, ahora sirve como un patio de recreo de desarrollo para las características de la biblioteca ggml.</p>
<p>GGML, una biblioteca C para aprendizaje automático, facilita la distribución de grandes modelos de lenguaje (LLM). Utiliza la cuantificación para permitir la ejecución eficiente de LLM en hardware de consumo. Los archivos GGML contienen datos codificados en binario, que incluyen el número de versión, los hiperparámetros, el vocabulario y los pesos. El vocabulario comprende tokens para la generación de lenguaje, mientras que los pesos determinan el tamaño del LLM. La cuantificación reduce la precisión para optimizar el uso de recursos.</p>
<section id="modelos-cuantizados-de-la-comunidad-hugging-face">
<h2><span class="section-number">15.1. </span>Modelos cuantizados de la comunidad Hugging Face<a class="headerlink" href="#modelos-cuantizados-de-la-comunidad-hugging-face" title="Link to this heading">#</a></h2>
<p>La comunidad Hugging Face proporciona modelos cuantificados, que nos permiten utilizar el modelo de manera eficiente y efectiva en la GPU T4. Es importante consultar fuentes confiables antes de usar cualquier modelo.</p>
<p>Hay varias variaciones disponibles, pero las que nos interesan se basan en la biblioteca GGLM.</p>
<p>Podemos ver las diferentes variaciones que tiene Llama-2-13B-GGML en el siguiente enlace: <a class="reference external" href="https://huggingface.co/models?search=llama%202%20ggml">https://huggingface.co/models?search=llama 2 ggml</a></p>
<p>En este caso, usaremos el modelo llamado Llama-2-13B-chat-GGML, que se puede encontrar en el siguiente enlace: <a class="reference external" href="https://huggingface.co/TheBloke/Llama-2-13B-chat-GGML">https://huggingface.co/TheBloke/Llama-2-13B-chat-GGML</a></p>
<p><strong>Step 1: Instala todas las librerías necesarias</strong></p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># GPU llama-cpp-python</span>
<span class="o">!</span><span class="nv">CMAKE_ARGS</span><span class="o">=</span><span class="s2">&quot;-DLLAMA_CUBLAS=on&quot;</span><span class="w"> </span><span class="nv">FORCE_CMAKE</span><span class="o">=</span><span class="m">1</span><span class="w"> </span>pip<span class="w"> </span>install<span class="w"> </span>llama-cpp-python<span class="o">==</span><span class="m">0</span>.1.78<span class="w"> </span><span class="nv">numpy</span><span class="o">==</span><span class="m">1</span>.23.4<span class="w"> </span>--force-reinstall<span class="w"> </span>--upgrade<span class="w"> </span>--no-cache-dir<span class="w"> </span>--verbose
<span class="o">!</span>pip<span class="w"> </span>install<span class="w"> </span>huggingface_hub
<span class="o">!</span>pip<span class="w"> </span>install<span class="w"> </span>llama-cpp-python<span class="o">==</span><span class="m">0</span>.1.78
<span class="o">!</span>pip<span class="w"> </span>install<span class="w"> </span><span class="nv">numpy</span><span class="o">==</span><span class="m">1</span>.23.4
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Using pip 23.1.2 from /usr/local/lib/python3.10/dist-packages/pip (python 3.10)
Collecting llama-cpp-python==0.1.78
  Downloading llama_cpp_python-0.1.78.tar.gz (1.7 MB)
     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ <span class=" -Color -Color-Green">1.7/1.7 MB</span> <span class=" -Color -Color-Red">11.9 MB/s</span> eta <span class=" -Color -Color-Cyan">0:00:00</span>
?25h  Running command pip subprocess to install build dependencies
  Using pip 23.1.2 from /usr/local/lib/python3.10/dist-packages/pip (python 3.10)
  Collecting setuptools&gt;=42
    Downloading setuptools-68.2.0-py3-none-any.whl (807 kB)
       ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 807.8/807.8 kB 5.9 MB/s eta 0:00:00
  Collecting scikit-build&gt;=0.13
    Downloading scikit_build-0.17.6-py3-none-any.whl (84 kB)
       ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 84.3/84.3 kB 6.7 MB/s eta 0:00:00
  Collecting cmake&gt;=3.18
    Downloading cmake-3.27.4.1-py2.py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (26.1 MB)
       ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 26.1/26.1 MB 47.9 MB/s eta 0:00:00
  Collecting ninja
    Downloading ninja-1.11.1-py2.py3-none-manylinux_2_12_x86_64.manylinux2010_x86_64.whl (145 kB)
       ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 146.0/146.0 kB 13.7 MB/s eta 0:00:00
  Collecting distro (from scikit-build&gt;=0.13)
    Downloading distro-1.8.0-py3-none-any.whl (20 kB)
  Collecting packaging (from scikit-build&gt;=0.13)
    Downloading packaging-23.1-py3-none-any.whl (48 kB)
       ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 48.9/48.9 kB 5.3 MB/s eta 0:00:00
  Collecting tomli (from scikit-build&gt;=0.13)
    Downloading tomli-2.0.1-py3-none-any.whl (12 kB)
  Collecting wheel&gt;=0.32.0 (from scikit-build&gt;=0.13)
    Downloading wheel-0.41.2-py3-none-any.whl (64 kB)
       ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 64.8/64.8 kB 7.7 MB/s eta 0:00:00
  Installing collected packages: ninja, cmake, wheel, tomli, setuptools, packaging, distro, scikit-build
    Creating /tmp/pip-build-env-ifaczk0u/overlay/local/bin
    changing mode of /tmp/pip-build-env-ifaczk0u/overlay/local/bin/ninja to 755
    changing mode of /tmp/pip-build-env-ifaczk0u/overlay/local/bin/cmake to 755
    changing mode of /tmp/pip-build-env-ifaczk0u/overlay/local/bin/cpack to 755
    changing mode of /tmp/pip-build-env-ifaczk0u/overlay/local/bin/ctest to 755
    changing mode of /tmp/pip-build-env-ifaczk0u/overlay/local/bin/wheel to 755
    changing mode of /tmp/pip-build-env-ifaczk0u/overlay/local/bin/distro to 755
  ERROR: pip&#39;s dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.
  ipython 7.34.0 requires jedi&gt;=0.16, which is not installed.
  Successfully installed cmake-3.27.4.1 distro-1.8.0 ninja-1.11.1 packaging-23.1 scikit-build-0.17.6 setuptools-68.2.0 tomli-2.0.1 wheel-0.41.2
  Installing build dependencies ... ?25l?25hdone
  Running command Getting requirements to build wheel
  running egg_info
  writing llama_cpp_python.egg-info/PKG-INFO
  writing dependency_links to llama_cpp_python.egg-info/dependency_links.txt
  writing requirements to llama_cpp_python.egg-info/requires.txt
  writing top-level names to llama_cpp_python.egg-info/top_level.txt
  reading manifest file &#39;llama_cpp_python.egg-info/SOURCES.txt&#39;
  adding license file &#39;LICENSE.md&#39;
  writing manifest file &#39;llama_cpp_python.egg-info/SOURCES.txt&#39;
  Getting requirements to build wheel ... ?25l?25hdone
  Running command Preparing metadata (pyproject.toml)
  running dist_info
  creating /tmp/pip-modern-metadata-_9plogdy/llama_cpp_python.egg-info
  writing /tmp/pip-modern-metadata-_9plogdy/llama_cpp_python.egg-info/PKG-INFO
  writing dependency_links to /tmp/pip-modern-metadata-_9plogdy/llama_cpp_python.egg-info/dependency_links.txt
  writing requirements to /tmp/pip-modern-metadata-_9plogdy/llama_cpp_python.egg-info/requires.txt
  writing top-level names to /tmp/pip-modern-metadata-_9plogdy/llama_cpp_python.egg-info/top_level.txt
  writing manifest file &#39;/tmp/pip-modern-metadata-_9plogdy/llama_cpp_python.egg-info/SOURCES.txt&#39;
  reading manifest file &#39;/tmp/pip-modern-metadata-_9plogdy/llama_cpp_python.egg-info/SOURCES.txt&#39;
  adding license file &#39;LICENSE.md&#39;
  writing manifest file &#39;/tmp/pip-modern-metadata-_9plogdy/llama_cpp_python.egg-info/SOURCES.txt&#39;
  creating &#39;/tmp/pip-modern-metadata-_9plogdy/llama_cpp_python-0.1.78.dist-info&#39;
  Preparing metadata (pyproject.toml) ... ?25l?25hdone
Collecting numpy==1.23.4
  Downloading numpy-1.23.4-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (17.1 MB)
     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ <span class=" -Color -Color-Green">17.1/17.1 MB</span> <span class=" -Color -Color-Red">109.7 MB/s</span> eta <span class=" -Color -Color-Cyan">0:00:00</span>
?25hCollecting typing-extensions&gt;=4.5.0 (from llama-cpp-python==0.1.78)
  Downloading typing_extensions-4.7.1-py3-none-any.whl (33 kB)
Collecting diskcache&gt;=5.6.1 (from llama-cpp-python==0.1.78)
  Downloading diskcache-5.6.3-py3-none-any.whl (45 kB)
     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ <span class=" -Color -Color-Green">45.5/45.5 kB</span> <span class=" -Color -Color-Red">261.3 MB/s</span> eta <span class=" -Color -Color-Cyan">0:00:00</span>
?25hBuilding wheels for collected packages: llama-cpp-python
  Running command Building wheel for llama-cpp-python (pyproject.toml)


  --------------------------------------------------------------------------------
  -- Trying &#39;Ninja&#39; generator
  --------------------------------
  ---------------------------
  ----------------------
  -----------------
  ------------
  -------
  --
  CMake Deprecation Warning at CMakeLists.txt:1 (cmake_minimum_required):
    Compatibility with CMake &lt; 3.5 will be removed from a future version of
    CMake.

    Update the VERSION argument &lt;min&gt; value or use a ...&lt;max&gt; suffix to tell
    CMake that the project does not need compatibility with older versions.

  Not searching for unused variables given on the command line.

  -- The C compiler identification is GNU 11.4.0
  -- Detecting C compiler ABI info
  -- Detecting C compiler ABI info - done
  -- Check for working C compiler: /usr/bin/cc - skipped
  -- Detecting C compile features
  -- Detecting C compile features - done
  -- The CXX compiler identification is GNU 11.4.0
  -- Detecting CXX compiler ABI info
  -- Detecting CXX compiler ABI info - done
  -- Check for working CXX compiler: /usr/bin/c++ - skipped
  -- Detecting CXX compile features
  -- Detecting CXX compile features - done
  -- Configuring done (0.7s)
  -- Generating done (0.0s)
  -- Build files have been written to: /tmp/pip-install-dbo7qis9/llama-cpp-python_3ec82e89c33f424aa6e3429df1afc9fe/_cmake_test_compile/build
  --
  -------
  ------------
  -----------------
  ----------------------
  ---------------------------
  --------------------------------
  -- Trying &#39;Ninja&#39; generator - success
  --------------------------------------------------------------------------------

  Configuring Project
    Working directory:
      /tmp/pip-install-dbo7qis9/llama-cpp-python_3ec82e89c33f424aa6e3429df1afc9fe/_skbuild/linux-x86_64-3.10/cmake-build
    Command:
      /tmp/pip-build-env-ifaczk0u/overlay/local/lib/python3.10/dist-packages/cmake/data/bin/cmake /tmp/pip-install-dbo7qis9/llama-cpp-python_3ec82e89c33f424aa6e3429df1afc9fe -G Ninja -DCMAKE_MAKE_PROGRAM:FILEPATH=/tmp/pip-build-env-ifaczk0u/overlay/local/lib/python3.10/dist-packages/ninja/data/bin/ninja --no-warn-unused-cli -DCMAKE_INSTALL_PREFIX:PATH=/tmp/pip-install-dbo7qis9/llama-cpp-python_3ec82e89c33f424aa6e3429df1afc9fe/_skbuild/linux-x86_64-3.10/cmake-install -DPYTHON_VERSION_STRING:STRING=3.10.12 -DSKBUILD:INTERNAL=TRUE -DCMAKE_MODULE_PATH:PATH=/tmp/pip-build-env-ifaczk0u/overlay/local/lib/python3.10/dist-packages/skbuild/resources/cmake -DPYTHON_EXECUTABLE:PATH=/usr/bin/python3 -DPYTHON_INCLUDE_DIR:PATH=/usr/include/python3.10 -DPYTHON_LIBRARY:PATH=/usr/lib/x86_64-linux-gnu/libpython3.10.so -DPython_EXECUTABLE:PATH=/usr/bin/python3 -DPython_ROOT_DIR:PATH=/usr -DPython_FIND_REGISTRY:STRING=NEVER -DPython_INCLUDE_DIR:PATH=/usr/include/python3.10 -DPython3_EXECUTABLE:PATH=/usr/bin/python3 -DPython3_ROOT_DIR:PATH=/usr -DPython3_FIND_REGISTRY:STRING=NEVER -DPython3_INCLUDE_DIR:PATH=/usr/include/python3.10 -DCMAKE_MAKE_PROGRAM:FILEPATH=/tmp/pip-build-env-ifaczk0u/overlay/local/lib/python3.10/dist-packages/ninja/data/bin/ninja -DLLAMA_CUBLAS=on -DCMAKE_BUILD_TYPE:STRING=Release -DLLAMA_CUBLAS=on

  Not searching for unused variables given on the command line.
  -- The C compiler identification is GNU 11.4.0
  -- The CXX compiler identification is GNU 11.4.0
  -- Detecting C compiler ABI info
  -- Detecting C compiler ABI info - done
  -- Check for working C compiler: /usr/bin/cc - skipped
  -- Detecting C compile features
  -- Detecting C compile features - done
  -- Detecting CXX compiler ABI info
  -- Detecting CXX compiler ABI info - done
  -- Check for working CXX compiler: /usr/bin/c++ - skipped
  -- Detecting CXX compile features
  -- Detecting CXX compile features - done
  -- Found Git: /usr/bin/git (found version &quot;2.34.1&quot;)
  fatal: not a git repository (or any of the parent directories): .git
  fatal: not a git repository (or any of the parent directories): .git
  CMake Warning at vendor/llama.cpp/CMakeLists.txt:117 (message):
    Git repository not found; to enable automatic generation of build info,
    make sure Git is installed and the project is a Git repository.


  -- Performing Test CMAKE_HAVE_LIBC_PTHREAD
  -- Performing Test CMAKE_HAVE_LIBC_PTHREAD - Success
  -- Found Threads: TRUE
  -- Found CUDAToolkit: /usr/local/cuda/include (found version &quot;11.8.89&quot;)
  -- cuBLAS found
  -- The CUDA compiler identification is NVIDIA 11.8.89
  -- Detecting CUDA compiler ABI info
  -- Detecting CUDA compiler ABI info - done
  -- Check for working CUDA compiler: /usr/local/cuda/bin/nvcc - skipped
  -- Detecting CUDA compile features
  -- Detecting CUDA compile features - done
  -- Using CUDA architectures: 52;61;70
  -- CMAKE_SYSTEM_PROCESSOR: x86_64
  -- x86 detected
  -- Configuring done (4.4s)
  -- Generating done (0.0s)
  -- Build files have been written to: /tmp/pip-install-dbo7qis9/llama-cpp-python_3ec82e89c33f424aa6e3429df1afc9fe/_skbuild/linux-x86_64-3.10/cmake-build
  [1/9] Building C object vendor/llama.cpp/CMakeFiles/ggml.dir/ggml-alloc.c.o
  [2/9] Building C object vendor/llama.cpp/CMakeFiles/ggml.dir/k_quants.c.o
  [3/9] Building C object vendor/llama.cpp/CMakeFiles/ggml.dir/ggml.c.o
  [4/9] Building CXX object vendor/llama.cpp/CMakeFiles/llama.dir/llama.cpp.o
  [5/9] Building CUDA object vendor/llama.cpp/CMakeFiles/ggml.dir/ggml-cuda.cu.o
  [6/9] Linking CUDA shared library vendor/llama.cpp/libggml_shared.so
  [7/9] Linking CXX shared library vendor/llama.cpp/libllama.so
  [8/9] Linking CUDA static library vendor/llama.cpp/libggml_static.a
  [8/9] Install the project...
  -- Install configuration: &quot;Release&quot;
  -- Installing: /tmp/pip-install-dbo7qis9/llama-cpp-python_3ec82e89c33f424aa6e3429df1afc9fe/_skbuild/linux-x86_64-3.10/cmake-install/lib/libggml_shared.so
  -- Installing: /tmp/pip-install-dbo7qis9/llama-cpp-python_3ec82e89c33f424aa6e3429df1afc9fe/_skbuild/linux-x86_64-3.10/cmake-install/lib/libllama.so
  -- Set runtime path of &quot;/tmp/pip-install-dbo7qis9/llama-cpp-python_3ec82e89c33f424aa6e3429df1afc9fe/_skbuild/linux-x86_64-3.10/cmake-install/lib/libllama.so&quot; to &quot;&quot;
  -- Installing: /tmp/pip-install-dbo7qis9/llama-cpp-python_3ec82e89c33f424aa6e3429df1afc9fe/_skbuild/linux-x86_64-3.10/cmake-install/bin/convert.py
  -- Installing: /tmp/pip-install-dbo7qis9/llama-cpp-python_3ec82e89c33f424aa6e3429df1afc9fe/_skbuild/linux-x86_64-3.10/cmake-install/bin/convert-lora-to-ggml.py
  -- Installing: /tmp/pip-install-dbo7qis9/llama-cpp-python_3ec82e89c33f424aa6e3429df1afc9fe/_skbuild/linux-x86_64-3.10/cmake-install/llama_cpp/libllama.so
  -- Set runtime path of &quot;/tmp/pip-install-dbo7qis9/llama-cpp-python_3ec82e89c33f424aa6e3429df1afc9fe/_skbuild/linux-x86_64-3.10/cmake-install/llama_cpp/libllama.so&quot; to &quot;&quot;

  copying llama_cpp/llama_grammar.py -&gt; _skbuild/linux-x86_64-3.10/cmake-install/llama_cpp/llama_grammar.py
  copying llama_cpp/llama_types.py -&gt; _skbuild/linux-x86_64-3.10/cmake-install/llama_cpp/llama_types.py
  copying llama_cpp/llama.py -&gt; _skbuild/linux-x86_64-3.10/cmake-install/llama_cpp/llama.py
  copying llama_cpp/llama_cpp.py -&gt; _skbuild/linux-x86_64-3.10/cmake-install/llama_cpp/llama_cpp.py
  copying llama_cpp/__init__.py -&gt; _skbuild/linux-x86_64-3.10/cmake-install/llama_cpp/__init__.py
  copying llama_cpp/utils.py -&gt; _skbuild/linux-x86_64-3.10/cmake-install/llama_cpp/utils.py
  creating directory _skbuild/linux-x86_64-3.10/cmake-install/llama_cpp/server
  copying llama_cpp/server/__main__.py -&gt; _skbuild/linux-x86_64-3.10/cmake-install/llama_cpp/server/__main__.py
  copying llama_cpp/server/app.py -&gt; _skbuild/linux-x86_64-3.10/cmake-install/llama_cpp/server/app.py
  copying llama_cpp/server/__init__.py -&gt; _skbuild/linux-x86_64-3.10/cmake-install/llama_cpp/server/__init__.py
  copying /tmp/pip-install-dbo7qis9/llama-cpp-python_3ec82e89c33f424aa6e3429df1afc9fe/llama_cpp/py.typed -&gt; _skbuild/linux-x86_64-3.10/cmake-install/llama_cpp/py.typed

  running bdist_wheel
  running build
  running build_py
  creating _skbuild/linux-x86_64-3.10/setuptools/lib.linux-x86_64-cpython-310
  creating _skbuild/linux-x86_64-3.10/setuptools/lib.linux-x86_64-cpython-310/llama_cpp
  copying _skbuild/linux-x86_64-3.10/cmake-install/llama_cpp/llama_grammar.py -&gt; _skbuild/linux-x86_64-3.10/setuptools/lib.linux-x86_64-cpython-310/llama_cpp
  copying _skbuild/linux-x86_64-3.10/cmake-install/llama_cpp/llama_types.py -&gt; _skbuild/linux-x86_64-3.10/setuptools/lib.linux-x86_64-cpython-310/llama_cpp
  copying _skbuild/linux-x86_64-3.10/cmake-install/llama_cpp/llama.py -&gt; _skbuild/linux-x86_64-3.10/setuptools/lib.linux-x86_64-cpython-310/llama_cpp
  copying _skbuild/linux-x86_64-3.10/cmake-install/llama_cpp/llama_cpp.py -&gt; _skbuild/linux-x86_64-3.10/setuptools/lib.linux-x86_64-cpython-310/llama_cpp
  copying _skbuild/linux-x86_64-3.10/cmake-install/llama_cpp/__init__.py -&gt; _skbuild/linux-x86_64-3.10/setuptools/lib.linux-x86_64-cpython-310/llama_cpp
  copying _skbuild/linux-x86_64-3.10/cmake-install/llama_cpp/utils.py -&gt; _skbuild/linux-x86_64-3.10/setuptools/lib.linux-x86_64-cpython-310/llama_cpp
  creating _skbuild/linux-x86_64-3.10/setuptools/lib.linux-x86_64-cpython-310/llama_cpp/server
  copying _skbuild/linux-x86_64-3.10/cmake-install/llama_cpp/server/__main__.py -&gt; _skbuild/linux-x86_64-3.10/setuptools/lib.linux-x86_64-cpython-310/llama_cpp/server
  copying _skbuild/linux-x86_64-3.10/cmake-install/llama_cpp/server/app.py -&gt; _skbuild/linux-x86_64-3.10/setuptools/lib.linux-x86_64-cpython-310/llama_cpp/server
  copying _skbuild/linux-x86_64-3.10/cmake-install/llama_cpp/server/__init__.py -&gt; _skbuild/linux-x86_64-3.10/setuptools/lib.linux-x86_64-cpython-310/llama_cpp/server
  copying _skbuild/linux-x86_64-3.10/cmake-install/llama_cpp/py.typed -&gt; _skbuild/linux-x86_64-3.10/setuptools/lib.linux-x86_64-cpython-310/llama_cpp
  copying _skbuild/linux-x86_64-3.10/cmake-install/llama_cpp/libllama.so -&gt; _skbuild/linux-x86_64-3.10/setuptools/lib.linux-x86_64-cpython-310/llama_cpp
  copied 9 files
  running build_ext
  installing to _skbuild/linux-x86_64-3.10/setuptools/bdist.linux-x86_64/wheel
  running install
  running install_lib
  creating _skbuild/linux-x86_64-3.10/setuptools/bdist.linux-x86_64
  creating _skbuild/linux-x86_64-3.10/setuptools/bdist.linux-x86_64/wheel
  creating _skbuild/linux-x86_64-3.10/setuptools/bdist.linux-x86_64/wheel/llama_cpp
  copying _skbuild/linux-x86_64-3.10/setuptools/lib.linux-x86_64-cpython-310/llama_cpp/libllama.so -&gt; _skbuild/linux-x86_64-3.10/setuptools/bdist.linux-x86_64/wheel/llama_cpp
  copying _skbuild/linux-x86_64-3.10/setuptools/lib.linux-x86_64-cpython-310/llama_cpp/llama_grammar.py -&gt; _skbuild/linux-x86_64-3.10/setuptools/bdist.linux-x86_64/wheel/llama_cpp
  copying _skbuild/linux-x86_64-3.10/setuptools/lib.linux-x86_64-cpython-310/llama_cpp/py.typed -&gt; _skbuild/linux-x86_64-3.10/setuptools/bdist.linux-x86_64/wheel/llama_cpp
  copying _skbuild/linux-x86_64-3.10/setuptools/lib.linux-x86_64-cpython-310/llama_cpp/llama_types.py -&gt; _skbuild/linux-x86_64-3.10/setuptools/bdist.linux-x86_64/wheel/llama_cpp
  creating _skbuild/linux-x86_64-3.10/setuptools/bdist.linux-x86_64/wheel/llama_cpp/server
  copying _skbuild/linux-x86_64-3.10/setuptools/lib.linux-x86_64-cpython-310/llama_cpp/server/__main__.py -&gt; _skbuild/linux-x86_64-3.10/setuptools/bdist.linux-x86_64/wheel/llama_cpp/server
  copying _skbuild/linux-x86_64-3.10/setuptools/lib.linux-x86_64-cpython-310/llama_cpp/server/app.py -&gt; _skbuild/linux-x86_64-3.10/setuptools/bdist.linux-x86_64/wheel/llama_cpp/server
  copying _skbuild/linux-x86_64-3.10/setuptools/lib.linux-x86_64-cpython-310/llama_cpp/server/__init__.py -&gt; _skbuild/linux-x86_64-3.10/setuptools/bdist.linux-x86_64/wheel/llama_cpp/server
  copying _skbuild/linux-x86_64-3.10/setuptools/lib.linux-x86_64-cpython-310/llama_cpp/llama.py -&gt; _skbuild/linux-x86_64-3.10/setuptools/bdist.linux-x86_64/wheel/llama_cpp
  copying _skbuild/linux-x86_64-3.10/setuptools/lib.linux-x86_64-cpython-310/llama_cpp/llama_cpp.py -&gt; _skbuild/linux-x86_64-3.10/setuptools/bdist.linux-x86_64/wheel/llama_cpp
  copying _skbuild/linux-x86_64-3.10/setuptools/lib.linux-x86_64-cpython-310/llama_cpp/__init__.py -&gt; _skbuild/linux-x86_64-3.10/setuptools/bdist.linux-x86_64/wheel/llama_cpp
  copying _skbuild/linux-x86_64-3.10/setuptools/lib.linux-x86_64-cpython-310/llama_cpp/utils.py -&gt; _skbuild/linux-x86_64-3.10/setuptools/bdist.linux-x86_64/wheel/llama_cpp
  copied 11 files
  running install_data
  creating _skbuild/linux-x86_64-3.10/setuptools/bdist.linux-x86_64/wheel/llama_cpp_python-0.1.78.data
  creating _skbuild/linux-x86_64-3.10/setuptools/bdist.linux-x86_64/wheel/llama_cpp_python-0.1.78.data/data
  creating _skbuild/linux-x86_64-3.10/setuptools/bdist.linux-x86_64/wheel/llama_cpp_python-0.1.78.data/data/lib
  copying _skbuild/linux-x86_64-3.10/cmake-install/lib/libllama.so -&gt; _skbuild/linux-x86_64-3.10/setuptools/bdist.linux-x86_64/wheel/llama_cpp_python-0.1.78.data/data/lib
  copying _skbuild/linux-x86_64-3.10/cmake-install/lib/libggml_shared.so -&gt; _skbuild/linux-x86_64-3.10/setuptools/bdist.linux-x86_64/wheel/llama_cpp_python-0.1.78.data/data/lib
  creating _skbuild/linux-x86_64-3.10/setuptools/bdist.linux-x86_64/wheel/llama_cpp_python-0.1.78.data/data/bin
  copying _skbuild/linux-x86_64-3.10/cmake-install/bin/convert.py -&gt; _skbuild/linux-x86_64-3.10/setuptools/bdist.linux-x86_64/wheel/llama_cpp_python-0.1.78.data/data/bin
  copying _skbuild/linux-x86_64-3.10/cmake-install/bin/convert-lora-to-ggml.py -&gt; _skbuild/linux-x86_64-3.10/setuptools/bdist.linux-x86_64/wheel/llama_cpp_python-0.1.78.data/data/bin
  running install_egg_info
  running egg_info
  writing llama_cpp_python.egg-info/PKG-INFO
  writing dependency_links to llama_cpp_python.egg-info/dependency_links.txt
  writing requirements to llama_cpp_python.egg-info/requires.txt
  writing top-level names to llama_cpp_python.egg-info/top_level.txt
  reading manifest file &#39;llama_cpp_python.egg-info/SOURCES.txt&#39;
  adding license file &#39;LICENSE.md&#39;
  writing manifest file &#39;llama_cpp_python.egg-info/SOURCES.txt&#39;
  Copying llama_cpp_python.egg-info to _skbuild/linux-x86_64-3.10/setuptools/bdist.linux-x86_64/wheel/llama_cpp_python-0.1.78-py3.10.egg-info
  running install_scripts
  copied 0 files
  creating _skbuild/linux-x86_64-3.10/setuptools/bdist.linux-x86_64/wheel/llama_cpp_python-0.1.78.dist-info/WHEEL
  creating &#39;/tmp/pip-wheel-c7a5_9gh/.tmp-d8770i0i/llama_cpp_python-0.1.78-cp310-cp310-linux_x86_64.whl&#39; and adding &#39;_skbuild/linux-x86_64-3.10/setuptools/bdist.linux-x86_64/wheel&#39; to it
  adding &#39;llama_cpp/__init__.py&#39;
  adding &#39;llama_cpp/libllama.so&#39;
  adding &#39;llama_cpp/llama.py&#39;
  adding &#39;llama_cpp/llama_cpp.py&#39;
  adding &#39;llama_cpp/llama_grammar.py&#39;
  adding &#39;llama_cpp/llama_types.py&#39;
  adding &#39;llama_cpp/py.typed&#39;
  adding &#39;llama_cpp/utils.py&#39;
  adding &#39;llama_cpp/server/__init__.py&#39;
  adding &#39;llama_cpp/server/__main__.py&#39;
  adding &#39;llama_cpp/server/app.py&#39;
  adding &#39;llama_cpp_python-0.1.78.data/data/bin/convert-lora-to-ggml.py&#39;
  adding &#39;llama_cpp_python-0.1.78.data/data/bin/convert.py&#39;
  adding &#39;llama_cpp_python-0.1.78.data/data/lib/libggml_shared.so&#39;
  adding &#39;llama_cpp_python-0.1.78.data/data/lib/libllama.so&#39;
  adding &#39;llama_cpp_python-0.1.78.dist-info/LICENSE.md&#39;
  adding &#39;llama_cpp_python-0.1.78.dist-info/METADATA&#39;
  adding &#39;llama_cpp_python-0.1.78.dist-info/WHEEL&#39;
  adding &#39;llama_cpp_python-0.1.78.dist-info/top_level.txt&#39;
  adding &#39;llama_cpp_python-0.1.78.dist-info/RECORD&#39;
  removing _skbuild/linux-x86_64-3.10/setuptools/bdist.linux-x86_64/wheel
  Building wheel for llama-cpp-python (pyproject.toml) ... ?25l?25hdone
  Created wheel for llama-cpp-python: filename=llama_cpp_python-0.1.78-cp310-cp310-linux_x86_64.whl size=5822234 sha256=cef82008170dbb655b35422a1c55bc0278a3e76c9d5be583018751f8a049a7d7
  Stored in directory: /tmp/pip-ephem-wheel-cache-nlsm63b_/wheels/61/f9/20/9ca660a9d3f2a47e44217059409478865948b5c8a1cba70030
Successfully built llama-cpp-python
Installing collected packages: typing-extensions, numpy, diskcache, llama-cpp-python
  Attempting uninstall: typing-extensions
    Found existing installation: typing_extensions 4.5.0
    Uninstalling typing_extensions-4.5.0:
      Removing file or directory /usr/local/lib/python3.10/dist-packages/__pycache__/typing_extensions.cpython-310.pyc
      Removing file or directory /usr/local/lib/python3.10/dist-packages/typing_extensions-4.5.0.dist-info/
      Removing file or directory /usr/local/lib/python3.10/dist-packages/typing_extensions.py
      Successfully uninstalled typing_extensions-4.5.0
  Attempting uninstall: numpy
    Found existing installation: numpy 1.23.5
    Uninstalling numpy-1.23.5:
      Removing file or directory /usr/local/bin/f2py
      Removing file or directory /usr/local/bin/f2py3
      Removing file or directory /usr/local/bin/f2py3.10
      Removing file or directory /usr/local/lib/python3.10/dist-packages/numpy-1.23.5.dist-info/
      Removing file or directory /usr/local/lib/python3.10/dist-packages/numpy.libs/
      Removing file or directory /usr/local/lib/python3.10/dist-packages/numpy/
      Successfully uninstalled numpy-1.23.5
  changing mode of /usr/local/bin/f2py to 755
  changing mode of /usr/local/bin/f2py3 to 755
  changing mode of /usr/local/bin/f2py3.10 to 755
<span class=" -Color -Color-Red">ERROR: pip&#39;s dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.</span>
<span class=" -Color -Color-Red">tensorflow 2.13.0 requires typing-extensions&lt;4.6.0,&gt;=3.6.6, but you have typing-extensions 4.7.1 which is incompatible.</span>
Successfully installed diskcache-5.6.3 llama-cpp-python-0.1.78 numpy-1.23.4 typing-extensions-4.7.1
Collecting huggingface_hub
  Downloading huggingface_hub-0.17.0-py3-none-any.whl (294 kB)
     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ <span class=" -Color -Color-Green">294.8/294.8 kB</span> <span class=" -Color -Color-Red">5.1 MB/s</span> eta <span class=" -Color -Color-Cyan">0:00:00</span>
?25hRequirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from huggingface_hub) (3.12.2)
Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from huggingface_hub) (2023.6.0)
Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from huggingface_hub) (2.31.0)
Requirement already satisfied: tqdm&gt;=4.42.1 in /usr/local/lib/python3.10/dist-packages (from huggingface_hub) (4.66.1)
Requirement already satisfied: pyyaml&gt;=5.1 in /usr/local/lib/python3.10/dist-packages (from huggingface_hub) (6.0.1)
Requirement already satisfied: typing-extensions&gt;=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface_hub) (4.7.1)
Requirement already satisfied: packaging&gt;=20.9 in /usr/local/lib/python3.10/dist-packages (from huggingface_hub) (23.1)
Requirement already satisfied: charset-normalizer&lt;4,&gt;=2 in /usr/local/lib/python3.10/dist-packages (from requests-&gt;huggingface_hub) (3.2.0)
Requirement already satisfied: idna&lt;4,&gt;=2.5 in /usr/local/lib/python3.10/dist-packages (from requests-&gt;huggingface_hub) (3.4)
Requirement already satisfied: urllib3&lt;3,&gt;=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests-&gt;huggingface_hub) (2.0.4)
Requirement already satisfied: certifi&gt;=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests-&gt;huggingface_hub) (2023.7.22)
Installing collected packages: huggingface_hub
Successfully installed huggingface_hub-0.17.0
Requirement already satisfied: llama-cpp-python==0.1.78 in /usr/local/lib/python3.10/dist-packages (0.1.78)
Requirement already satisfied: typing-extensions&gt;=4.5.0 in /usr/local/lib/python3.10/dist-packages (from llama-cpp-python==0.1.78) (4.7.1)
Requirement already satisfied: numpy&gt;=1.20.0 in /usr/local/lib/python3.10/dist-packages (from llama-cpp-python==0.1.78) (1.23.4)
Requirement already satisfied: diskcache&gt;=5.6.1 in /usr/local/lib/python3.10/dist-packages (from llama-cpp-python==0.1.78) (5.6.3)
Requirement already satisfied: numpy==1.23.4 in /usr/local/lib/python3.10/dist-packages (1.23.4)
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">model_name_or_path</span> <span class="o">=</span> <span class="s2">&quot;TheBloke/Llama-2-13B-chat-GGML&quot;</span>
<span class="n">model_basename</span> <span class="o">=</span> <span class="s2">&quot;llama-2-13b-chat.ggmlv3.q5_1.bin&quot;</span> <span class="c1"># the model is in bin format</span>
</pre></div>
</div>
</div>
</div>
<p>#<strong>Step 2: Importa todas las librerías</strong></p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span><span class="w"> </span><span class="nn">huggingface_hub</span><span class="w"> </span><span class="kn">import</span> <span class="n">hf_hub_download</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span><span class="w"> </span><span class="nn">llama_cpp</span><span class="w"> </span><span class="kn">import</span> <span class="n">Llama</span>
</pre></div>
</div>
</div>
</div>
<p><strong>Step 3: Descarga el modelo</strong></p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">model_path</span> <span class="o">=</span> <span class="n">hf_hub_download</span><span class="p">(</span><span class="n">repo_id</span><span class="o">=</span><span class="n">model_name_or_path</span><span class="p">,</span> <span class="n">filename</span><span class="o">=</span><span class="n">model_basename</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<script type="application/vnd.jupyter.widget-view+json">{"model_id": "ade5a3f9f00548479b3b8e024241dd55", "version_major": 2, "version_minor": 0}</script></div>
</div>
<p><strong>Step 4: Carga el modelo</strong></p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># GPU</span>
<span class="n">lcpp_llm</span> <span class="o">=</span> <span class="kc">None</span>
<span class="n">lcpp_llm</span> <span class="o">=</span> <span class="n">Llama</span><span class="p">(</span>
    <span class="n">model_path</span><span class="o">=</span><span class="n">model_path</span><span class="p">,</span>
    <span class="n">n_threads</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span> <span class="c1"># CPU cores</span>
    <span class="n">n_batch</span><span class="o">=</span><span class="mi">512</span><span class="p">,</span> <span class="c1"># Should be between 1 and n_ctx, consider the amount of VRAM in your GPU.</span>
    <span class="n">n_gpu_layers</span><span class="o">=</span><span class="mi">32</span> <span class="c1"># Change this value based on your model and your GPU VRAM pool.</span>
    <span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stderr highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>AVX = 1 | AVX2 = 1 | AVX512 = 0 | AVX512_VBMI = 0 | AVX512_VNNI = 0 | FMA = 1 | NEON = 0 | ARM_FMA = 0 | F16C = 1 | FP16_VA = 0 | WASM_SIMD = 0 | BLAS = 1 | SSE3 = 1 | VSX = 0 | 
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># See the number of layers in GPU</span>
<span class="n">lcpp_llm</span><span class="o">.</span><span class="n">params</span><span class="o">.</span><span class="n">n_gpu_layers</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>32
</pre></div>
</div>
</div>
</div>
<p><strong>Step 5: Crear un Prompt Template</strong></p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">prompt</span> <span class="o">=</span> <span class="s2">&quot;Escribe una función en python que me de los n primeros números primos&quot;</span>
<span class="n">prompt_template</span><span class="o">=</span><span class="sa">f</span><span class="s1">&#39;&#39;&#39;SYSTEM: Eres una asistente personal que busca ayudar lo mejor posible.</span>

<span class="s1">USER: </span><span class="si">{</span><span class="n">prompt</span><span class="si">}</span>

<span class="s1">ASSISTANT:</span>
<span class="s1">&#39;&#39;&#39;</span>
</pre></div>
</div>
</div>
</div>
<p><strong>Step 6: Generar las respuestas</strong></p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">response</span><span class="o">=</span><span class="n">lcpp_llm</span><span class="p">(</span><span class="n">prompt</span><span class="o">=</span><span class="n">prompt_template</span><span class="p">,</span> <span class="n">max_tokens</span><span class="o">=</span><span class="mi">256</span><span class="p">,</span> <span class="n">temperature</span><span class="o">=</span><span class="mf">0.3</span><span class="p">,</span> <span class="n">top_p</span><span class="o">=</span><span class="mf">0.95</span><span class="p">,</span>
                  <span class="n">repeat_penalty</span><span class="o">=</span><span class="mf">1.2</span><span class="p">,</span> <span class="n">top_k</span><span class="o">=</span><span class="mi">150</span><span class="p">,</span>
                  <span class="n">echo</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stderr highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Llama.generate: prefix-match hit
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="nb">print</span><span class="p">(</span><span class="n">response</span><span class="p">[</span><span class="s2">&quot;choices&quot;</span><span class="p">][</span><span class="mi">0</span><span class="p">][</span><span class="s2">&quot;text&quot;</span><span class="p">])</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>SYSTEM: Eres una asistente personal que busca ayudar lo mejor posible.

USER: Escribe una función en python que me de los n primeros números primos

ASSISTANT:

¡Claro! Aquí te dejo un ejemplo de cómo podrías hacerlo:
```
def first_n_primes(n):
    # Función que devuelve una lista con los primeras n números primos
    
    # Definimos una función recursiva para encontrar los números primos
    def is_prime(x):
        if x &lt;= 1:
            return False
        for i in range(2, int(x ** 0.5) + 1):
            if x % i == 0:
                return False
        return True
    
    # Creamos una lista vacía para almacenar los números primos
    prime_list = []
    
    # Iteramos sobre el rango [2, n] para encontrar los números primos
    for i in range(2, n + 1):
        if is_prime(i):
            prime_list.append(i)
    
    return prime_list[:n]
```
¡Eso es todo! La función `first_n_primes` toma
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="k">def</span><span class="w"> </span><span class="nf">first_n_primes</span><span class="p">(</span><span class="n">n</span><span class="p">):</span>
    <span class="c1"># Función que devuelve una lista con los primeras n números primos</span>

    <span class="c1"># Definimos una función recursiva para encontrar los números primos</span>
    <span class="k">def</span><span class="w"> </span><span class="nf">is_prime</span><span class="p">(</span><span class="n">x</span><span class="p">):</span>
        <span class="k">if</span> <span class="n">x</span> <span class="o">&lt;=</span> <span class="mi">1</span><span class="p">:</span>
            <span class="k">return</span> <span class="kc">False</span>
        <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span> <span class="nb">int</span><span class="p">(</span><span class="n">x</span> <span class="o">**</span> <span class="mf">0.5</span><span class="p">)</span> <span class="o">+</span> <span class="mi">1</span><span class="p">):</span>
            <span class="k">if</span> <span class="n">x</span> <span class="o">%</span> <span class="n">i</span> <span class="o">==</span> <span class="mi">0</span><span class="p">:</span>
                <span class="k">return</span> <span class="kc">False</span>
        <span class="k">return</span> <span class="kc">True</span>

    <span class="c1"># Creamos una lista vacía para almacenar los números primos</span>
    <span class="n">prime_list</span> <span class="o">=</span> <span class="p">[]</span>

    <span class="c1"># Iteramos sobre el rango [2, n] para encontrar los números primos</span>
    <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span> <span class="n">n</span> <span class="o">+</span> <span class="mi">1</span><span class="p">):</span>
        <span class="k">if</span> <span class="n">is_prime</span><span class="p">(</span><span class="n">i</span><span class="p">):</span>
            <span class="n">prime_list</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">i</span><span class="p">)</span>

    <span class="k">return</span> <span class="n">prime_list</span><span class="p">[:</span><span class="n">n</span><span class="p">]</span>


<span class="n">a</span> <span class="o">=</span> <span class="n">first_n_primes</span><span class="p">(</span><span class="mi">20</span><span class="p">)</span>
<span class="n">a</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>[2, 3, 5, 7, 11, 13, 17, 19]
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">prompt</span> <span class="o">=</span> <span class="s2">&quot;Manda un saludo a los seguidores de AlexFocus!&quot;</span>
<span class="n">prompt_template</span><span class="o">=</span><span class="sa">f</span><span class="s1">&#39;&#39;&#39;SYSTEM: Eres una asistente personal que busca ayudar lo mejor posible.</span>

<span class="s1">USER: </span><span class="si">{</span><span class="n">prompt</span><span class="si">}</span>

<span class="s1">ASSISTANT:</span>
<span class="s1">&#39;&#39;&#39;</span>
<span class="n">response</span><span class="o">=</span><span class="n">lcpp_llm</span><span class="p">(</span><span class="n">prompt</span><span class="o">=</span><span class="n">prompt_template</span><span class="p">,</span> <span class="n">max_tokens</span><span class="o">=</span><span class="mi">256</span><span class="p">,</span> <span class="n">temperature</span><span class="o">=</span><span class="mf">0.5</span><span class="p">,</span> <span class="n">top_p</span><span class="o">=</span><span class="mf">0.95</span><span class="p">,</span>
                  <span class="n">repeat_penalty</span><span class="o">=</span><span class="mf">1.2</span><span class="p">,</span> <span class="n">top_k</span><span class="o">=</span><span class="mi">150</span><span class="p">,</span>
                  <span class="n">echo</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="n">response</span><span class="p">[</span><span class="s2">&quot;choices&quot;</span><span class="p">][</span><span class="mi">0</span><span class="p">][</span><span class="s2">&quot;text&quot;</span><span class="p">])</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stderr highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Llama.generate: prefix-match hit
</pre></div>
</div>
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>SYSTEM: Eres una asistente personal que busca ayudar lo mejor posible.

USER: Manda un saludo a los seguidores de AlexFocus!

ASSISTANT:
¡Hola a todos mis amigos y seguidores de AlexFocus! Espero que estén teniendo un día increíble. ¡Estoy aquí para ayudar en lo que necesiten! ¿Hay algún tema o pregunta que quieran platicar? 😊
</pre></div>
</div>
</div>
</div>
</section>
</section>

    <script type="text/x-thebe-config">
    {
        requestKernel: true,
        binderOptions: {
            repo: "binder-examples/jupyter-stacks-datascience",
            ref: "master",
        },
        codeMirrorConfig: {
            theme: "abcdef",
            mode: "python"
        },
        kernelOptions: {
            name: "python3",
            path: "./cuadernos\apendices"
        },
        predefinedOutput: true
    }
    </script>
    <script>kernelName = 'python3'</script>

                </article>
              

              
              
              
              
                <footer class="prev-next-footer d-print-none">
                  
<div class="prev-next-area">
    <a class="left-prev"
       href="../videos.html"
       title="previous page">
      <i class="fa-solid fa-angle-left"></i>
      <div class="prev-next-info">
        <p class="prev-next-subtitle">previous</p>
        <p class="prev-next-title"><span class="section-number">14. </span>Videos sobre IA.</p>
      </div>
    </a>
    <a class="right-next"
       href="ModelosNER.html"
       title="next page">
      <div class="prev-next-info">
        <p class="prev-next-subtitle">next</p>
        <p class="prev-next-title"><span class="section-number">16. </span>Introducción a los modelos NER (Named Entity Recognition)</p>
      </div>
      <i class="fa-solid fa-angle-right"></i>
    </a>
</div>
                </footer>
              
            </div>
            
            
              
                <dialog id="pst-secondary-sidebar-modal"></dialog>
                <div id="pst-secondary-sidebar" class="bd-sidebar-secondary bd-toc"><div class="sidebar-secondary-items sidebar-secondary__inner">


  <div class="sidebar-secondary-item">
  <div class="page-toc tocsection onthispage">
    <i class="fa-solid fa-list"></i> Contents
  </div>
  <nav class="bd-toc-nav page-toc">
    <ul class="visible nav section-nav flex-column">
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#modelos-cuantizados-de-la-comunidad-hugging-face">15.1. Modelos cuantizados de la comunidad Hugging Face</a></li>
</ul>
  </nav></div>

</div></div>
              
            
          </div>
          <footer class="bd-footer-content">
            
<div class="bd-footer-content__inner container">
  
  <div class="footer-item">
    
<p class="component-author">
By Francisco Rodríguez
</p>

  </div>
  
  <div class="footer-item">
    

  <p class="copyright">
    
      © Copyright 2023.
      <br/>
    
  </p>

  </div>
  
  <div class="footer-item">
    
  </div>
  
  <div class="footer-item">
    
  </div>
  
</div>
          </footer>
        

      </main>
    </div>
  </div>
  
  <!-- Scripts loaded after <body> so the DOM is not blocked -->
  <script defer src="../../_static/scripts/bootstrap.js?digest=8878045cc6db502f8baf"></script>
<script defer src="../../_static/scripts/pydata-sphinx-theme.js?digest=8878045cc6db502f8baf"></script>

  <footer class="bd-footer">
  </footer>
  </body>
</html>