
<!DOCTYPE html>


<html lang="en" data-content_root="../" >

  <head>
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" /><meta name="viewport" content="width=device-width, initial-scale=1" />

    <title>1. Introducción a LangChain &#8212; IA generativa</title>
  
  
  
  <script data-cfasync="false">
    document.documentElement.dataset.mode = localStorage.getItem("mode") || "";
    document.documentElement.dataset.theme = localStorage.getItem("theme") || "";
  </script>
  <!--
    this give us a css class that will be invisible only if js is disabled
  -->
  <noscript>
    <style>
      .pst-js-only { display: none !important; }

    </style>
  </noscript>
  
  <!-- Loaded before other Sphinx assets -->
  <link href="../_static/styles/theme.css?digest=8878045cc6db502f8baf" rel="stylesheet" />
<link href="../_static/styles/pydata-sphinx-theme.css?digest=8878045cc6db502f8baf" rel="stylesheet" />

    <link rel="stylesheet" type="text/css" href="../_static/pygments.css?v=03e43079" />
    <link rel="stylesheet" type="text/css" href="../_static/styles/sphinx-book-theme.css?v=a3416100" />
    <link rel="stylesheet" type="text/css" href="../_static/togglebutton.css?v=13237357" />
    <link rel="stylesheet" type="text/css" href="../_static/copybutton.css?v=76b2166b" />
    <link rel="stylesheet" type="text/css" href="../_static/mystnb.4510f1fc1dee50b3e5859aac5469c37c29e427902b24a333a5f9fcb2f0b3ac41.css?v=be8a1c11" />
    <link rel="stylesheet" type="text/css" href="../_static/sphinx-thebe.css?v=4fa983c6" />
    <link rel="stylesheet" type="text/css" href="../_static/sphinx-design.min.css?v=95c83b7e" />
  
  <!-- So that users can add custom icons -->
  <script src="../_static/scripts/fontawesome.js?digest=8878045cc6db502f8baf"></script>
  <!-- Pre-loaded scripts that we'll load fully later -->
  <link rel="preload" as="script" href="../_static/scripts/bootstrap.js?digest=8878045cc6db502f8baf" />
<link rel="preload" as="script" href="../_static/scripts/pydata-sphinx-theme.js?digest=8878045cc6db502f8baf" />

    <script src="../_static/documentation_options.js?v=9eb32ce0"></script>
    <script src="../_static/doctools.js?v=9a2dae69"></script>
    <script src="../_static/sphinx_highlight.js?v=dc90522c"></script>
    <script src="../_static/clipboard.min.js?v=a7894cd8"></script>
    <script src="../_static/copybutton.js?v=f281be69"></script>
    <script src="../_static/scripts/sphinx-book-theme.js?v=887ef09a"></script>
    <script>let toggleHintShow = 'Click to show';</script>
    <script>let toggleHintHide = 'Click to hide';</script>
    <script>let toggleOpenOnPrint = 'true';</script>
    <script src="../_static/togglebutton.js?v=4a39c7ea"></script>
    <script>var togglebuttonSelector = '.toggle, .admonition.dropdown';</script>
    <script src="../_static/design-tabs.js?v=f930bc37"></script>
    <script>const THEBE_JS_URL = "https://unpkg.com/thebe@0.8.2/lib/index.js"; const thebe_selector = ".thebe,.cell"; const thebe_selector_input = "pre"; const thebe_selector_output = ".output, .cell_output"</script>
    <script async="async" src="../_static/sphinx-thebe.js?v=c100c467"></script>
    <script>var togglebuttonSelector = '.toggle, .admonition.dropdown';</script>
    <script>const THEBE_JS_URL = "https://unpkg.com/thebe@0.8.2/lib/index.js"; const thebe_selector = ".thebe,.cell"; const thebe_selector_input = "pre"; const thebe_selector_output = ".output, .cell_output"</script>
    <script>DOCUMENTATION_OPTIONS.pagename = 'cuadernos/langchain';</script>
    <link rel="index" title="Index" href="../genindex.html" />
    <link rel="search" title="Search" href="../search.html" />
    <link rel="next" title="2. Bases de datos en LangChain." href="basesDatosLangchain.html" />
    <link rel="prev" title="IA generativa" href="../introduccion.html" />
  <meta name="viewport" content="width=device-width, initial-scale=1"/>
  <meta name="docsearch:language" content="en"/>
  <meta name="docsearch:version" content="" />
  </head>
  
  
  <body data-bs-spy="scroll" data-bs-target=".bd-toc-nav" data-offset="180" data-bs-root-margin="0px 0px -60%" data-default-mode="">

  
  
  <div id="pst-skip-link" class="skip-link d-print-none"><a href="#main-content">Skip to main content</a></div>
  
  <div id="pst-scroll-pixel-helper"></div>
  
  <button type="button" class="btn rounded-pill" id="pst-back-to-top">
    <i class="fa-solid fa-arrow-up"></i>Back to top</button>

  
  <dialog id="pst-search-dialog">
    
<form class="bd-search d-flex align-items-center"
      action="../search.html"
      method="get">
  <i class="fa-solid fa-magnifying-glass"></i>
  <input type="search"
         class="form-control"
         name="q"
         placeholder="Search..."
         aria-label="Search..."
         autocomplete="off"
         autocorrect="off"
         autocapitalize="off"
         spellcheck="false"/>
  <span class="search-button__kbd-shortcut"><kbd class="kbd-shortcut__modifier">Ctrl</kbd>+<kbd>K</kbd></span>
</form>
  </dialog>

  <div class="pst-async-banner-revealer d-none">
  <aside id="bd-header-version-warning" class="d-none d-print-none" aria-label="Version warning"></aside>
</div>

  
    <header class="bd-header navbar navbar-expand-lg bd-navbar d-print-none">
    </header>
  

  <div class="bd-container">
    <div class="bd-container__inner bd-page-width">
      
      
      
      <dialog id="pst-primary-sidebar-modal"></dialog>
      <div id="pst-primary-sidebar" class="bd-sidebar-primary bd-sidebar">
        

  
  <div class="sidebar-header-items sidebar-primary__section">
    
    
    
    
  </div>
  
    <div class="sidebar-primary-items__start sidebar-primary__section">
        <div class="sidebar-primary-item">

  
    
  

<a class="navbar-brand logo" href="../introduccion.html">
  
  
  
  
  
  
    <p class="title logo__title">IA generativa</p>
  
</a></div>
        <div class="sidebar-primary-item">

<button class="btn search-button-field search-button__button pst-js-only" title="Search" aria-label="Search" data-bs-placement="bottom" data-bs-toggle="tooltip">
 <i class="fa-solid fa-magnifying-glass"></i>
 <span class="search-button__default-text">Search</span>
 <span class="search-button__kbd-shortcut"><kbd class="kbd-shortcut__modifier">Ctrl</kbd>+<kbd class="kbd-shortcut__modifier">K</kbd></span>
</button></div>
        <div class="sidebar-primary-item"><nav class="bd-links bd-docs-nav" aria-label="Main">
    <div class="bd-toc-item navbar-nav active">
        <p aria-level="2" class="caption" role="heading"><span class="caption-text">IA generativa</span></p>
<ul class="current nav bd-sidenav">
<li class="toctree-l1 current active"><a class="current reference internal" href="#">1. Langchain</a></li>
<li class="toctree-l1"><a class="reference internal" href="basesDatosLangchain.html">2. Conectores a Bases de Datos</a></li>
<li class="toctree-l1"><a class="reference internal" href="cadenas.html">3. Las cadenas de LangChain</a></li>
<li class="toctree-l1"><a class="reference internal" href="memoria.html">4. La memoria en LangChain</a></li>
<li class="toctree-l1"><a class="reference internal" href="Agenteslangchain.html">5. Los Agentes en LangChain</a></li>
<li class="toctree-l1"><a class="reference internal" href="ollama.html">6. Ollama</a></li>
<li class="toctree-l1"><a class="reference internal" href="embeddings.html">7. Cómo hacer embeding's</a></li>
<li class="toctree-l1"><a class="reference internal" href="assistants/introAsistentes.html">8. La Api de OpenAI.</a></li>

<li class="toctree-l1"><a class="reference internal" href="conpago.html">10. Métodos de pago</a></li>
</ul>
<p aria-level="2" class="caption" role="heading"><span class="caption-text">IA generativa (Apéndices)</span></p>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="apendice.html">11. Apéndice</a></li>
<li class="toctree-l1"><a class="reference internal" href="videos.html">12. Vídeos interesantes</a></li>
<li class="toctree-l1"><a class="reference internal" href="apendices/Llama_2.html">13. Llama 2 en Colab</a></li>
<li class="toctree-l1"><a class="reference internal" href="apendices/ModelosNER.html">14. Named Entity Recognition(NER)</a></li>
<li class="toctree-l1"><a class="reference internal" href="apendices/GeneracionTexto.html">15. Auto-Completado de texto</a></li>
<li class="toctree-l1"><a class="reference internal" href="apendices/gradio.html">16. Gradio</a></li>
</ul>
<p aria-level="2" class="caption" role="heading"><span class="caption-text">Casos de uso</span></p>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="casosUso/EjemploLangGraph.html">17. Ejemplo con LangGraph</a></li>
</ul>
<p aria-level="2" class="caption" role="heading"><span class="caption-text">Índice de términos</span></p>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="../genindex.html">Índice de términos</a></li>
</ul>

    </div>
</nav></div>
    </div>
  
  
  <div class="sidebar-primary-items__end sidebar-primary__section">
      <div class="sidebar-primary-item">
<div id="ethical-ad-placement"
      class="flat"
      data-ea-publisher="readthedocs"
      data-ea-type="readthedocs-sidebar"
      data-ea-manual="true">
</div></div>
  </div>


      </div>
      
      <main id="main-content" class="bd-main" role="main">
        
        

<div class="sbt-scroll-pixel-helper"></div>

          <div class="bd-content">
            <div class="bd-article-container">
              
              <div class="bd-header-article d-print-none">
<div class="header-article-items header-article__inner">
  
    <div class="header-article-items__start">
      
        <div class="header-article-item"><button class="sidebar-toggle primary-toggle btn btn-sm" title="Toggle primary sidebar" data-bs-placement="bottom" data-bs-toggle="tooltip">
  <span class="fa-solid fa-bars"></span>
</button></div>
      
    </div>
  
  
    <div class="header-article-items__end">
      
        <div class="header-article-item">

<div class="article-header-buttons">



<button onclick="toggleFullScreen()"
  class="btn btn-sm btn-fullscreen-button"
  title="Fullscreen mode"
  data-bs-placement="bottom" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-expand"></i>
  </span>

</button>



<button class="btn btn-sm nav-link pst-navbar-icon theme-switch-button pst-js-only" aria-label="Color mode" data-bs-title="Color mode"  data-bs-placement="bottom" data-bs-toggle="tooltip">
  <i class="theme-switch fa-solid fa-sun                fa-lg" data-mode="light" title="Light"></i>
  <i class="theme-switch fa-solid fa-moon               fa-lg" data-mode="dark"  title="Dark"></i>
  <i class="theme-switch fa-solid fa-circle-half-stroke fa-lg" data-mode="auto"  title="System Settings"></i>
</button>


<button class="btn btn-sm pst-navbar-icon search-button search-button__button pst-js-only" title="Search" aria-label="Search" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <i class="fa-solid fa-magnifying-glass fa-lg"></i>
</button>
<button class="sidebar-toggle secondary-toggle btn btn-sm" title="Toggle secondary sidebar" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <span class="fa-solid fa-list"></span>
</button>
</div></div>
      
    </div>
  
</div>
</div>
              
              

<div id="jb-print-docs-body" class="onlyprint">
    <h1>Introducción a LangChain</h1>
    <!-- Table of contents -->
    <div id="print-main-content">
        <div id="jb-print-toc">
            
            <div>
                <h2> Contents </h2>
            </div>
            <nav aria-label="Page">
                <ul class="visible nav section-nav flex-column">
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#apartados-de-langchain">1.1. Apartados de Langchain.</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#como-trabajar-con-langchain-en-remoto">1.2. Cómo trabajar con LangChain en remoto</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#como-trabajar-con-google-colab">1.3. Como trabajar con Google Colab.</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#como-trabajar-con-langchain-en-local">1.4. Cómo trabajar con LangChain en local.</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#las-plantillas-templates-en-langchain">1.5. Las plantillas (templates) en LangChain.</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#parsear-y-procesar-la-salida">1.6. Parsear y procesar la salida.</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#parsear-lista-elementos-separados-por-comas">1.6.1. Parsear lista elementos separados por comas.</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#parseador-en-formato-de-fechas">1.6.2. Parseador en formato de fechas</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#metodos-para-solucionar-problemas-de-parcheo">1.6.3. Métodos para solucionar problemas de parcheo.</a><ul class="nav section-nav flex-column">
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#auto-fix-parser">1.6.3.1. Auto-Fix Parser.</a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#system-promt">1.6.3.2. System Promt</a></li>
</ul>
</li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#serializacion-de-prompts">1.7. Serialización de Prompts.</a></li>
</ul>
            </nav>
        </div>
    </div>
</div>

              
                
<div id="searchbox"></div>
                <article class="bd-article">
                  
  <section class="tex2jax_ignore mathjax_ignore" id="introduccion-a-langchain">
<h1><span class="section-number">1. </span>Introducción a LangChain<a class="headerlink" href="#introduccion-a-langchain" title="Link to this heading">#</a></h1>
<p>En el ámbito de la inteligencia artificial y el desarrollo de aplicaciones basadas en modelos de lenguaje, <strong>LangChain</strong> se ha convertido en una de las herramientas más innovadoras y versátiles. Se trata de un framework diseñado para facilitar la integración de <strong>modelos de lenguaje de gran tamaño (LLMs, por sus siglas en inglés)</strong> en aplicaciones dinámicas e interactivas. Su objetivo principal es simplificar el desarrollo de flujos de trabajo complejos que combinan diferentes fuentes de datos, almacenamiento de memoria, ejecución de agentes y procesamiento estructurado de información.</p>
<p>Una de las características más destacadas de LangChain es su capacidad para conectar modelos de lenguaje con bases de datos, APIs y documentos externos, permitiendo a los desarrolladores crear aplicaciones avanzadas como <strong>asistentes conversacionales, motores de búsqueda mejorados, generación de código automático y automatización de tareas empresariales</strong>. Para lograrlo, LangChain proporciona módulos modulares y componibles que permiten personalizar y optimizar la interacción con los modelos de IA.</p>
<p>El framework se basa en cinco componentes clave:</p>
<ol class="arabic simple">
<li><p><strong>Modelos de lenguaje</strong>: Integra modelos como OpenAI GPT, Hugging Face Transformers, Cohere y muchos más, facilitando su uso en diversas tareas de procesamiento del lenguaje natural.</p></li>
<li><p><strong>Encadenamiento de procesos (Chains)</strong>: Permite combinar múltiples pasos en una sola secuencia lógica para tareas más avanzadas.</p></li>
<li><p><strong>Memoria</strong>: Proporciona almacenamiento de contexto en conversaciones para mejorar la coherencia y continuidad de las interacciones.</p></li>
<li><p><strong>Recuperación y conexión con datos externos</strong>: Facilita el acceso a bases de datos, documentos y APIs para enriquecer las respuestas del modelo.</p></li>
<li><p><strong>Agentes y herramientas</strong>: Permite el uso de modelos como agentes capaces de tomar decisiones y ejecutar acciones basadas en entradas dinámicas.</p></li>
</ol>
<p>El uso de LangChain está revolucionando sectores como la <strong>automatización empresarial, la educación, el soporte al cliente y la investigación</strong>, ofreciendo soluciones más inteligentes y adaptadas a las necesidades del usuario. Su flexibilidad y capacidad de integración hacen de este framework una opción ideal para quienes buscan desarrollar aplicaciones de inteligencia artificial con capacidades conversacionales avanzadas y una gestión eficiente del conocimiento.</p>
<p>En conclusión, LangChain representa un avance significativo en la forma en que los modelos de lenguaje interactúan con entornos del mundo real. Su estructura modular, junto con su compatibilidad con diferentes fuentes de información, permite a los desarrolladores crear soluciones más sofisticadas e inteligentes, abriendo un abanico de posibilidades en el campo de la inteligencia artificial aplicada.</p>
<section id="apartados-de-langchain">
<h2><span class="section-number">1.1. </span>Apartados de Langchain.<a class="headerlink" href="#apartados-de-langchain" title="Link to this heading">#</a></h2>
<p>Langchain cuenta con cinco grandes bloques o apartados: Models, Prompts, indexes, memori, cahin y agentes:</p>
<ul class="simple">
<li><p>Models: Indica la red neuronal que se va a uitilizar.</p></li>
<li><p>Promts: son los textos enviados al modelo. Pueden ser de diferentes tipos:</p></li>
</ul>
<p>1.- Promt template: Son textos que sirven como guia del modelo</p>
<p>2.- chat promt template. Modelos para los chats</p>
<p>3.- promt value. Seria el texto completo ya formateado</p>
<p>4.- Example selector. Ayuda y genera mejor las respuestas.</p>
<ul class="simple">
<li><p>Index. Para dar acceso a distinta fuentes de datos. Permite indexar un gran volumen de documentos. Pueden ser:</p></li>
</ul>
<p>1.- Document Loader: abrir, cargar y procesar diferentes archivos.</p>
<p>2.- Text spliter: partir un documento en bloques más cortos</p>
<p>3.- Vector stores: Donde almacena los embedings</p>
<p>4.- Retrieves. Ayuda a traer información de algún documento específico</p>
<ul class="simple">
<li><p>Memory: Dar memoria para por ejemplo los chats</p></li>
<li><p>cadenas/chain: Permitir unir modelos o cadenas entre sí</p></li>
<li><p>Agentes: Dar acceso a ciertas herramientas para solucionar mejor una determinada tarea</p></li>
</ul>
</section>
<section id="como-trabajar-con-langchain-en-remoto">
<h2><span class="section-number">1.2. </span>Cómo trabajar con LangChain en remoto<a class="headerlink" href="#como-trabajar-con-langchain-en-remoto" title="Link to this heading">#</a></h2>
<p>Existen métodos de pagos con los que se puede trabajar en IA de forma remota. Nosotros y con el fin de que el lector pueda adquirir conocimientos sobre esta materia sin incurrir en ningún costo, hemos desarrollado la mayor parte de los apartados en forma local de manera que no se genere coste alguno.</p>
<p>No obstante para aquellos que quieran saber cómo proceder mediante alguno de los métodos de pago existente, se invita al lector a <a class="reference internal" href="conpago.html#pago"><span class="std std-ref">visitar este apartado</span></a>.</p>
</section>
<section id="como-trabajar-con-google-colab">
<h2><span class="section-number">1.3. </span>Como trabajar con Google Colab.<a class="headerlink" href="#como-trabajar-con-google-colab" title="Link to this heading">#</a></h2>
<p id="index-0">Google Colab es una buena herramienta computacional para realizar trabajos con LLM, en este apartado vamos a mostrar dos formas de poder trabajar este tipo de modelos.</p>
<p>1.- Descargando directamente el model de Hugging Face, como se <a class="reference internal" href="apendices/Llama_2.html#llama2"><span class="std std-ref">muestra en este apartado</span></a>, y se <a href="https://www.youtube.com/watch?v=Xc5xNRM_hvk" target="_blank"> pude ver en este vídeo </a>.</p>
<p>2.- Otra opción interesante es instalar en Colab *colab-xtera” y alli instalar ollama. La forma de proceder en este caso <a href="https://www.youtube.com/watch?v=4tVdDLrucOk" target="_blank"> la puedes ver en este vídeo </a>.</p>
</section>
<section id="como-trabajar-con-langchain-en-local">
<h2><span class="section-number">1.4. </span>Cómo trabajar con LangChain en local.<a class="headerlink" href="#como-trabajar-con-langchain-en-local" title="Link to this heading">#</a></h2>
<p>Para poder trabajar en local, sin necesidad de tener conexión con proveedor de pago, lo que vamos a hacer es trabajar con ollama, cuyo formato de uso lo puedes ver dentro de este trabajo y en concreto <a class="reference internal" href="ollama.html#ollama1"><span class="std std-ref">en este apartado</span></a>, al que se invita a ir al lector si no conoce la materia. En adelante se presupone que el lector conoce esta herramienta para poder trabajar con modelos en local.</p>
<p>Una vez se tenga ese conocimiento, como inicio lo que vamos a crear una serie de solicitudes de entrada básicas para modelos y vamos a ver cómo gestionar los resultados que nos devuelven los LLM.</p>
<p>En esta sección nuestro interés se va a centrar en las funcionalidades básicas y la sintaxis que se necesita para hacer esto con LangChain.</p>
<p>El uso de Langchain y el componente Modelo IO nos permitirá construir cadenas más adelante, pero también nos dará más flexibilidad para cambiar de proveedor de LLM en el futuro , ya que la sintaxis está estandarizada en todos los LLM y solo cambian los parámetros o argumentos proporcionados.</p>
<p>Debemos tener en cuenta 2 parámetros importantes en las solicitudes a las APIs de los LLMs:</p>
<p><img alt="" src="../_images/tipos.PNG" /></p>
<p>Para verificar cómo conectar a los diferentes LLMs integrados en Langchain, ver el siguiente enlace :</p>
<p><a class="reference external" href="https://python.langchain.com/v0.2/docs/integrations/chat/">https://python.langchain.com/v0.2/docs/integrations/chat/</a></p>
<p>Procedemos a continuación a mostrar un ejemplo sobre la creación de un chat en local utilizando el modelo ue tenemos cargado en ollama y que se denomina ‘llama3.1’.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span><span class="w"> </span><span class="nn">langchain</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">langchain_openai</span><span class="w"> </span><span class="kn">import</span> <span class="n">ChatOpenAI</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">langchain.schema</span><span class="w"> </span><span class="kn">import</span> <span class="n">SystemMessage</span><span class="p">,</span> <span class="n">HumanMessage</span>

<span class="n">chat</span> <span class="o">=</span> <span class="n">ChatOpenAI</span><span class="p">(</span>
    <span class="n">model</span><span class="o">=</span><span class="s2">&quot;llama3.2&quot;</span><span class="p">,</span>
    <span class="n">base_url</span> <span class="o">=</span> <span class="s1">&#39;http://localhost:11434/v1&#39;</span><span class="p">,</span>
    <span class="n">api_key</span><span class="o">=</span><span class="s1">&#39;ollama&#39;</span><span class="p">,</span> <span class="c1"># required, but unused,</span>
<span class="p">)</span>

<span class="n">resultado</span> <span class="o">=</span> <span class="n">chat</span><span class="o">.</span><span class="n">invoke</span><span class="p">([</span><span class="n">HumanMessage</span><span class="p">(</span><span class="n">content</span><span class="o">=</span><span class="s2">&quot;¿Puedes decirme donde se encuentra cáceres?&quot;</span><span class="p">)])</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">resultado</span>
</pre></div>
</div>
</div>
</div>
<p>Si queremos ver sólo el resultado buscado, debemos ejecutar la siguiente instrucción</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">resultado</span><span class="o">.</span><span class="n">content</span>
</pre></div>
</div>
</div>
</div>
<p id="index-1">Especificamos el SystemMessage para definir la personalidad que debe tomar el sistema</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">resultado</span> <span class="o">=</span> <span class="n">chat</span><span class="o">.</span><span class="n">invoke</span><span class="p">([</span><span class="n">SystemMessage</span><span class="p">(</span><span class="n">content</span><span class="o">=</span><span class="s1">&#39;Eres un historiador que conoce los detalles de todas las ciudades del mundo&#39;</span><span class="p">),</span>
               <span class="n">HumanMessage</span><span class="p">(</span><span class="n">content</span><span class="o">=</span><span class="s1">&#39;¿Puedes decirme dónde se encuentra Cáceres&#39;</span><span class="p">)])</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">resultado</span><span class="o">.</span><span class="n">content</span>
</pre></div>
</div>
</div>
</div>
<p>Se pueden obtener varios resultados invocando al chat de OpenAI con “generate”. Observar en este ejemplo la importancia que tiene el dar una información previa al modelo para indicar qué tipo de respuesta nos va a dar.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">resultado</span> <span class="o">=</span> <span class="n">chat</span><span class="o">.</span><span class="n">generate</span><span class="p">(</span>
    <span class="p">[</span>
        <span class="p">[</span><span class="n">SystemMessage</span><span class="p">(</span><span class="n">content</span><span class="o">=</span><span class="s1">&#39;Eres un historiador que conoce los detalles de todas las ciudades del mundo&#39;</span><span class="p">),</span>
         <span class="n">HumanMessage</span><span class="p">(</span><span class="n">content</span><span class="o">=</span><span class="s1">&#39;¿Puedes decirme dónde se encuentra Cáceres&#39;</span><span class="p">)],</span>
        <span class="p">[</span><span class="n">SystemMessage</span><span class="p">(</span><span class="n">content</span><span class="o">=</span><span class="s1">&#39;Eres un joven rudo que no le gusta que le pregunten, solo quiere estar de fiesta&#39;</span><span class="p">),</span>
         <span class="n">HumanMessage</span><span class="p">(</span><span class="n">content</span><span class="o">=</span><span class="s1">&#39;¿Puedes decirme dónde se encuentra Cáceres&#39;</span><span class="p">)]</span>
    <span class="p">]</span>
<span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1">#Resultado con primer sistema</span>
<span class="nb">print</span><span class="p">(</span><span class="n">resultado</span><span class="o">.</span><span class="n">generations</span><span class="p">[</span><span class="mi">0</span><span class="p">][</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">text</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1">#Resultado con segundo sistema</span>
<span class="nb">print</span><span class="p">(</span><span class="n">resultado</span><span class="o">.</span><span class="n">generations</span><span class="p">[</span><span class="mi">1</span><span class="p">][</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">text</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
</section>
<section id="las-plantillas-templates-en-langchain">
<h2><span class="section-number">1.5. </span>Las plantillas (templates) en LangChain.<a class="headerlink" href="#las-plantillas-templates-en-langchain" title="Link to this heading">#</a></h2>
<ul class="simple" id="index-2">
<li><p>Las plantillas nos permiten configurar y modificar fácilmente nuestras indicaciones de entrada para las
llamadas de LLM.
• Las plantillas ofrecen un enfoque más sistemático para pasar variables a solicitudes de modelos, en lugar de
usar literales de cadena f o llamadas . format (), PromptTemplate las convierte en nombres de parámetros de
función que podemos pasar.
• Es recomendable usar plantillas para estandarizar los mensajes que enviamos a los LLMs para mayor
flexibilidad y facilidad en futuros usos.</p></li>
</ul>
<p>Veamos cómo utilizar estas plantillas en LangChain. Primero importamos las librerías:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span><span class="w"> </span><span class="nn">langchain.prompts</span><span class="w"> </span><span class="kn">import</span> <span class="p">(</span>
    <span class="n">ChatPromptTemplate</span><span class="p">,</span>
    <span class="n">PromptTemplate</span><span class="p">,</span>
    <span class="n">SystemMessagePromptTemplate</span><span class="p">,</span>
    <span class="n">AIMessagePromptTemplate</span><span class="p">,</span>
    <span class="n">HumanMessagePromptTemplate</span><span class="p">,</span>
<span class="p">)</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">langchain.schema</span><span class="w"> </span><span class="kn">import</span> <span class="p">(</span>
    <span class="n">AIMessage</span><span class="p">,</span>
    <span class="n">HumanMessage</span><span class="p">,</span>
    <span class="n">SystemMessage</span>
<span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<p>Y después procedemeos a crear diferentes plantillas. Comenzamos con una plantilla para el sistema</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1">#Creamos la plantilla del sistema (system_template)</span>
<span class="n">system_template</span><span class="o">=</span><span class="s2">&quot;Eres una IA especializada en coches de tipo </span><span class="si">{tipo_coches}</span><span class="s2"> y generar artículos que se leen en </span><span class="si">{tiempo_lectura}</span><span class="s2">.&quot;</span>
<span class="c1"># Las variables las metemeos entre corchetes</span>
<span class="n">system_message_prompt</span> <span class="o">=</span> <span class="n">SystemMessagePromptTemplate</span><span class="o">.</span><span class="n">from_template</span><span class="p">(</span><span class="n">system_template</span><span class="p">)</span>
<span class="c1"># Mostramos las variables que admite esta plantilla</span>
<span class="n">system_message_prompt</span><span class="o">.</span><span class="n">input_variables</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1">#De forma similar al procedimiento anterior, creamos la plantilla de usuario (human_template)</span>
<span class="n">human_template</span><span class="o">=</span><span class="s2">&quot;Necesito un artículo para vehículos con motor </span><span class="si">{peticion_tipo_motor}</span><span class="s2">&quot;</span>
<span class="n">human_message_prompt</span> <span class="o">=</span> <span class="n">HumanMessagePromptTemplate</span><span class="o">.</span><span class="n">from_template</span><span class="p">(</span><span class="n">human_template</span><span class="p">)</span>
<span class="c1"># También mostramos las variables que admite</span>
<span class="n">human_message_prompt</span><span class="o">.</span><span class="n">input_variables</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1">#Creamos una plantilla de chat con la concatenación tanto de mensajes del sistema como del humano</span>
<span class="n">chat_prompt</span> <span class="o">=</span> <span class="n">ChatPromptTemplate</span><span class="o">.</span><span class="n">from_messages</span><span class="p">([</span><span class="n">system_message_prompt</span><span class="p">,</span> <span class="n">human_message_prompt</span><span class="p">])</span>
<span class="c1"># Los nombres de las variables, serán todas las definidas anteriormente</span>
<span class="n">chat_prompt</span><span class="o">.</span><span class="n">input_variables</span>
</pre></div>
</div>
</div>
</div>
<p>Una vez definida la plantilla, lo que nos queda es introducir los valores concretos que queremos tenga las variables definidas anteriormente</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># Completar el chat gracias al formateo de los mensajes</span>
<span class="n">chat_prompt</span><span class="o">.</span><span class="n">format_prompt</span><span class="p">(</span><span class="n">peticion_tipo_motor</span><span class="o">=</span><span class="s2">&quot;híbrido enchufable&quot;</span><span class="p">,</span> <span class="n">tiempo_lectura</span><span class="o">=</span><span class="s2">&quot;10 min&quot;</span><span class="p">,</span> <span class="n">tipo_coches</span><span class="o">=</span><span class="s2">&quot;japoneses&quot;</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1">#Transformamos el objeto prompt a una lista de mensajes y lo guardamos en &quot;solicitud_completa&quot; que es lo que pasaremos al LLM finalmente</span>
<span class="n">solicitud_completa</span> <span class="o">=</span> <span class="n">chat_prompt</span><span class="o">.</span><span class="n">format_prompt</span><span class="p">(</span><span class="n">peticion_tipo_motor</span><span class="o">=</span><span class="s2">&quot;híbrido enchufable&quot;</span><span class="p">,</span> <span class="n">tiempo_lectura</span><span class="o">=</span><span class="s2">&quot;10 min&quot;</span><span class="p">,</span> <span class="n">tipo_coches</span><span class="o">=</span><span class="s2">&quot;japoneses&quot;</span><span class="p">)</span><span class="o">.</span><span class="n">to_messages</span><span class="p">()</span>
</pre></div>
</div>
</div>
</div>
<p>Con esto ya tenemos completada nuestra plantilla y lista para que pueda ser procesada pro el LLM con el que estemos trabajando</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span><span class="w"> </span><span class="nn">langchain</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">langchain_openai</span><span class="w"> </span><span class="kn">import</span> <span class="n">ChatOpenAI</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">langchain.schema</span><span class="w"> </span><span class="kn">import</span> <span class="n">SystemMessage</span><span class="p">,</span> <span class="n">HumanMessage</span>

<span class="n">chat</span> <span class="o">=</span> <span class="n">ChatOpenAI</span><span class="p">(</span>
    <span class="n">model</span><span class="o">=</span><span class="s2">&quot;llama3.2&quot;</span><span class="p">,</span>
    <span class="n">base_url</span> <span class="o">=</span> <span class="s1">&#39;http://localhost:11434/v1&#39;</span><span class="p">,</span>
    <span class="n">api_key</span><span class="o">=</span><span class="s1">&#39;ollama&#39;</span><span class="p">,</span> <span class="c1"># required, but unused,</span>
<span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">result</span> <span class="o">=</span> <span class="n">chat</span><span class="o">.</span><span class="n">invoke</span><span class="p">(</span><span class="n">solicitud_completa</span><span class="p">)</span>
<span class="n">result</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">result</span><span class="o">.</span><span class="n">content</span>
</pre></div>
</div>
</div>
</div>
</section>
<section id="parsear-y-procesar-la-salida">
<h2><span class="section-number">1.6. </span>Parsear y procesar la salida.<a class="headerlink" href="#parsear-y-procesar-la-salida" title="Link to this heading">#</a></h2>
<p id="index-3">A menudo, al conectar la salida de un LLM (modelo de lenguaje grande), necesitas que esté en un formato particular, por ejemplo, puedes querer un objeto datetime de Python o un objeto JSON.</p>
<p>LangChain viene con utilidades de análisis que te permiten convertir fácilmente las salidas en tipos de datos
precisos gracias a los <strong>parseadores</strong>.</p>
<p>Los elementos claves de estos parseadores son los siguientes:</p>
<ul class="simple">
<li><p>Método parse (): Método concreto para evaluar la cadena de texto string ) y parsearla al tipo deseado.</p></li>
<li><p>Format_instructions : Una cadena de texto extra que Langchain añade al final del prompt para asistir y facilitar la interpretación por el LLM del formato deseado.</p></li>
</ul>
<p>Si no consigues el resultado parseado correctamente (por ejemplo, la respuesta del LLM es más extensa que únicamente una fecha que quieras parsear ), hay dos soluciones:</p>
<p>1.- Usar parseador Auto fix</p>
<p>2.- Usar un “ system prompt ” para dar mayor detalle al LLM de cómo debe actuar y responder.</p>
<p>Veamos algunos ejemplos prácticos para dejar más claro cómo se puede operar en LangChain con estos parseadores. Primero impostamos librerías necesarias e instanciamos un modelo de chat</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span><span class="w"> </span><span class="nn">langchain</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">langchain.prompts</span><span class="w"> </span><span class="kn">import</span> <span class="n">PromptTemplate</span><span class="p">,</span> <span class="n">SystemMessagePromptTemplate</span><span class="p">,</span><span class="n">ChatPromptTemplate</span><span class="p">,</span> <span class="n">HumanMessagePromptTemplate</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">langchain_openai</span><span class="w"> </span><span class="kn">import</span> <span class="n">ChatOpenAI</span>


<span class="n">chat</span> <span class="o">=</span> <span class="n">ChatOpenAI</span><span class="p">(</span>
    <span class="n">model</span><span class="o">=</span><span class="s2">&quot;llama3.2&quot;</span><span class="p">,</span>
    <span class="n">base_url</span> <span class="o">=</span> <span class="s1">&#39;http://localhost:11434/v1&#39;</span><span class="p">,</span>
    <span class="n">api_key</span><span class="o">=</span><span class="s1">&#39;ollama&#39;</span><span class="p">,</span> <span class="c1"># required, but unused,</span>
<span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<section id="parsear-lista-elementos-separados-por-comas">
<h3><span class="section-number">1.6.1. </span>Parsear lista elementos separados por comas.<a class="headerlink" href="#parsear-lista-elementos-separados-por-comas" title="Link to this heading">#</a></h3>
<p>Este puede ser el caso que necesitemos obtener una salida de los elementos que después sean procesados como un fichero de tipo csv.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span><span class="w"> </span><span class="nn">langchain.output_parsers</span><span class="w"> </span><span class="kn">import</span> <span class="n">CommaSeparatedListOutputParser</span>

<span class="n">output_parser</span> <span class="o">=</span> <span class="n">CommaSeparatedListOutputParser</span><span class="p">()</span>
<span class="n">format_instructions</span> <span class="o">=</span> <span class="n">output_parser</span><span class="o">.</span><span class="n">get_format_instructions</span><span class="p">()</span> 
<span class="c1">#Nos devuelve las instrucciones que va a pasar al LLM en función del parseador concreto</span>
<span class="nb">print</span><span class="p">(</span><span class="n">format_instructions</span><span class="p">)</span>
<span class="c1"># Como vemos es una instrucción que el parseador manda al LLM para obtener la lista separada por comas.</span>
<span class="c1"># Viene en ingles, pero funciona perfectamente si trabajamos en castellano</span>
</pre></div>
</div>
</div>
</div>
<p>Vamos a ver un ejemplo imaginario. Suponemos que nos ha devuelto una serie de palabras, separadas por comas. Entonces vamos a ver como pasar eso a una lista que contenga las palabras anteriores que están separadas pro comas.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1">#Respuesta imaginaria</span>
<span class="n">respuesta</span> <span class="o">=</span> <span class="s2">&quot;coche, árbol, carretera&quot;</span>
<span class="n">output_parser</span><span class="o">.</span><span class="n">parse</span><span class="p">(</span><span class="n">respuesta</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<p>Veamos un ejemplo más concreto.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1">#Creamos la plantilla de usuario (human_template) con la concatenación de la variable &quot;request&quot; (la solicitud) y la variable &quot;format_instructions&quot; con </span>
<span class="c1">#las instrucciones adicionales que le pasaremos al LLM</span>
<span class="n">human_template</span> <span class="o">=</span> <span class="s1">&#39;</span><span class="si">{request}</span><span class="se">\n</span><span class="si">{format_instructions}</span><span class="s1">&#39;</span>
<span class="n">human_prompt</span> <span class="o">=</span> <span class="n">HumanMessagePromptTemplate</span><span class="o">.</span><span class="n">from_template</span><span class="p">(</span><span class="n">human_template</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1">#Creamos el prompt y le damos formato a las variables</span>
<span class="n">chat_prompt</span> <span class="o">=</span> <span class="n">ChatPromptTemplate</span><span class="o">.</span><span class="n">from_messages</span><span class="p">([</span><span class="n">human_prompt</span><span class="p">])</span>

<span class="n">chat_prompt</span><span class="o">.</span><span class="n">format_prompt</span><span class="p">(</span><span class="n">request</span><span class="o">=</span><span class="s2">&quot;dime 5 características de los coches americanos&quot;</span><span class="p">,</span>
                   <span class="n">format_instructions</span> <span class="o">=</span> <span class="n">output_parser</span><span class="o">.</span><span class="n">get_format_instructions</span><span class="p">())</span> <span class="c1">#Las instrucciones son las que proporciona el propio parseador</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1">#Transformamos el objeto prompt a una lista de mensajes y lo guardamos en &quot;solicitud_completa&quot; que es lo que pasaremos al LLM finalmente</span>
<span class="n">solicitud_completa</span> <span class="o">=</span> <span class="n">chat_prompt</span><span class="o">.</span><span class="n">format_prompt</span><span class="p">(</span><span class="n">request</span><span class="o">=</span><span class="s2">&quot;dime 5 características de los coches americanos&quot;</span><span class="p">,</span>
                   <span class="n">format_instructions</span> <span class="o">=</span> <span class="n">output_parser</span><span class="o">.</span><span class="n">get_format_instructions</span><span class="p">())</span><span class="o">.</span><span class="n">to_messages</span><span class="p">()</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">result</span> <span class="o">=</span> <span class="n">chat</span><span class="o">.</span><span class="n">invoke</span><span class="p">(</span><span class="n">solicitud_completa</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">result</span><span class="o">.</span><span class="n">content</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># Convertir a la salida esperada</span>
<span class="n">output_parser</span><span class="o">.</span><span class="n">parse</span><span class="p">(</span><span class="n">result</span><span class="o">.</span><span class="n">content</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
</section>
<section id="parseador-en-formato-de-fechas">
<h3><span class="section-number">1.6.2. </span>Parseador en formato de fechas<a class="headerlink" href="#parseador-en-formato-de-fechas" title="Link to this heading">#</a></h3>
<p>Veamos ahora cómo podemos parsear una fecha</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span><span class="w"> </span><span class="nn">langchain.output_parsers</span><span class="w"> </span><span class="kn">import</span> <span class="n">DatetimeOutputParser</span>

<span class="n">output_parser</span> <span class="o">=</span> <span class="n">DatetimeOutputParser</span><span class="p">()</span>
<span class="nb">print</span><span class="p">(</span><span class="n">output_parser</span><span class="o">.</span><span class="n">get_format_instructions</span><span class="p">())</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">template_text</span> <span class="o">=</span> <span class="s2">&quot;</span><span class="si">{request}</span><span class="se">\n</span><span class="si">{format_instructions}</span><span class="s2">&quot;</span>
<span class="n">human_prompt</span><span class="o">=</span><span class="n">HumanMessagePromptTemplate</span><span class="o">.</span><span class="n">from_template</span><span class="p">(</span><span class="n">template_text</span><span class="p">)</span>
<span class="n">chat_prompt</span> <span class="o">=</span> <span class="n">ChatPromptTemplate</span><span class="o">.</span><span class="n">from_messages</span><span class="p">([</span><span class="n">human_prompt</span><span class="p">])</span>
<span class="nb">print</span><span class="p">(</span><span class="n">chat_prompt</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">request</span><span class="o">=</span><span class="s2">&quot;¿Cuándo es el día de la declaración de independencia de los EEUU?&quot;</span><span class="p">,</span>
                   <span class="n">format_instructions</span><span class="o">=</span><span class="n">output_parser</span><span class="o">.</span><span class="n">get_format_instructions</span><span class="p">()</span>
                   <span class="p">))</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">solicitud_completa</span> <span class="o">=</span> <span class="n">chat_prompt</span><span class="o">.</span><span class="n">format_prompt</span><span class="p">(</span><span class="n">request</span><span class="o">=</span><span class="s2">&quot;¿Cuándo es el día de la declaración de independencia de los EEUU?&quot;</span><span class="p">,</span>
                   <span class="n">format_instructions</span><span class="o">=</span><span class="n">output_parser</span><span class="o">.</span><span class="n">get_format_instructions</span><span class="p">()</span>
                   <span class="p">)</span><span class="o">.</span><span class="n">to_messages</span><span class="p">()</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">result</span> <span class="o">=</span> <span class="n">chat</span><span class="o">.</span><span class="n">invoke</span><span class="p">(</span><span class="n">solicitud_completa</span><span class="p">)</span>
<span class="n">result</span><span class="o">.</span><span class="n">content</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">output_parser</span><span class="o">.</span><span class="n">parse</span><span class="p">(</span><span class="n">result</span><span class="o">.</span><span class="n">content</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
</section>
<section id="metodos-para-solucionar-problemas-de-parcheo">
<h3><span class="section-number">1.6.3. </span>Métodos para solucionar problemas de parcheo.<a class="headerlink" href="#metodos-para-solucionar-problemas-de-parcheo" title="Link to this heading">#</a></h3>
<p>Hay ocasiones en las que no obtenemos la salida que nosotros queremos y existen algunas soluciones para dar solución a esos problemas.</p>
<section id="auto-fix-parser">
<h4><span class="section-number">1.6.3.1. </span>Auto-Fix Parser.<a class="headerlink" href="#auto-fix-parser" title="Link to this heading">#</a></h4>
<div class="cell docutils container" id="index-4">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span><span class="w"> </span><span class="nn">langchain.output_parsers</span><span class="w"> </span><span class="kn">import</span> <span class="n">OutputFixingParser</span>

<span class="n">output_parser_dates</span> <span class="o">=</span> <span class="n">DatetimeOutputParser</span><span class="p">()</span>

<span class="n">misformatted</span> <span class="o">=</span> <span class="n">result</span><span class="o">.</span><span class="n">content</span>

<span class="n">misformatted</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">new_parser</span> <span class="o">=</span> <span class="n">OutputFixingParser</span><span class="o">.</span><span class="n">from_llm</span><span class="p">(</span><span class="n">parser</span><span class="o">=</span><span class="n">output_parser_dates</span><span class="p">,</span> <span class="n">llm</span><span class="o">=</span><span class="n">chat</span><span class="p">)</span>
<span class="n">new_parser</span><span class="o">.</span><span class="n">parse</span><span class="p">(</span><span class="n">misformatted</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
</section>
<section id="system-promt">
<h4><span class="section-number">1.6.3.2. </span>System Promt<a class="headerlink" href="#system-promt" title="Link to this heading">#</a></h4>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">system_prompt</span> <span class="o">=</span> <span class="n">SystemMessagePromptTemplate</span><span class="o">.</span><span class="n">from_template</span><span class="p">(</span><span class="s2">&quot;Tienes que responder únicamente con un patrón de fechas&quot;</span><span class="p">)</span>
<span class="n">template_text</span> <span class="o">=</span> <span class="s2">&quot;</span><span class="si">{request}</span><span class="se">\n</span><span class="si">{format_instructions}</span><span class="s2">&quot;</span>
<span class="n">human_prompt</span><span class="o">=</span><span class="n">HumanMessagePromptTemplate</span><span class="o">.</span><span class="n">from_template</span><span class="p">(</span><span class="n">template_text</span><span class="p">)</span>
<span class="n">chat_prompt</span> <span class="o">=</span> <span class="n">ChatPromptTemplate</span><span class="o">.</span><span class="n">from_messages</span><span class="p">([</span><span class="n">system_prompt</span><span class="p">,</span><span class="n">human_prompt</span><span class="p">])</span>
<span class="nb">print</span><span class="p">(</span><span class="n">chat_prompt</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">request</span><span class="o">=</span><span class="s2">&quot;¿Cuándo es el día de la declaración de independencia de los EEUU?&quot;</span><span class="p">,</span>
                   <span class="n">format_instructions</span><span class="o">=</span><span class="n">output_parser_dates</span><span class="o">.</span><span class="n">get_format_instructions</span><span class="p">()</span>
                   <span class="p">))</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">solicitud_completa</span> <span class="o">=</span> <span class="n">chat_prompt</span><span class="o">.</span><span class="n">format_prompt</span><span class="p">(</span><span class="n">request</span><span class="o">=</span><span class="s2">&quot;¿Cuándo es el día de la declaración de independencia de los EEUU?&quot;</span><span class="p">,</span>
                   <span class="n">format_instructions</span><span class="o">=</span><span class="n">output_parser_dates</span><span class="o">.</span><span class="n">get_format_instructions</span><span class="p">()</span>
                   <span class="p">)</span><span class="o">.</span><span class="n">to_messages</span><span class="p">()</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">result</span> <span class="o">=</span> <span class="n">chat</span><span class="o">.</span><span class="n">invoke</span><span class="p">(</span><span class="n">solicitud_completa</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">result</span><span class="o">.</span><span class="n">content</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">output_parser_dates</span><span class="o">.</span><span class="n">parse</span><span class="p">(</span><span class="n">result</span><span class="o">.</span><span class="n">content</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<p>Podemos ver que al final con esta última herramienta hemos podido solucionar los errores de parseo que obteníamos anteriormente</p>
</section>
</section>
</section>
<section id="serializacion-de-prompts">
<h2><span class="section-number">1.7. </span>Serialización de Prompts.<a class="headerlink" href="#serializacion-de-prompts" title="Link to this heading">#</a></h2>
<p>En un capítulo anterior, hemos visto cómo poder trabajar con los denominados Pompts de LangChain. En ciertas ocasiones y bajo determinadas circunstancias, pudiera ocurrir que esos prompts lo queramos guardar, paro por ejemplo utilizarlos para futuros trabajos o compartir con otras personas. En esta sección, vamos a ver cómo podemos conseguir todo esto.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span><span class="w"> </span><span class="nn">langchain.prompts</span><span class="w"> </span><span class="kn">import</span> <span class="n">PromptTemplate</span><span class="p">,</span> <span class="n">SystemMessagePromptTemplate</span><span class="p">,</span><span class="n">ChatPromptTemplate</span><span class="p">,</span> <span class="n">HumanMessagePromptTemplate</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">langchain_openai</span><span class="w"> </span><span class="kn">import</span> <span class="n">ChatOpenAI</span>
</pre></div>
</div>
</div>
</div>
<p>Creamos la plantilla y la guardamos con la denominación “prompt.json”</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">plantilla</span> <span class="o">=</span> <span class="s2">&quot;Pregunta: </span><span class="si">{pregunta_usuario}</span><span class="se">\n\n</span><span class="s2">Respuesta: Vamos a verlo paso a paso.&quot;</span>
<span class="n">prompt</span> <span class="o">=</span> <span class="n">PromptTemplate</span><span class="p">(</span><span class="n">template</span><span class="o">=</span><span class="n">plantilla</span><span class="p">)</span>
<span class="n">prompt</span><span class="o">.</span><span class="n">save</span><span class="p">(</span><span class="s2">&quot;prompt.json&quot;</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<p>Una vez hecho todo esto, posteriormente podremos cargar y utilizar de nuevo esa plantilla, de la siguiente manera:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span><span class="w"> </span><span class="nn">langchain.prompts</span><span class="w"> </span><span class="kn">import</span> <span class="n">load_prompt</span>
<span class="n">prompt_cargado</span> <span class="o">=</span> <span class="n">load_prompt</span><span class="p">(</span><span class="s1">&#39;prompt.json&#39;</span><span class="p">)</span>
<span class="n">prompt_cargado</span>
</pre></div>
</div>
</div>
</div>
</section>
</section>

    <script type="text/x-thebe-config">
    {
        requestKernel: true,
        binderOptions: {
            repo: "binder-examples/jupyter-stacks-datascience",
            ref: "master",
        },
        codeMirrorConfig: {
            theme: "abcdef",
            mode: "python"
        },
        kernelOptions: {
            name: "python3",
            path: "./cuadernos"
        },
        predefinedOutput: true
    }
    </script>
    <script>kernelName = 'python3'</script>

                </article>
              

              
              
              
              
                <footer class="prev-next-footer d-print-none">
                  
<div class="prev-next-area">
    <a class="left-prev"
       href="../introduccion.html"
       title="previous page">
      <i class="fa-solid fa-angle-left"></i>
      <div class="prev-next-info">
        <p class="prev-next-subtitle">previous</p>
        <p class="prev-next-title">IA generativa</p>
      </div>
    </a>
    <a class="right-next"
       href="basesDatosLangchain.html"
       title="next page">
      <div class="prev-next-info">
        <p class="prev-next-subtitle">next</p>
        <p class="prev-next-title"><span class="section-number">2. </span>Bases de datos en LangChain.</p>
      </div>
      <i class="fa-solid fa-angle-right"></i>
    </a>
</div>
                </footer>
              
            </div>
            
            
              
                <dialog id="pst-secondary-sidebar-modal"></dialog>
                <div id="pst-secondary-sidebar" class="bd-sidebar-secondary bd-toc"><div class="sidebar-secondary-items sidebar-secondary__inner">


  <div class="sidebar-secondary-item">
  <div class="page-toc tocsection onthispage">
    <i class="fa-solid fa-list"></i> Contents
  </div>
  <nav class="bd-toc-nav page-toc">
    <ul class="visible nav section-nav flex-column">
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#apartados-de-langchain">1.1. Apartados de Langchain.</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#como-trabajar-con-langchain-en-remoto">1.2. Cómo trabajar con LangChain en remoto</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#como-trabajar-con-google-colab">1.3. Como trabajar con Google Colab.</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#como-trabajar-con-langchain-en-local">1.4. Cómo trabajar con LangChain en local.</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#las-plantillas-templates-en-langchain">1.5. Las plantillas (templates) en LangChain.</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#parsear-y-procesar-la-salida">1.6. Parsear y procesar la salida.</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#parsear-lista-elementos-separados-por-comas">1.6.1. Parsear lista elementos separados por comas.</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#parseador-en-formato-de-fechas">1.6.2. Parseador en formato de fechas</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#metodos-para-solucionar-problemas-de-parcheo">1.6.3. Métodos para solucionar problemas de parcheo.</a><ul class="nav section-nav flex-column">
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#auto-fix-parser">1.6.3.1. Auto-Fix Parser.</a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#system-promt">1.6.3.2. System Promt</a></li>
</ul>
</li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#serializacion-de-prompts">1.7. Serialización de Prompts.</a></li>
</ul>
  </nav></div>

</div></div>
              
            
          </div>
          <footer class="bd-footer-content">
            
<div class="bd-footer-content__inner container">
  
  <div class="footer-item">
    
<p class="component-author">
By Francisco Rodríguez
</p>

  </div>
  
  <div class="footer-item">
    

  <p class="copyright">
    
      © Copyright 2023.
      <br/>
    
  </p>

  </div>
  
  <div class="footer-item">
    
  </div>
  
  <div class="footer-item">
    
  </div>
  
</div>
          </footer>
        

      </main>
    </div>
  </div>
  
  <!-- Scripts loaded after <body> so the DOM is not blocked -->
  <script defer src="../_static/scripts/bootstrap.js?digest=8878045cc6db502f8baf"></script>
<script defer src="../_static/scripts/pydata-sphinx-theme.js?digest=8878045cc6db502f8baf"></script>

  <footer class="bd-footer">
  </footer>
  </body>
</html>